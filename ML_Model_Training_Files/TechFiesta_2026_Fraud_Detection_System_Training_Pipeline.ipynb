{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f600c06-2c89-4864-b531-2c0df316b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸  Compute device: GPU (NVIDIA)\n",
      "ðŸ”§ MAX_THREADS: 8\n",
      "================================================================================\n",
      "ðŸš€ ULTIMATE FRAUD DETECTION ENGINE \n",
      "   ðŸŽ¯ Target: Maximize F2 Score with balanced Precision-Recall\n",
      "   âš¡ 3 ML Models: XGBoost, LightGBM, CatBoost\n",
      "================================================================================\n",
      "ðŸ“‚ Loading data from C:\\Users\\ADMIN\\Documents\\TechFiesta_2026\\data\\realistic_transaction_dataset_821.csv\n",
      "   âœ“ Converted timestamp to datetime\n",
      "   âœ“ Converted user_signup_date to datetime\n",
      "âœ… Loaded: (150000, 38)\n",
      "ðŸ’° Fraud rate: 23.67%\n",
      "\n",
      "[Step 1/8] Data Preparation & Feature Engineering...\n",
      "\n",
      "ðŸ“Š Preparing time-aware data splits (70:15:15)...\n",
      "\n",
      "ðŸ”§ ULTIMATE Feature Engineering (150+ features)...\n",
      "   â° Temporal features...\n",
      "   ðŸ’µ Amount features...\n",
      "   ðŸ‘¤ User behavioral features...\n",
      "   ðŸŒ Location features...\n",
      "   ðŸª Merchant features...\n",
      "   ðŸ“± Device & Channel features...\n",
      "   ðŸ•¸ï¸ Creating Graph Network features...\n",
      "   âš ï¸ Risk flags & composite scores...\n",
      "   ðŸŽ¯ Enhanced balance features...\n",
      "   ðŸ”— Feature interactions...\n",
      "   ðŸŽ¯ Pattern-aligned features (matching fraud injection logic)...\n",
      "   ðŸ“Š Creating composite pattern scores...\n",
      "   ðŸ”¤ One-hot encoding...\n",
      "   âœ‚ï¸ Feature selection...\n",
      "   âœ… Total features: 235\n",
      "\n",
      "ðŸ“Š Feature matrix: (150000, 235)\n",
      "ðŸ’° Fraud rate: 23.67%\n",
      "\n",
      "ðŸ”„ Applying hybrid scaling (Robust + Quantile)...\n",
      "\n",
      "âœ… Data splits:\n",
      "   Train: (105000, 235) (fraud: 24.04%)\n",
      "   Val:   (22500, 235) (fraud: 22.80%)\n",
      "   Test:  (22500, 235) (fraud: 22.80%)\n",
      "\n",
      "[Step 2/8] Verifying Class Distribution...\n",
      "\n",
      "ðŸ“Š Class Distribution Verification...\n",
      "   Data: (105000, 235), Fraud: 24.04%\n",
      "   âœ“ Using existing distribution (balanced via model weights)\n",
      "\n",
      "[Step 3/8] Initializing Rule-Based Engine and Training BNN\n",
      "\n",
      "ðŸ”§ Training Framework Components...\n",
      "   [1/2] Initializing Rule Engine (10 expert rules)...\n",
      "   âœ“ Rule Engine ready\n",
      "   [2/2] Training Bayesian High-Risk Identifier...\n",
      "\n",
      "   ðŸ§  Training Bayesian High-Risk Identifier...\n",
      "      Gray zone: 18285 samples (8828 fraud, 9457 legit)\n",
      "      ðŸ“Š Probability distribution: 43 unique values\n",
      "         Min=0.183, Max=0.923, Median=0.485\n",
      "      ðŸ”„ Self-adjusting: Target high-risk rate 20-40%...\n",
      "      âš ï¸ No threshold in target range, finding closest...\n",
      "      âœ… Forced 30% HR threshold=0.491 (actual HR=47.7%)\n",
      "      âœ… BNN calibrated (threshold=0.491)\n",
      "      High-risk rate: 47.7% (target: 20-40%)\n",
      "      Gray zone: Recall=53.7%, Prec=54.3%\n",
      "   âœ“ Framework components ready\n",
      "\n",
      "[Step 4/8] Training ML Models (pre-tuned hyperparameters)...\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ TRAINING WITH OPTIMAL HYPERPARAMETERS (FOUND USING BAYESIAN OPTIMIZATION)\n",
      "   Using best parameters from previous optimization runs\n",
      "\n",
      "[1/3] Training XGBoost with optimal hyperparameters...\n",
      "   âœ… XGBoost trained (n_estimators=1600, lr=0.0126)\n",
      "\n",
      "[2/3] Training LightGBM with optimal hyperparameters...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[676]\tvalid_0's auc: 0.891298\n",
      "   âœ… LightGBM trained (n_estimators=1511, lr=0.0145)\n",
      "\n",
      "[3/3] Training CatBoost with optimal hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… CatBoost trained (iterations=1481, lr=0.0687)\n",
      "\n",
      "âœ… All models trained with optimal hyperparameters (Using Bayesian Optimization backed optimal weights)\n",
      "   Total models: 3\n",
      "\n",
      "[Step 5/8] Assigning Optimized Ensemble Weights...\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ ENSEMBLE WEIGHT ASSIGNMENT (Using Bayesian Optimization backed optimal weights)\n",
      "   Using optimal weights from previous optimization\n",
      "================================================================================\n",
      "   ðŸ“Š Setting weights for 3 models: ['xgboost', 'lightgbm', 'catboost']\n",
      "\n",
      "ðŸ† OPTIMAL WEIGHTS (from Trial 256, Score=0.7325):\n",
      "   xgboost     : 0.2416 (24.16%)\n",
      "   lightgbm    : 0.6847 (68.47%)\n",
      "   catboost    : 0.0737 (7.37%)\n",
      "\n",
      "âœ… Ensemble weights set (Using Bayesian Optimization backed optimal weights)\n",
      "\n",
      "[Step 6/8] Comparing ALL Voting Strategies...\n",
      "\n",
      "================================================================================\n",
      "ðŸ”¬ COMPARING ALL VOTING STRATEGIES (Validation Set)\n",
      "================================================================================\n",
      "\n",
      "Strategy            Acc%    Prec%     Rec%      F1%      F2%     AUC%\n",
      "--------------------------------------------------------------------------------\n",
      "average            83.16    60.17âœ“    77.39    67.70    73.20    89.16 â­\n",
      "weighted           83.15    60.16âœ“    77.33    67.67    73.16    89.18   \n",
      "bayesian_opt       83.12    60.14âœ“    76.98    67.53    72.90    89.18   \n",
      "tiered_vote        83.54    61.13âœ“    76.42    67.93    72.78    89.17   \n",
      "conf_weighted      83.13    60.17âœ“    77.00    67.56    72.92    89.18   \n",
      "delphi             83.16    60.17âœ“    77.39    67.70    73.20    89.16   \n",
      "   ðŸš€ Training enhanced stacking meta-model (Using Bayesian Optimization backed optimal weights)...\n",
      "      Using optimal parameters from previous optimization\n",
      "      âœ… Meta-model configured with optimal parameters\n",
      "      Meta-model input weights:\n",
      "         xgboost     : 0.3525\n",
      "         lightgbm    : 0.5674\n",
      "         catboost    : 1.8950\n",
      "      Optimized threshold: 0.479\n",
      "   âœ“ Enhanced meta-model ready (Using Bayesian Optimization backed optimal weights)\n",
      "stack_enhanced     83.30    60.50âœ“    77.08    67.79    73.08    89.06   \n",
      "   ðŸš¨ Hard Blocked (score>=85): 1297/22500 (5.8%)\n",
      "   âš¡ High Risk Blend (35% rule, BNN): 1843\n",
      "   ðŸ“Š Low Risk Blend (10% rule): 19360\n",
      "bouncer            83.32    60.65âœ“    76.42    67.63    72.64    88.93   \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ† BEST STRATEGY: AVERAGE (F2 + Precision â‰¥ 60%)\n",
      "   F2: 73.20%, AUC: 89.16%, Precision: 60.17%\n",
      "   Optimal Threshold: 0.510\n",
      "================================================================================\n",
      "\n",
      "[Step 7/8] Threshold Analysis (ALL METRICS)...\n",
      "\n",
      "====================================================================================================\n",
      "ðŸŽ¯ CASCADE-BASED THRESHOLD ANALYSIS\n",
      "   Using full pipeline: Rules (â‰¥85% hard block) â†’ BNN â†’ AVERAGE â†’ Blend\n",
      "   Constraint: Max Recall for Precision â‰ˆ 50-55%\n",
      "====================================================================================================\n",
      "\n",
      " Thresh â”‚  Recall%    Prec%      F1%      F2% â”‚     TP     FP     FN â”‚ Notes\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  0.100 â”‚    99.36    26.91    42.35    64.59 â”‚   5098  13844     33 â”‚ âœ“Recâ‰¥80\n",
      "  0.110 â”‚    99.18    27.51    43.07    65.20 â”‚   5089  13413     42 â”‚ âœ“Recâ‰¥80\n",
      "  0.120 â”‚    98.79    28.15    43.82    65.78 â”‚   5069  12938     62 â”‚ âœ“Recâ‰¥80\n",
      "  0.130 â”‚    98.48    28.82    44.59    66.39 â”‚   5053  12481     78 â”‚ âœ“Recâ‰¥80\n",
      "  0.140 â”‚    97.99    29.45    45.29    66.87 â”‚   5028  12046    103 â”‚ âœ“Recâ‰¥80\n",
      "  0.150 â”‚    97.56    30.10    46.01    67.37 â”‚   5006  11624    125 â”‚ âœ“Recâ‰¥80\n",
      "  0.160 â”‚    97.06    30.82    46.79    67.88 â”‚   4980  11177    151 â”‚ âœ“Recâ‰¥80\n",
      "  0.170 â”‚    96.47    31.62    47.63    68.41 â”‚   4950  10706    181 â”‚ âœ“Recâ‰¥80\n",
      "  0.180 â”‚    95.69    32.49    48.51    68.89 â”‚   4910  10203    221 â”‚ âœ“Recâ‰¥80\n",
      "  0.190 â”‚    95.07    33.55    49.60    69.56 â”‚   4878   9661    253 â”‚ âœ“Recâ‰¥80\n",
      "  0.200 â”‚    94.17    34.60    50.60    70.05 â”‚   4832   9135    299 â”‚ âœ“Recâ‰¥80\n",
      "  0.210 â”‚    93.32    35.77    51.72    70.60 â”‚   4788   8597    343 â”‚ âœ“Recâ‰¥80\n",
      "  0.220 â”‚    92.54    37.04    52.91    71.20 â”‚   4748   8069    383 â”‚ âœ“Recâ‰¥80\n",
      "  0.230 â”‚    91.66    38.51    54.24    71.83 â”‚   4703   7509    428 â”‚ âœ“Recâ‰¥80\n",
      "  0.240 â”‚    90.80    39.86    55.40    72.32 â”‚   4659   7029    472 â”‚ âœ“Recâ‰¥80\n",
      "  0.250 â”‚    89.79    41.35    56.62    72.75 â”‚   4607   6534    524 â”‚ âœ“Recâ‰¥80\n",
      "  0.260 â”‚    89.07    42.77    57.79    73.22 â”‚   4570   6115    561 â”‚ âœ“Recâ‰¥80\n",
      "  0.270 â”‚    87.90    44.17    58.80    73.37 â”‚   4510   5700    621 â”‚ âœ“Recâ‰¥80\n",
      "  0.280 â”‚    86.96    45.54    59.77    73.57 â”‚   4462   5337    669 â”‚ âœ“Recâ‰¥80\n",
      "  0.290 â”‚    86.30    46.76    60.66    73.82 â”‚   4428   5041    703 â”‚ âœ“Recâ‰¥80\n",
      "  0.300 â”‚    85.71    47.67    61.27    73.92 â”‚   4398   4828    733 â”‚ âœ“Recâ‰¥80\n",
      "  0.310 â”‚    85.23    48.59    61.90    74.06 â”‚   4373   4626    758 â”‚ âœ“Recâ‰¥80\n",
      "  0.320 â”‚    84.68    49.29    62.31    74.05 â”‚   4345   4470    786 â”‚ âœ“Recâ‰¥80\n",
      "  0.330 â”‚    84.29    49.95    62.73    74.10 â”‚   4325   4334    806 â”‚ âœ“Recâ‰¥80\n",
      "  0.340 â”‚    83.82    50.59    63.10    74.09 â”‚   4301   4201    830 â”‚ âœ“Recâ‰¥80 â˜…P:50-55%\n",
      "  0.350 â”‚    83.38    51.28    63.50    74.10 â”‚   4278   4064    853 â”‚ âœ“Recâ‰¥80 â˜…P:50-55%\n",
      "  0.360 â”‚    83.01    52.03    63.97    74.18 â”‚   4259   3926    872 â”‚ âœ“Recâ‰¥80 â˜…P:50-55%\n",
      "  0.370 â”‚    82.38    52.63    64.23    74.01 â”‚   4227   3805    904 â”‚ âœ“Recâ‰¥80 â˜…P:50-55%\n",
      "  0.380 â”‚    81.87    53.50    64.72    74.02 â”‚   4201   3651    930 â”‚ âœ“Recâ‰¥80 â˜…P:50-55%\n",
      "  0.390 â”‚    81.39    54.32    65.15    74.01 â”‚   4176   3512    955 â”‚ âœ“Recâ‰¥80 â˜…P:50-55%\n",
      "  0.400 â”‚    80.57    55.07    65.42    73.74 â”‚   4134   3373    997 â”‚ âœ“Recâ‰¥80 âœ“Precâ‰¥50\n",
      "  0.410 â”‚    79.71    55.81    65.65    73.42 â”‚   4090   3239   1041 â”‚ âœ“Precâ‰¥50\n",
      "  0.420 â”‚    78.99    56.80    66.08    73.26 â”‚   4053   3083   1078 â”‚ âœ“Precâ‰¥50\n",
      "  0.430 â”‚    78.35    57.83    66.54    73.15 â”‚   4020   2932   1111 â”‚ âœ“Precâ‰¥50\n",
      "  0.440 â”‚    77.67    58.80    66.93    72.98 â”‚   3985   2792   1146 â”‚ âœ“Precâ‰¥50\n",
      "  0.450 â”‚    77.00    59.95    67.42    72.86 â”‚   3951   2639   1180 â”‚ âœ“Precâ‰¥50\n",
      "  0.460 â”‚    76.55    61.00    67.90    72.84 â”‚   3928   2511   1203 â”‚ âœ“Precâ‰¥50\n",
      "  0.470 â”‚    75.54    61.83    68.00    72.33 â”‚   3876   2393   1255 â”‚ âœ“Precâ‰¥50\n",
      "  0.480 â”‚    74.80    62.74    68.24    72.03 â”‚   3838   2279   1293 â”‚ âœ“Precâ‰¥50\n",
      "  0.490 â”‚    73.98    63.74    68.48    71.68 â”‚   3796   2159   1335 â”‚ âœ“Precâ‰¥50\n",
      "  0.500 â”‚    73.26    64.94    68.85    71.43 â”‚   3759   2029   1372 â”‚ âœ“Precâ‰¥50\n",
      "  0.510 â”‚    72.36    66.27    69.18    71.06 â”‚   3713   1890   1418 â”‚ âœ“Precâ‰¥50\n",
      "  0.520 â”‚    71.41    67.18    69.23    70.52 â”‚   3664   1790   1467 â”‚ âœ“Precâ‰¥50\n",
      "  0.530 â”‚    70.18    67.99    69.07    69.73 â”‚   3601   1695   1530 â”‚ âœ“Precâ‰¥50\n",
      "  0.540 â”‚    69.28    68.99    69.14    69.23 â”‚   3555   1598   1576 â”‚ âœ“Precâ‰¥50\n",
      "  0.550 â”‚    68.13    69.95    69.03    68.49 â”‚   3496   1502   1635 â”‚ âœ“Precâ‰¥50\n",
      "  0.560 â”‚    66.89    70.85    68.81    67.64 â”‚   3432   1412   1699 â”‚ âœ“Precâ‰¥50\n",
      "  0.570 â”‚    65.41    71.71    68.41    66.58 â”‚   3356   1324   1775 â”‚ âœ“Precâ‰¥50\n",
      "  0.580 â”‚    63.61    72.47    67.75    65.21 â”‚   3264   1240   1867 â”‚ âœ“Precâ‰¥50\n",
      "  0.590 â”‚    61.57    73.33    66.94    63.61 â”‚   3159   1149   1972 â”‚ âœ“Precâ‰¥50\n",
      "  0.600 â”‚    58.74    74.24    65.59    61.30 â”‚   3014   1046   2117 â”‚ âœ“Precâ‰¥50\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ðŸŽ¯ AUTOMATIC THRESHOLD SELECTION:\n",
      "   Constraint: Best F2 with Precision in 50%-55%\n",
      "\n",
      "   âœ… OPTIMAL THRESHOLD (Algorithm-Chosen): 0.360\n",
      "      â†’ F2-Score: 74.18% (MAXIMIZED)\n",
      "      â†’ Recall: 83.01%\n",
      "      â†’ Precision: 52.03% (in target range)\n",
      "\n",
      "ðŸ“Š REFERENCE THRESHOLDS:\n",
      "   Best F2:       0.360 â†’ F2=74.18%, Rec=83.01%, Prec=52.03%\n",
      "   Best F1:       0.520 â†’ F1=69.23%, Rec=71.41%, Prec=67.18%\n",
      "   Max Rec@Pâ‰¥50%: 0.340 â†’ Rec=83.82%, Prec=50.59%\n",
      "\n",
      "âœ… Selected threshold: 0.360\n",
      "\n",
      "[Step 8/8] Final Cascaded Evaluation on Test Set...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š FINAL CASCADED SYSTEM EVALUATION ON TEST SET\n",
      "================================================================================\n",
      "   ðŸš¨ HYBRID MODE: Rule Engine (Bouncer) + AVERAGE\n",
      "   ðŸš¨ Rule Engine Hard Blocked: 1289/22500 (5.73%)\n",
      "   âš¡ High Risk Blend (35% rule, BNN+Rules): 1856 (0 by rules only)\n",
      "   ðŸ“Š Low Risk Blend (10% rule): 19355\n",
      "\n",
      "ðŸ“ˆ METRICS (Threshold=0.360):\n",
      "   Accuracy:            79.02% âš ï¸\n",
      "   Balanced Accuracy:   80.43%\n",
      "   Precision:           52.52% âš ï¸\n",
      "   Recall:              83.02% âœ…\n",
      "   F1-Score:            64.34% âš ï¸\n",
      "   F2-Score:            74.38%\n",
      "   ROC-AUC:             89.00%\n",
      "   PR-AUC:              78.49%\n",
      "   MCC:                 0.5318\n",
      "   Cohen's Kappa:       0.5052\n",
      "\n",
      "ðŸ”¢ Confusion Matrix:\n",
      "   TN:  13522 | FP:   3849\n",
      "   FN:    871 | TP:   4258\n",
      "\n",
      "ðŸ“Š Individual Model Performance:\n",
      "   xgboost     : F1=63.58%, AUC=89.21%\n",
      "   lightgbm    : F1=62.74%, AUC=89.17%\n",
      "   catboost    : F1=63.12%, AUC=89.00%\n",
      "\n",
      "ðŸ“Š Ensemble Strategy Comparison (Test Set):\n",
      "   ðŸ† average        : F2=74.38%, F1=63.26%, Rec=84.25%, AUC=89.22%\n",
      "      weighted       : F2=74.42%, F1=63.31%, Rec=84.29%, AUC=89.23%\n",
      "      bayesian_opt   : F2=74.32%, F1=63.01%, Rec=84.42%, AUC=89.22%\n",
      "      conf_weighted  : F2=74.39%, F1=63.15%, Rec=84.40%, AUC=89.22%\n",
      "      stack_enhanced : F2=73.38%, F1=67.62%, Rec=77.79%, AUC=89.09%\n",
      "\n",
      "ðŸŽ¯ Optimized Ensemble Weights:\n",
      "   xgboost     : 0.2416 (24.16%)\n",
      "   lightgbm    : 0.6847 (68.47%)\n",
      "   catboost    : 0.0737 (7.37%)\n",
      "\n",
      "ðŸŽ¯ BALANCE ANALYSIS:\n",
      "   Precision-Recall Gap: 30.5% ðŸ“Š\n",
      "   Balance Score: 0.633 ðŸ“Š\n",
      "   F1 Score: 64.34% ðŸ“Š\n",
      "   Overall Balance: âœ… GOOD\n",
      "\n",
      "   âœ… PRECISION IMPROVED: 52.5% (+15.6% from baseline)\n",
      "\n",
      "ðŸ§¬ OPTIMIZATION SUMMARY:\n",
      "   â€¢ Mode: FAST (Hardcoded Optimal Parameters)\n",
      "   â€¢ Bayesian Optimization: SKIPPED for all models\n",
      "   â€¢ Cost Savings: ~600+ trials avoided\n",
      "\n",
      "ðŸ“Š Generating comprehensive analysis plots...\n",
      "   âœ“ Saved: ultimate_fraud_analysis.png\n",
      "\n",
      "ðŸ’¾ Saving cascaded fraud detection system...\n",
      "------------------------------------------------------------\n",
      "   âœ… Stage 1: ultimate_fraud_system_fast.joblib_stage1.joblib\n",
      "      â””â”€â”€ 30 rules (config) + BNN model, hard_blockâ‰¥85\n",
      "   âœ… Stage 1 Config (JSON): ultimate_fraud_system_fast.joblib_rules.json\n",
      "      â””â”€â”€ Includes 29 declarative rule conditions\n",
      "   ðŸ”„ XGBoost converted to CPU for portable inference\n",
      "   âœ… Stage 2: ultimate_fraud_system_fast.joblib_ensemble.joblib\n",
      "      â””â”€â”€ 3 models + scalers, threshold=0.360\n",
      "   âœ… Feature Importance: ultimate_fraud_system_fast.joblib_feature_importance.csv\n",
      "   âœ… Ensemble Weights: ultimate_fraud_system_fast.joblib_ensemble_weights.json\n",
      "   âœ… Feature Config: ultimate_fraud_system_fast.joblib_feature_config.json\n",
      "      â””â”€â”€ 235 training columns, 8 categorical cols\n",
      "   âœ… User/Merchant Stats: ultimate_fraud_system_fast.joblib_user_merchant_stats.joblib\n",
      "      â””â”€â”€ 8000 users, 2000 merchants\n",
      "------------------------------------------------------------\n",
      "ðŸŽ‰ CASCADED SYSTEM SAVED SUCCESSFULLY!\n",
      "   ðŸ“Œ ARCHITECTURE:\n",
      "      Stage 1: Rules + BNN â†’ ultimate_fraud_system_fast.joblib_stage1.joblib\n",
      "      Stage 2: ML Ensemble â†’ ultimate_fraud_system_fast.joblib_ensemble.joblib\n",
      "      Feature Eng: ultimate_fraud_system_fast.joblib_feature_config.json + ultimate_fraud_system_fast.joblib_user_merchant_stats.joblib\n",
      "   ðŸ“Œ THRESHOLDS:\n",
      "      Rule Engine:  hard_blockâ‰¥85 (ALGORITHM-CHOSEN)\n",
      "      Ensemble ML:  0.360 (ALGORITHM-CHOSEN)\n",
      "   To load in production:\n",
      "   â€¢ Stage 1: joblib.load('ultimate_fraud_system_fast.joblib_stage1.joblib')\n",
      "   â€¢ Stage 2: joblib.load('ultimate_fraud_system_fast.joblib_ensemble.joblib')\n",
      "   â€¢ Feature Eng: FeatureEngineer('ultimate_fraud_system_fast.joblib_feature_config.json', 'ultimate_fraud_system_fast.joblib_user_merchant_stats.joblib')\n",
      "\n",
      "================================================================================\n",
      "âœ… FAST MODE ENGINE - COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š SUMMARY:\n",
      "   â€¢ Models trained: 3 (with pre-tuned hyperparameters)\n",
      "   â€¢ Bayesian optimization: SKIPPED (using best params from previous run)\n",
      "   â€¢ Threshold table: Printed above for manual selection\n",
      "\n",
      "ðŸŽ¯ PERFORMANCE:\n",
      "   â€¢ Recall: 83.02%\n",
      "   â€¢ Precision: 52.52%\n",
      "   â€¢ F1-Score: 64.34%\n",
      "\n",
      "ðŸ“Œ TO OVERRIDE THRESHOLD:\n",
      "   results = engine.evaluate_final_recall_optimized()\n",
      "\n",
      "ðŸ’¾ SAVED:\n",
      "   - ultimate_fraud_system_fast.joblib\n",
      "   - ultimate_evaluation_results_fast.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ULTIMATE FRAUD DETECTION ENGINE v2.1 - RECALL-OPTIMIZED HYBRID SYSTEM\n",
    "========================================================================================\n",
    "ENHANCEMENTS ADDED TO ORIGINAL ROBUST SYSTEM:\n",
    "1. Multi-Stage Bayesian Optimization (200+ trials per model with early stopping)\n",
    "2. Recall-Focused Multi-Objective Optimization (F2-weighted scoring)\n",
    "3. Advanced Stacking with multiple meta-learners\n",
    "4. Dynamic ensemble strategy selection based on recall thresholds\n",
    "5. Extended hyperparameter search spaces\n",
    "6. Maintains ALL original 150+ features and robustness features\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ============================================================================\n",
    "# CENTRALIZED THREAD MANAGEMENT\n",
    "# ============================================================================\n",
    "# Use half of CPU cores to prevent oversubscription\n",
    "MAX_THREADS = max(1, os.cpu_count() // 2)\n",
    "\n",
    "# Thread control for numerical libraries\n",
    "os.environ['OMP_NUM_THREADS'] = str(MAX_THREADS)\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = str(MAX_THREADS)\n",
    "os.environ['MKL_NUM_THREADS'] = str(MAX_THREADS)\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(MAX_THREADS)\n",
    "\n",
    "# ============================================================================\n",
    "# DEVICE DETECTION\n",
    "# ============================================================================\n",
    "try:\n",
    "    import torch\n",
    "    DEVICE_AVAILABLE = {\n",
    "        'cuda': torch.cuda.is_available(),\n",
    "        'mps': torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
    "    }\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    # Fallback: check nvidia-smi for CUDA\n",
    "    try:\n",
    "        subprocess.run(['nvidia-smi'], capture_output=True, check=True)\n",
    "        DEVICE_AVAILABLE = {'cuda': True, 'mps': False}\n",
    "    except:\n",
    "        DEVICE_AVAILABLE = {'cuda': False, 'mps': False}\n",
    "\n",
    "def get_compute_device():\n",
    "    \"\"\"Auto-detect best available device\"\"\"\n",
    "    if DEVICE_AVAILABLE['cuda']:\n",
    "        return 'cuda', 'GPU (NVIDIA)'\n",
    "    elif DEVICE_AVAILABLE['mps']:\n",
    "        return 'mps', 'GPU (Apple Silicon)'\n",
    "    else:\n",
    "        return 'cpu', 'CPU'\n",
    "\n",
    "DEVICE, DEVICE_NAME = get_compute_device()\n",
    "print(f\"ðŸ–¥ï¸  Compute device: {DEVICE_NAME}\")\n",
    "print(f\"ðŸ”§ MAX_THREADS: {MAX_THREADS}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Core ML Libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, average_precision_score, f1_score,\n",
    "    accuracy_score, precision_score, recall_score, roc_curve, \n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score,\n",
    "    fbeta_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Gradient Boosting Ensemble\n",
    "import xgboost as xgb\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"âš ï¸ LightGBM not available\")\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "    print(\"âš ï¸ CatBoost not available\")\n",
    "\n",
    "# Network Analysis\n",
    "try:\n",
    "    import networkx as nx\n",
    "    NETWORKX_AVAILABLE = True\n",
    "except:\n",
    "    NETWORKX_AVAILABLE = False\n",
    "    print(\"âš ï¸ NetworkX not available (graph features disabled)\")\n",
    "\n",
    "# Bayesian Optimization\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.pruners import SuccessiveHalvingPruner, MedianPruner\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"âš ï¸ Optuna not available (using default parameters)\")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python native types\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HAVERSINE DISTANCE FUNCTION (for Impossible Travel Detection)\n",
    "# ============================================================================\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on Earth.\n",
    "    Returns distance in kilometers.\n",
    "    \n",
    "    Used for impossible travel detection - if a user appears in two\n",
    "    locations faster than physically possible (e.g., >900 km/h), it's fraud.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lat1, lon1 : float - First location (latitude, longitude in degrees)\n",
    "    lat2, lon2 : float - Second location (latitude, longitude in degrees)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Distance in kilometers\n",
    "    \"\"\"\n",
    "    R = 6371  # Earth's radius in km\n",
    "    \n",
    "    try:\n",
    "        phi1 = radians(float(lat1))\n",
    "        phi2 = radians(float(lat2))\n",
    "        dphi = radians(float(lat2) - float(lat1))\n",
    "        dlambda = radians(float(lon2) - float(lon1))\n",
    "        \n",
    "        a = sin(dphi / 2) ** 2 + cos(phi1) * cos(phi2) * sin(dlambda / 2) ** 2\n",
    "        \n",
    "        return 2 * R * atan2(sqrt(a), sqrt(1 - a))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RULE WEIGHTS (Calibrated based on fraud detection research)\n",
    "# ============================================================================\n",
    "RULE_WEIGHTS = {\n",
    "    # =========================================================================\n",
    "    # EXISTING RULES (Weights 10-50)\n",
    "    # =========================================================================\n",
    "    \"new_device\": 15,              # Transaction from unknown device\n",
    "    \"new_country\": 20,             # Transaction from different country\n",
    "    \"high_amount\": 25,             # Amount significantly above user average (>5x)\n",
    "    \"velocity_attack\": 30,         # Too many transactions in short time (>=5 in 10min)\n",
    "    \"velocity_attack_extreme\": 50, # Extreme velocity attack (>10 txns in 10min)\n",
    "    \"velocity_suspicious\": 15,     # Suspicious velocity (3-4 txns in 10min)\n",
    "    \"impossible_travel\": 40,       # Physically impossible location change (>900 km/h)\n",
    "    \"night_transaction\": 10,       # Transaction during unusual hours (0-5 AM)\n",
    "    \"amount_anomaly_extreme\": 35,  # Amount >10x user average\n",
    "    \"new_device_night_high\": 45,   # Combination: new device + night + high amount\n",
    "    \"new_country_high_amount\": 40, # Combination: new country + high amount\n",
    "    \"first_txn_high\": 30,          # First transaction is unusually high\n",
    "    \"fraud_history\": 50,           # User has past fraud incidents\n",
    "    \"rapid_burst\": 25,             # >3 transactions in 1 minute\n",
    "    \"high_risk_merchant_night\": 20, # Risky merchant category at night\n",
    "    \"country_mismatch\": 35,        # Multiple country mismatches\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NEW RULES - PHASE 1: SEQUENTIAL PATTERN RULES\n",
    "    # =========================================================================\n",
    "    \"card_testing_sequence\": 85,   # Micro txns (<$15) followed by large txn (>$300)\n",
    "    \"merchant_category_hopping\": 70, # 3+ high-risk categories in 1 hour\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NEW RULES - PHASE 1: PHYSICAL IMPOSSIBILITY RULES\n",
    "    # =========================================================================\n",
    "    \"speed_of_light_violation\": 98,  # Travel >1500 km/h (HARD BLOCK - impossible)\n",
    "    \"suspicious_travel\": 50,         # Travel 500-900 km/h (rare but possible)\n",
    "    \"timezone_impossibility\": 60,    # 2-5 AM local + country mismatch\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NEW RULES - PHASE 1: BUSINESS LOGIC RULES\n",
    "    # =========================================================================\n",
    "    \"sanctioned_country_merchant\": 90, # US/UK user + sanctioned country merchant\n",
    "    \"payment_method_mismatch\": 65,   # Crypto merchant + card payment\n",
    "    \"refund_before_purchase\": 98,    # Refund before purchase (HARD BLOCK - 100% fraud)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NEW RULES - PHASE 1: IDENTITY COHERENCE RULES\n",
    "    # =========================================================================\n",
    "    \"impossible_user_profile\": 75,   # New signup + KYC failed + high value\n",
    "    \"device_fingerprint_chaos\": 80,  # 5+ devices + 5+ IPs in 24 hours\n",
    "    \"email_country_mismatch\": 40,    # Email domain (.ru) != billing country (US)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NEW RULES - PHASE 2: REPEAT OFFENDER & MICRO TXN PATTERNS\n",
    "    # =========================================================================\n",
    "    \"repeat_fraud_offender\": 85,     # User with 2+ past fraud incidents (HARD BLOCK)\n",
    "    \"micro_txn_velocity\": 85,        # 3+ micro transactions in 5 min (HARD BLOCK - card testing)\n",
    "    \"fraud_history_high\": 65,        # User with fraud history + above-avg purchase\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RULE CONDITIONS (Declarative format - can be serialized to JSON)\n",
    "# ============================================================================\n",
    "# Each rule is defined as a dict with:\n",
    "#   - field: The transaction field to check\n",
    "#   - op: Operator (>, >=, <, <=, ==, between, in, and, or)\n",
    "#   - value: The comparison value(s)\n",
    "#\n",
    "# For compound conditions, use 'and'/'or' operators with nested conditions\n",
    "# ============================================================================\n",
    "RULE_CONDITIONS = {\n",
    "    # =========================================================================\n",
    "    # TIER 1: CRITICAL RULES\n",
    "    # =========================================================================\n",
    "    \"impossible_travel\": {\n",
    "        \"field\": \"travel_speed_kmh\",\n",
    "        \"op\": \">\",\n",
    "        \"value\": 900\n",
    "    },\n",
    "    \"fraud_history_high\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"user_past_fraud_count\", \"op\": \">=\", \"value\": 2},\n",
    "            {\"field\": \"amount\", \"op\": \">\", \"value\": 200}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 2: HIGH-RISK COMBINATION RULES\n",
    "    # =========================================================================\n",
    "    \"new_device_night_high\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"is_new_device\", \"op\": \"==\", \"value\": 1},\n",
    "            {\"field\": \"is_night\", \"op\": \"==\", \"value\": 1},\n",
    "            {\"field\": \"amount\", \"op\": \">\", \"value\": 500}\n",
    "        ]\n",
    "    },\n",
    "    \"new_country_high_amount\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"is_new_country\", \"op\": \"==\", \"value\": 1},\n",
    "            {\"field\": \"amount\", \"op\": \">\", \"value\": 1000}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 3: VELOCITY & BEHAVIORAL RULES (Graduated Thresholds)\n",
    "    # =========================================================================\n",
    "    \"velocity_attack_extreme\": {\n",
    "        \"field\": \"txns_last_10min\",\n",
    "        \"op\": \">\",\n",
    "        \"value\": 10\n",
    "    },\n",
    "    \"velocity_attack\": {\n",
    "        \"field\": \"txns_last_10min\",\n",
    "        \"op\": \"between\",\n",
    "        \"value\": [5, 10]\n",
    "    },\n",
    "    \"velocity_suspicious\": {\n",
    "        \"field\": \"txns_last_10min\",\n",
    "        \"op\": \"between\",\n",
    "        \"value\": [3, 4]\n",
    "    },\n",
    "    \"rapid_burst\": {\n",
    "        \"field\": \"is_burst_txn\",\n",
    "        \"op\": \"==\",\n",
    "        \"value\": 1\n",
    "    },\n",
    "    \"amount_anomaly_extreme\": {\n",
    "        \"field\": \"amount_vs_user_avg\",\n",
    "        \"op\": \">\",\n",
    "        \"value\": 10\n",
    "    },\n",
    "    \"high_amount\": {\n",
    "        \"field\": \"amount_vs_user_avg\",\n",
    "        \"op\": \">\",\n",
    "        \"value\": 5\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 4: SINGLE INDICATOR RULES\n",
    "    # =========================================================================\n",
    "    \"first_txn_high\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"user_total_txn_count\", \"op\": \"==\", \"value\": 0},\n",
    "            {\"field\": \"amount\", \"op\": \">\", \"value\": 1000}\n",
    "        ]\n",
    "    },\n",
    "    \"country_mismatch_risk\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"country_mismatch_count\", \"op\": \">=\", \"value\": 2},\n",
    "            {\"field\": \"high_risk_country_count\", \"op\": \">=\", \"value\": 1}\n",
    "        ]\n",
    "    },\n",
    "    \"new_device\": {\n",
    "        \"field\": \"is_new_device\",\n",
    "        \"op\": \"==\",\n",
    "        \"value\": 1\n",
    "    },\n",
    "    \"new_country\": {\n",
    "        \"field\": \"is_new_country\",\n",
    "        \"op\": \"==\",\n",
    "        \"value\": 1\n",
    "    },\n",
    "    \"night_transaction\": {\n",
    "        \"field\": \"is_night\",\n",
    "        \"op\": \"==\",\n",
    "        \"value\": 1\n",
    "    },\n",
    "    \"risky_merchant_night\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"is_high_risk_category\", \"op\": \"==\", \"value\": 1},\n",
    "            {\"field\": \"is_night\", \"op\": \"==\", \"value\": 1}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 5: SEQUENTIAL PATTERN RULES\n",
    "    # =========================================================================\n",
    "    \"card_testing_sequence\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"micro_txn_count_5min\", \"op\": \">=\", \"value\": 2},\n",
    "            {\"field\": \"amount\", \"op\": \">\", \"value\": 300},\n",
    "            {\"field\": \"time_since_last_micro_txn_min\", \"op\": \"<\", \"value\": 5}\n",
    "        ]\n",
    "    },\n",
    "    \"merchant_category_hopping\": {\n",
    "        \"field\": \"distinct_high_risk_categories_1hr\",\n",
    "        \"op\": \">=\",\n",
    "        \"value\": 3\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 6: PHYSICAL IMPOSSIBILITY RULES\n",
    "    # =========================================================================\n",
    "    \"speed_of_light_violation\": {\n",
    "        \"field\": \"travel_speed_kmh\",\n",
    "        \"op\": \">\",\n",
    "        \"value\": 1500\n",
    "    },\n",
    "    \"suspicious_travel\": {\n",
    "        \"field\": \"travel_speed_kmh\",\n",
    "        \"op\": \"between\",\n",
    "        \"value\": [500, 900]\n",
    "    },\n",
    "    \"timezone_impossibility\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"is_night\", \"op\": \"==\", \"value\": 1},\n",
    "            {\"field\": \"is_new_country\", \"op\": \"==\", \"value\": 1},\n",
    "            {\"field\": \"local_hour\", \"op\": \"between\", \"value\": [2, 5]}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 7: BUSINESS LOGIC RULES\n",
    "    # =========================================================================\n",
    "    \"sanctioned_country_merchant\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"user_country\", \"op\": \"in\", \"value\": [\"US\", \"UK\", \"GB\", \"DE\", \"FR\"]},\n",
    "            {\"field\": \"merchant_country\", \"op\": \"in\", \"value\": [\"KP\", \"IR\", \"SY\", \"CU\", \"VE\", \"RU\", \"BY\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"payment_method_mismatch\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"merchant_category\", \"op\": \"in\", \"value\": [\"crypto\", \"cryptocurrency\", \"digital_wallet\"]},\n",
    "            {\"field\": \"payment_method\", \"op\": \"in\", \"value\": [\"credit_card\", \"debit_card\", \"card\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"refund_before_purchase\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"is_refund\", \"op\": \"==\", \"value\": 1},\n",
    "            {\"field\": \"user_total_purchase_count\", \"op\": \"==\", \"value\": 0}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 8: IDENTITY COHERENCE RULES\n",
    "    # =========================================================================\n",
    "    \"impossible_user_profile\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"account_age_days\", \"op\": \"<\", \"value\": 7},\n",
    "            {\"field\": \"kyc_status\", \"op\": \"in\", \"value\": [\"failed\", \"pending\", \"not_submitted\"]},\n",
    "            {\"field\": \"amount\", \"op\": \">\", \"value\": 500}\n",
    "        ]\n",
    "    },\n",
    "    \"device_fingerprint_chaos\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"distinct_devices_24hr\", \"op\": \">=\", \"value\": 5},\n",
    "            {\"field\": \"distinct_ips_24hr\", \"op\": \">=\", \"value\": 5}\n",
    "        ]\n",
    "    },\n",
    "    \"email_country_mismatch\": {\n",
    "        \"op\": \"and\",\n",
    "        \"conditions\": [\n",
    "            {\"field\": \"email_domain_country\", \"op\": \"!=\", \"value\": \"\"},\n",
    "            {\"field\": \"billing_country\", \"op\": \"!=\", \"value\": \"\"},\n",
    "            {\"field\": \"_custom\", \"op\": \"field_ne\", \"value\": [\"email_domain_country\", \"billing_country\"]}\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TIER 9: REPEAT OFFENDER & MICRO TXN PATTERNS\n",
    "    # =========================================================================\n",
    "    \"repeat_fraud_offender\": {\n",
    "        \"field\": \"user_past_fraud_count\",\n",
    "        \"op\": \">=\",\n",
    "        \"value\": 2\n",
    "    },\n",
    "    \"micro_txn_velocity\": {\n",
    "        \"field\": \"micro_txn_count_5min\",\n",
    "        \"op\": \">=\",\n",
    "        \"value\": 3\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: GRADUATED THRESHOLDS FOR TRAVEL SPEED\n",
    "# ============================================================================\n",
    "def compute_travel_risk(speed_kmh):\n",
    "    \"\"\"\n",
    "    Graduated thresholds for travel speed detection.\n",
    "    \n",
    "    Instead of a binary cutoff, this function returns graduated risk scores\n",
    "    based on how physically impossible the travel speed is.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    speed_kmh : float\n",
    "        Calculated travel speed in kilometers per hour\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (score, reason)\n",
    "        - score: int (0-95) risk score\n",
    "        - reason: str or None - human readable reason\n",
    "    \n",
    "    Speed Thresholds:\n",
    "    - >1500 km/h: Teleportation (impossible even by supersonic jet)\n",
    "    - 900-1500 km/h: Impossible for commercial travel (max ~900 km/h)\n",
    "    - 500-900 km/h: Suspicious but possible (private jet, connection)\n",
    "    - <500 km/h: Normal travel\n",
    "    \"\"\"\n",
    "    if speed_kmh > 1500:\n",
    "        return 95, \"Speed-of-Light Violation (teleportation detected)\"\n",
    "    elif speed_kmh > 900:\n",
    "        return 80, \"Impossible Travel (faster than commercial aircraft)\"\n",
    "    elif speed_kmh > 500:\n",
    "        return 50, \"Suspicious Travel (unusually fast)\"\n",
    "    else:\n",
    "        return 0, None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: CONTEXTUAL RULE WEIGHTING\n",
    "# ============================================================================\n",
    "def contextual_weight_night_transaction(base_weight, context):\n",
    "    \"\"\"\n",
    "    Dynamically adjust night transaction rule weight based on context.\n",
    "    \n",
    "    Business users working late, entertainment purchases, etc. should\n",
    "    have reduced weight, while financial transactions at night are riskier.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_weight : int\n",
    "        The base rule weight (default 10 for night_transaction)\n",
    "    context : dict\n",
    "        Transaction context with keys like:\n",
    "        - user_segment: 'consumer' | 'corporate' | 'vip'\n",
    "        - merchant_category: category string\n",
    "        - is_new_user: bool\n",
    "        - amount: float\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int: Adjusted weight (capped at 95)\n",
    "    \n",
    "    Adjustment Logic:\n",
    "    - Corporate users: 70% reduction (they work late)\n",
    "    - Entertainment/Food: 50% reduction (normal at night)\n",
    "    - Crypto/Money Transfer: 2x increase (high risk at night)\n",
    "    - New user + high value: 3x increase (major red flag)\n",
    "    \"\"\"\n",
    "    weight = float(base_weight)\n",
    "    \n",
    "    # Reduce for business users (they work late, international calls, etc.)\n",
    "    user_segment = context.get('user_segment', 'consumer')\n",
    "    if user_segment in ['corporate', 'business', 'enterprise']:\n",
    "        weight *= 0.3\n",
    "    elif user_segment == 'vip':\n",
    "        weight *= 0.5  # VIP users have unusual patterns\n",
    "    \n",
    "    # Reduce for entertainment/food (expected at night)\n",
    "    merchant_category = context.get('merchant_category', '').lower()\n",
    "    if merchant_category in ['food', 'restaurant', 'entertainment', 'bar', 'club', 'gaming']:\n",
    "        weight *= 0.5\n",
    "    \n",
    "    # Increase for financial transactions at night (high risk)\n",
    "    if merchant_category in ['crypto', 'cryptocurrency', 'money_transfer', 'wire_transfer', 'forex']:\n",
    "        weight *= 2.0\n",
    "    \n",
    "    # Massive increase for new user + late night + high value\n",
    "    is_new_user = context.get('is_new_user', False) or context.get('account_age_days', 9999) < 30\n",
    "    amount = context.get('amount', 0)\n",
    "    if is_new_user and amount > 1000:\n",
    "        weight *= 3.0\n",
    "    elif is_new_user and amount > 500:\n",
    "        weight *= 2.0\n",
    "    \n",
    "    # Cap at 95 to leave room for critical rules\n",
    "    return int(min(95, max(0, weight)))\n",
    "\n",
    "\n",
    "\n",
    "# FRAMEWORK COMPONENT 1: RULE-BASED ENGINE (Enhanced with User Profile)\n",
    "# ============================================================================\n",
    "class RuleBasedEngine:\n",
    "    \"\"\"\n",
    "    Rule-Based Fraud Detection Engine\n",
    "    \n",
    "    A production-grade fraud detection system using expert-crafted rules.\n",
    "    Provides fast (~10-30ms), explainable decisions that complement ML models.\n",
    "    \n",
    "    Key Detection Patterns:\n",
    "    1. Impossible Travel - User can't be in two distant locations simultaneously\n",
    "    2. Velocity Attack - Too many transactions in short timeframe (>5 in 10 min)\n",
    "    3. New Device/Country - Unusual access patterns\n",
    "    4. Amount Anomaly - Transaction significantly above user's average (>5x)\n",
    "    5. Night Transactions - Unusual timing (0-5 AM local time)\n",
    "    6. Combination Rules - Multiple weak signals combining to strong evidence\n",
    "    \n",
    "    Risk Levels:\n",
    "    - LOW (0-30): Normal transaction\n",
    "    - MEDIUM-LOW (31-50): Minor anomaly, monitor\n",
    "    - MEDIUM (51-70): Suspicious, may require verification\n",
    "    - HIGH (71-90): Likely fraud, block or verify\n",
    "    - CRITICAL (91-100): Definite fraud, block immediately\n",
    "    \n",
    "    Usage:\n",
    "    ------\n",
    "    # Basic usage (with pre-computed features)\n",
    "    engine = RuleBasedEngine()\n",
    "    score, rules = engine.evaluate(transaction_dict)\n",
    "    \n",
    "    # With user profile (for real-time enrichment)\n",
    "    user_profile = {\"known_devices\": [...], \"home_country\": \"US\", \"avg_amount\": 200}\n",
    "    result = engine.evaluate_with_profile(transaction, user_profile)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = RULE_WEIGHTS\n",
    "        self.rules = []\n",
    "        self._setup_rules()\n",
    "    \n",
    "    def _setup_rules(self):\n",
    "        \"\"\"Define expert fraud detection rules with calibrated weights\"\"\"\n",
    "        self.rules = [\n",
    "            # =================================================================\n",
    "            # TIER 1: CRITICAL RULES (Immediate Fraud Indicators)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 1: Impossible Travel (>900 km/h travel speed)\n",
    "            # If user was in NYC and 10 mins later in London, that's physically impossible\n",
    "            ('impossible_travel', \n",
    "             lambda x: x.get('travel_speed_kmh', 0) > 900, \n",
    "             self.weights['impossible_travel']),\n",
    "            \n",
    "            # Rule 2: Fraud History + High Amount\n",
    "            ('fraud_history_high', \n",
    "             lambda x: (x.get('user_past_fraud_count', 0) >= 2 and x.get('amount', 0) > 200),\n",
    "             self.weights['fraud_history']),\n",
    "            \n",
    "            # =================================================================\n",
    "            # TIER 2: HIGH-RISK COMBINATION RULES\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 3: New Device + Night + High Amount (classic fraud pattern)\n",
    "            ('new_device_night_high', lambda x: (\n",
    "                x.get('is_new_device', 0) == 1 and \n",
    "                x.get('is_night', 0) == 1 and \n",
    "                x.get('amount', 0) > 500\n",
    "            ), self.weights['new_device_night_high']),\n",
    "            \n",
    "            # Rule 4: New Country + High Amount\n",
    "            ('new_country_high_amount', lambda x: (\n",
    "                x.get('is_new_country', 0) == 1 and \n",
    "                x.get('amount', 0) > 1000\n",
    "            ), self.weights['new_country_high_amount']),\n",
    "            \n",
    "            # =================================================================\n",
    "            # TIER 3: VELOCITY & BEHAVIORAL RULES (Graduated Thresholds)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 5a: Extreme Velocity Attack (>10 txns in 10 min) - CRITICAL\n",
    "            ('velocity_attack_extreme', lambda x: x.get('txns_last_10min', 0) > 10, \n",
    "             self.weights['velocity_attack_extreme']),\n",
    "            \n",
    "            # Rule 5b: Velocity Attack (>=5 txns in 10 min) - HIGH RISK\n",
    "            ('velocity_attack', lambda x: 5 <= x.get('txns_last_10min', 0) <= 10, \n",
    "             self.weights['velocity_attack']),\n",
    "            \n",
    "            # Rule 5c: Suspicious Velocity (3-4 txns in 10 min) - MODERATE\n",
    "            ('velocity_suspicious', lambda x: 3 <= x.get('txns_last_10min', 0) < 5, \n",
    "             self.weights['velocity_suspicious']),\n",
    "            \n",
    "            # Rule 6: Rapid Burst (>3 txns in 1 min)\n",
    "            ('rapid_burst', lambda x: x.get('is_burst_txn', 0) == 1, \n",
    "             self.weights['rapid_burst']),\n",
    "            \n",
    "            # Rule 7: Extreme Amount Anomaly (>10x user average)\n",
    "            ('amount_anomaly_extreme', lambda x: x.get('amount_vs_user_avg', 1) > 10,\n",
    "             self.weights['amount_anomaly_extreme']),\n",
    "            \n",
    "            # Rule 8: High Amount Anomaly (>5x user average) - From friend's code\n",
    "            ('high_amount', lambda x: x.get('amount_vs_user_avg', 1) > 5,\n",
    "             self.weights['high_amount']),\n",
    "            \n",
    "            # =================================================================\n",
    "            # TIER 4: SINGLE INDICATOR RULES\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 9: First Transaction > $1000\n",
    "            ('first_txn_high', lambda x: (\n",
    "                x.get('user_total_txn_count', 0) == 0 and \n",
    "                x.get('amount', 0) > 1000\n",
    "            ), self.weights['first_txn_high']),\n",
    "            \n",
    "            # Rule 10: Country Mismatch + High Risk\n",
    "            ('country_mismatch_risk', lambda x: (\n",
    "                x.get('country_mismatch_count', 0) >= 2 and \n",
    "                x.get('high_risk_country_count', 0) >= 1\n",
    "            ), self.weights['country_mismatch']),\n",
    "            \n",
    "            # Rule 11: New Device (standalone)\n",
    "            ('new_device', lambda x: x.get('is_new_device', 0) == 1,\n",
    "             self.weights['new_device']),\n",
    "            \n",
    "            # Rule 12: New Country (standalone)\n",
    "            ('new_country', lambda x: x.get('is_new_country', 0) == 1,\n",
    "             self.weights['new_country']),\n",
    "            \n",
    "            # Rule 13: Night Transaction (0-5 AM) - From friend's code\n",
    "            ('night_transaction', lambda x: x.get('is_night', 0) == 1,\n",
    "             self.weights['night_transaction']),\n",
    "            \n",
    "            # Rule 14: High-Risk Merchant at Night\n",
    "            ('risky_merchant_night', lambda x: (\n",
    "                x.get('is_high_risk_category', 0) == 1 and \n",
    "                x.get('is_night', 0) == 1\n",
    "            ), self.weights['high_risk_merchant_night']),\n",
    "            \n",
    "            # =================================================================\n",
    "            # TIER 5: SEQUENTIAL PATTERN RULES (ML models miss these)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 15: Card Testing Sequence\n",
    "            # Pattern: 2+ micro transactions (<$15) in 5min followed by large (>$300)\n",
    "            ('card_testing_sequence', lambda x: (\n",
    "                x.get('micro_txn_count_5min', 0) >= 2 and \n",
    "                x.get('amount', 0) > 300 and\n",
    "                x.get('time_since_last_micro_txn_min', 999) < 5\n",
    "            ), self.weights['card_testing_sequence']),\n",
    "            \n",
    "            # Rule 16: Rapid Merchant Category Hopping\n",
    "            # Pattern: 3+ different high-risk categories in 1 hour\n",
    "            ('merchant_category_hopping', lambda x: (\n",
    "                x.get('distinct_high_risk_categories_1hr', 0) >= 3\n",
    "            ), self.weights['merchant_category_hopping']),\n",
    "            \n",
    "            # =================================================================\n",
    "            # TIER 6: PHYSICAL IMPOSSIBILITY RULES (Graduated Thresholds)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 17: Speed-of-Light Violation (>1500 km/h - completely impossible)\n",
    "            ('speed_of_light_violation', lambda x: x.get('travel_speed_kmh', 0) > 1500, \n",
    "             self.weights['speed_of_light_violation']),\n",
    "            \n",
    "            # Rule 18: Suspicious Travel (500-900 km/h - rare but possible with fast jets)\n",
    "            ('suspicious_travel', lambda x: (\n",
    "                500 < x.get('travel_speed_kmh', 0) <= 900\n",
    "            ), self.weights['suspicious_travel']),\n",
    "            \n",
    "            # Rule 19: Timezone Impossibility\n",
    "            # Transaction at 2-5 AM user's local time + different country\n",
    "            ('timezone_impossibility', lambda x: (\n",
    "                x.get('is_night', 0) == 1 and\n",
    "                x.get('is_new_country', 0) == 1 and\n",
    "                x.get('local_hour', 12) >= 2 and\n",
    "                x.get('local_hour', 12) <= 5\n",
    "            ), self.weights['timezone_impossibility']),\n",
    "            \n",
    "            # =================================================================\n",
    "            # TIER 7: BUSINESS LOGIC RULES (Domain knowledge)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 20: Sanctioned Country + Merchant Combination\n",
    "            # US/UK/EU user transacting with merchant in sanctioned country\n",
    "            ('sanctioned_country_merchant', lambda x: (\n",
    "                x.get('user_country', '').upper() in ['US', 'UK', 'GB', 'DE', 'FR'] and\n",
    "                x.get('merchant_country', '').upper() in ['KP', 'IR', 'SY', 'CU', 'VE', 'RU', 'BY']\n",
    "            ), self.weights['sanctioned_country_merchant']),\n",
    "            \n",
    "            # Rule 21: Payment Method Mismatch\n",
    "            # Crypto/digital-only merchant accepting traditional card payment\n",
    "            ('payment_method_mismatch', lambda x: (\n",
    "                x.get('merchant_category', '').lower() in ['crypto', 'cryptocurrency', 'digital_wallet'] and\n",
    "                x.get('payment_method', '').lower() in ['credit_card', 'debit_card', 'card']\n",
    "            ), self.weights['payment_method_mismatch']),\n",
    "            \n",
    "            # Rule 22: Refund Before Purchase (100% fraud indicator)\n",
    "            ('refund_before_purchase', lambda x: (\n",
    "                x.get('is_refund', 0) == 1 and\n",
    "                x.get('user_total_purchase_count', 1) == 0\n",
    "            ), self.weights['refund_before_purchase']),\n",
    "            \n",
    "            # =================================================================\n",
    "            # TIER 8: IDENTITY COHERENCE RULES (Profile consistency)\n",
    "            # =================================================================\n",
    "            \n",
    "            # Rule 23: Impossible User Profile\n",
    "            # New user + failed KYC + high value first transaction\n",
    "            ('impossible_user_profile', lambda x: (\n",
    "                x.get('account_age_days', 9999) < 7 and\n",
    "                x.get('kyc_status', 'passed').lower() in ['failed', 'pending', 'not_submitted'] and\n",
    "                x.get('amount', 0) > 500\n",
    "            ), self.weights['impossible_user_profile']),\n",
    "            \n",
    "            # Rule 24: Device Fingerprint Chaos\n",
    "            # 5+ different devices AND 5+ different IPs in 24 hours\n",
    "            ('device_fingerprint_chaos', lambda x: (\n",
    "                x.get('distinct_devices_24hr', 0) >= 5 and\n",
    "                x.get('distinct_ips_24hr', 0) >= 5\n",
    "            ), self.weights['device_fingerprint_chaos']),\n",
    "            \n",
    "            # Rule 25: Email-Country Mismatch\n",
    "            # Email domain suggests different country than billing address\n",
    "            ('email_country_mismatch', lambda x: (\n",
    "                x.get('email_domain_country', '').upper() != '' and\n",
    "                x.get('billing_country', '').upper() != '' and\n",
    "                x.get('email_domain_country', '').upper() != x.get('billing_country', '').upper()\n",
    "            ), self.weights['email_country_mismatch']),\n",
    "        ]\n",
    "    \n",
    "    def evaluate(self, transaction_dict):\n",
    "        \"\"\"\n",
    "        Evaluate all rules and return risk score (0-100).\n",
    "        Uses the MAX score approach (most severe rule wins).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        transaction_dict : dict\n",
    "            Transaction features as a dictionary\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (max_score, triggered_rules)\n",
    "            - max_score: int (0-100)\n",
    "            - triggered_rules: list of (rule_name, score) tuples\n",
    "        \"\"\"\n",
    "        triggered_rules = []\n",
    "        max_score = 0\n",
    "        \n",
    "        for rule_name, rule_func, score in self.rules:\n",
    "            try:\n",
    "                if rule_func(transaction_dict):\n",
    "                    triggered_rules.append((rule_name, score))\n",
    "                    max_score = max(max_score, score)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return max_score, triggered_rules\n",
    "    \n",
    "    def evaluate_with_profile(self, transaction, user_profile):\n",
    "        \"\"\"\n",
    "        Evaluate transaction with real-time user profile enrichment.\n",
    "        This is the method from friend's implementation that uses user context.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        transaction : dict\n",
    "            Raw transaction data with keys like:\n",
    "            - device_id, country, amount, lat, lon, timestamp, hours_since_last_txn\n",
    "        \n",
    "        user_profile : dict\n",
    "            User context with keys like:\n",
    "            - known_devices: list of device IDs user has used before\n",
    "            - home_country: user's primary country\n",
    "            - avg_amount: user's average transaction amount\n",
    "            - txns_last_10_min: count of transactions in last 10 minutes\n",
    "            - last_lat, last_lon: last known location\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict with:\n",
    "            - rule_risk_score: 0-100 score\n",
    "            - risk_level: LOW/MEDIUM-LOW/MEDIUM/HIGH/CRITICAL\n",
    "            - triggered_rules: list of rule names\n",
    "        \"\"\"\n",
    "        # Enrich transaction with user profile\n",
    "        enriched = self._enrich_with_profile(transaction, user_profile)\n",
    "        \n",
    "        # Evaluate enriched transaction\n",
    "        risk_score = 0\n",
    "        triggered_rules = []\n",
    "        \n",
    "        # Rule 1: New Device (from friend's code)\n",
    "        if transaction.get(\"device_id\") not in user_profile.get(\"known_devices\", []):\n",
    "            risk_score += self.weights[\"new_device\"]\n",
    "            triggered_rules.append(\"New Device\")\n",
    "        \n",
    "        # Rule 2: New Country (from friend's code)\n",
    "        if transaction.get(\"country\") != user_profile.get(\"home_country\", \"\"):\n",
    "            risk_score += self.weights[\"new_country\"]\n",
    "            triggered_rules.append(\"New Country\")\n",
    "        \n",
    "        # Rule 3: High Amount vs User Average (from friend's code)\n",
    "        avg_amount = user_profile.get(\"avg_amount\", 0)\n",
    "        if avg_amount > 0:\n",
    "            amount_ratio = transaction.get(\"amount\", 0) / avg_amount\n",
    "            if amount_ratio > 5:\n",
    "                risk_score += self.weights[\"high_amount\"]\n",
    "                triggered_rules.append(\"High Amount (>5x avg)\")\n",
    "        \n",
    "        # Rule 4: Velocity Attack (from friend's code)\n",
    "        if user_profile.get(\"txns_last_10_min\", 0) > 5:\n",
    "            risk_score += self.weights[\"velocity_attack\"]\n",
    "            triggered_rules.append(\"Velocity Attack\")\n",
    "        \n",
    "        # Rule 5: Impossible Travel (from friend's code - uses haversine)\n",
    "        if all(k in transaction for k in ['lat', 'lon']) and \\\n",
    "           all(k in user_profile for k in ['last_lat', 'last_lon']):\n",
    "            distance = haversine(\n",
    "                transaction['lat'], transaction['lon'],\n",
    "                user_profile['last_lat'], user_profile['last_lon']\n",
    "            )\n",
    "            hours = max(transaction.get('hours_since_last_txn', 0.01), 0.01)\n",
    "            travel_speed = distance / hours\n",
    "            \n",
    "            if travel_speed > 900:  # > 900 km/h is impossible\n",
    "                risk_score += self.weights[\"impossible_travel\"]\n",
    "                triggered_rules.append(\"Impossible Travel\")\n",
    "        \n",
    "        # Rule 6: Night Transaction (from friend's code)\n",
    "        if 'timestamp' in transaction:\n",
    "            try:\n",
    "                hour = datetime.fromisoformat(str(transaction['timestamp'])).hour\n",
    "                if 0 <= hour <= 5:\n",
    "                    risk_score += self.weights[\"night_transaction\"]\n",
    "                    triggered_rules.append(\"Night Transaction\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Also run standard rule evaluation on enriched features\n",
    "        standard_score, standard_rules = self.evaluate(enriched)\n",
    "        \n",
    "        # Combine scores (take max)\n",
    "        final_score = max(risk_score, standard_score)\n",
    "        all_rules = triggered_rules + [r[0] for r in standard_rules]\n",
    "        \n",
    "        return {\n",
    "            \"rule_risk_score\": min(final_score, 100),\n",
    "            \"risk_level\": self.classify_risk(final_score),\n",
    "            \"triggered_rules\": list(set(all_rules))  # Remove duplicates\n",
    "        }\n",
    "    \n",
    "    def _enrich_with_profile(self, txn, profile):\n",
    "        \"\"\"\n",
    "        Add derived features based on user profile for standard rule evaluation.\n",
    "        \"\"\"\n",
    "        enriched = txn.copy() if isinstance(txn, dict) else {}\n",
    "        \n",
    "        # Check if device is new\n",
    "        if 'device_id' in txn and 'known_devices' in profile:\n",
    "            enriched['is_new_device'] = 1 if txn['device_id'] not in profile['known_devices'] else 0\n",
    "        \n",
    "        # Check if country is new\n",
    "        if 'country' in txn and 'home_country' in profile:\n",
    "            enriched['is_new_country'] = 1 if txn['country'] != profile['home_country'] else 0\n",
    "        \n",
    "        # Compute amount vs user average\n",
    "        if 'amount' in txn and 'avg_amount' in profile and profile['avg_amount'] > 0:\n",
    "            enriched['amount_vs_user_avg'] = txn['amount'] / profile['avg_amount']\n",
    "        \n",
    "        # Check velocity\n",
    "        if 'txns_last_10_min' in profile:\n",
    "            enriched['txns_last_10min'] = profile['txns_last_10_min']\n",
    "        \n",
    "        # Compute travel speed if location data available\n",
    "        if all(k in txn for k in ['lat', 'lon']) and \\\n",
    "           all(k in profile for k in ['last_lat', 'last_lon']):\n",
    "            distance = haversine(txn['lat'], txn['lon'], \n",
    "                                profile['last_lat'], profile['last_lon'])\n",
    "            hours = max(txn.get('hours_since_last_txn', 0.01), 0.01)\n",
    "            enriched['travel_speed_kmh'] = distance / hours\n",
    "        \n",
    "        # Check if night transaction\n",
    "        if 'timestamp' in txn:\n",
    "            try:\n",
    "                hour = datetime.fromisoformat(str(txn['timestamp'])).hour\n",
    "                enriched['is_night'] = 1 if 0 <= hour <= 5 else 0\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return enriched\n",
    "    \n",
    "    def classify_risk(self, score):\n",
    "        \"\"\"\n",
    "        Classify risk level based on cumulative score.\n",
    "        From friend's implementation.\n",
    "        \"\"\"\n",
    "        if score <= 30:\n",
    "            return \"LOW\"\n",
    "        elif score <= 50:\n",
    "            return \"MEDIUM-LOW\"\n",
    "        elif score <= 70:\n",
    "            return \"MEDIUM\"\n",
    "        elif score <= 90:\n",
    "            return \"HIGH\"\n",
    "        else:\n",
    "            return \"CRITICAL\"\n",
    "    \n",
    "    def predict_batch(self, X, feature_names):\n",
    "        \"\"\"Evaluate rules on batch of transactions (handles DataFrame or numpy array)\"\"\"\n",
    "        # Convert to numpy array if DataFrame\n",
    "        if hasattr(X, 'values'):\n",
    "            X_arr = X.values\n",
    "        else:\n",
    "            X_arr = np.asarray(X)\n",
    "        \n",
    "        scores = np.zeros(len(X_arr))\n",
    "        \n",
    "        for i in range(len(X_arr)):\n",
    "            txn_dict = dict(zip(feature_names, X_arr[i]))\n",
    "            score, _ = self.evaluate(txn_dict)\n",
    "            scores[i] = score / 100.0  # Normalize to 0-1\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def get_explanation(self, transaction_dict, user_profile=None):\n",
    "        \"\"\"\n",
    "        Get a human-readable explanation of the fraud assessment.\n",
    "        \"\"\"\n",
    "        if user_profile:\n",
    "            result = self.evaluate_with_profile(transaction_dict, user_profile)\n",
    "            score = result['rule_risk_score']\n",
    "            risk_level = result['risk_level']\n",
    "            rules = result['triggered_rules']\n",
    "        else:\n",
    "            score, rules = self.evaluate(transaction_dict)\n",
    "            risk_level = self.classify_risk(score)\n",
    "            rules = [r[0] for r in rules]\n",
    "        \n",
    "        explanation = []\n",
    "        explanation.append(f\"Risk Level: {risk_level} (Score: {score}/100)\")\n",
    "        \n",
    "        if rules:\n",
    "            explanation.append(\"Triggered Rules:\")\n",
    "            for rule in sorted(rules):\n",
    "                weight = self.weights.get(rule.lower().replace(' ', '_'), '?')\n",
    "                explanation.append(f\"  - {rule}: +{weight} points\")\n",
    "        else:\n",
    "            explanation.append(\"No fraud indicators detected.\")\n",
    "        \n",
    "        return \"\\n\".join(explanation)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BAYESIAN HIGH-RISK IDENTIFIER (Neural Network for 60-84 Zone Detection)\n",
    "# ============================================================================\n",
    "class BayesianHighRiskIdentifier:\n",
    "    \"\"\"\n",
    "    Bayesian Neural Network for High-Risk Transaction Identification\n",
    "    \n",
    "    This replaces the hardcoded 60-84 threshold range with a learned model\n",
    "    that identifies transactions needing extra rule influence (35% blend).\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: Binary rule signals (25 rules â†’ 25 features)\n",
    "    - Hidden: 2 dense layers with dropout (Bayesian approximation)\n",
    "    - Output: P(high_risk) + uncertainty via MC Dropout\n",
    "    \n",
    "    Training:\n",
    "    - Positive: Fraud transactions with rule score 30-84 (gray zone frauds)\n",
    "    - Negative: Legit transactions with rule score 30-84 (gray zone legits)\n",
    "    - Goal: Learn which rule combinations in the gray zone indicate fraud\n",
    "    \n",
    "    Inference:\n",
    "    - Returns is_high_risk (bool), confidence (0-1)\n",
    "    - High confidence â†’ trust the decision\n",
    "    - Low confidence â†’ let ML decide purely\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_rules=25, hidden_dim=16, dropout_rate=0.3):\n",
    "        self.n_rules = n_rules\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.model = None\n",
    "        self.is_fitted = False\n",
    "        self.threshold = 0.5  # Decision threshold\n",
    "        self.rule_names = []\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build a simple neural network using sklearn's MLPClassifier\"\"\"\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        \n",
    "        # MLPClassifier with similar architecture\n",
    "        # Hidden layers simulate the BNN structure\n",
    "        self.model = MLPClassifier(\n",
    "            hidden_layer_sizes=(self.hidden_dim, self.hidden_dim // 2),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.01,  # L2 regularization (Bayesian prior approximation)\n",
    "            max_iter=200,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.15,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def extract_rule_signals(self, X, feature_names, rule_engine):\n",
    "        \"\"\"\n",
    "        Extract binary rule signals from transactions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Transaction features (unscaled)\n",
    "        feature_names : list\n",
    "            Feature names matching X columns\n",
    "        rule_engine : RuleBasedEngine\n",
    "            Rule engine with defined rules\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Binary matrix (n_samples, n_rules)\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'values'):\n",
    "            X_arr = X.values\n",
    "        else:\n",
    "            X_arr = np.asarray(X)\n",
    "        \n",
    "        n_samples = len(X_arr)\n",
    "        n_rules = len(rule_engine.rules)\n",
    "        \n",
    "        # Extract binary signals for each rule\n",
    "        rule_signals = np.zeros((n_samples, n_rules))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            txn_dict = dict(zip(feature_names, X_arr[i]))\n",
    "            \n",
    "            # Check each rule\n",
    "            for j, (rule_name, rule_fn, weight) in enumerate(rule_engine.rules):\n",
    "                try:\n",
    "                    if rule_fn(txn_dict):\n",
    "                        rule_signals[i, j] = 1.0\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Store rule names for interpretability\n",
    "        self.rule_names = [r[0] for r in rule_engine.rules]\n",
    "        \n",
    "        return rule_signals\n",
    "    \n",
    "    def fit(self, X, feature_names, y, rule_engine, rule_scores=None):\n",
    "        \"\"\"\n",
    "        Train the BNN to identify high-risk transactions in the gray zone.\n",
    "        \n",
    "        Only trains on transactions with rule scores in 30-84 range\n",
    "        (the ambiguous zone where ML needs rule assistance).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Transaction features (unscaled)\n",
    "        feature_names : list\n",
    "            Feature names\n",
    "        y : np.ndarray\n",
    "            Fraud labels (0/1)\n",
    "        rule_engine : RuleBasedEngine\n",
    "            Rule engine instance\n",
    "        rule_scores : np.ndarray, optional\n",
    "            Pre-computed rule scores (0-1 normalized)\n",
    "        \"\"\"\n",
    "        print(\"\\n   ðŸ§  Training Bayesian High-Risk Identifier...\")\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "        # Get rule scores if not provided\n",
    "        if rule_scores is None:\n",
    "            rule_scores = rule_engine.predict_batch(X, feature_names)\n",
    "        \n",
    "        # Filter to gray zone (30-84, i.e., 0.30-0.84 normalized)\n",
    "        gray_zone_mask = (rule_scores >= 0.30) & (rule_scores < 0.85)\n",
    "        n_gray = gray_zone_mask.sum()\n",
    "        \n",
    "        if n_gray < 100:\n",
    "            print(f\"      âš ï¸ Only {n_gray} samples in gray zone, using full dataset\")\n",
    "            gray_zone_mask = np.ones(len(y), dtype=bool)\n",
    "            n_gray = len(y)\n",
    "        \n",
    "        # Extract rule signals for gray zone\n",
    "        rule_signals = self.extract_rule_signals(X, feature_names, rule_engine)\n",
    "        \n",
    "        X_train = rule_signals[gray_zone_mask]\n",
    "        y_train = y[gray_zone_mask]\n",
    "        \n",
    "        # Balance the training data\n",
    "        n_fraud = y_train.sum()\n",
    "        n_legit = len(y_train) - n_fraud\n",
    "        \n",
    "        print(f\"      Gray zone: {n_gray} samples ({n_fraud} fraud, {n_legit} legit)\")\n",
    "        \n",
    "        # Train the model\n",
    "        try:\n",
    "            self.model.fit(X_train, y_train)\n",
    "            self.is_fitted = True\n",
    "            \n",
    "            # ========== SELF-ADJUSTING THRESHOLD CALIBRATION ==========\n",
    "            # Goal: Find threshold that targets 20-40% high-risk rate\n",
    "            # while maximizing recall with decent precision\n",
    "            \n",
    "            train_proba = self.model.predict_proba(X_train)[:, 1]\n",
    "            \n",
    "            # DIAGNOSTIC: Check probability distribution\n",
    "            unique_probs = np.unique(train_proba)\n",
    "            print(f\"      ðŸ“Š Probability distribution: {len(unique_probs)} unique values\")\n",
    "            print(f\"         Min={train_proba.min():.3f}, Max={train_proba.max():.3f}, Median={np.median(train_proba):.3f}\")\n",
    "            \n",
    "            # If too few unique values, the model isn't discriminating well\n",
    "            if len(unique_probs) < 10:\n",
    "                print(f\"      âš ï¸ Low discrimination: only {len(unique_probs)} unique probability values\")\n",
    "            \n",
    "            # Target parameters\n",
    "            TARGET_HIGH_RISK_MIN = 0.20  # At least 20% should be high-risk (fraudish)\n",
    "            TARGET_HIGH_RISK_MAX = 0.40  # At most 40% (avoid over-boosting)\n",
    "            MIN_RECALL = 0.75  # Floor for recall\n",
    "            PRECISION_CEILING = 0.70  # Don't sacrifice too much precision\n",
    "            \n",
    "            print(f\"      ðŸ”„ Self-adjusting: Target high-risk rate 20-40%...\")\n",
    "            \n",
    "            best_score = -1\n",
    "            best_thresh = 0.5\n",
    "            best_metrics = {}\n",
    "            \n",
    "            # Search for optimal threshold\n",
    "            for thresh in np.linspace(0.3, 0.8, 51):\n",
    "                pred = (train_proba >= thresh).astype(int)\n",
    "                high_risk_rate = pred.mean()\n",
    "                \n",
    "                # Skip if outside target range\n",
    "                if high_risk_rate < TARGET_HIGH_RISK_MIN or high_risk_rate > TARGET_HIGH_RISK_MAX:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate metrics\n",
    "                rec = recall_score(y_train, pred, zero_division=0)\n",
    "                prec = precision_score(y_train, pred, zero_division=0)\n",
    "                f2 = fbeta_score(y_train, pred, beta=2, zero_division=0)\n",
    "                \n",
    "                # Score: Prioritize recall, penalize low precision\n",
    "                # Recall weight: 2.0 (fraud detection priority)\n",
    "                # Precision penalty if below ceiling\n",
    "                score = rec * 2.0 + f2\n",
    "                if prec < 0.30:  # Too many false positives\n",
    "                    score *= 0.5\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_thresh = thresh\n",
    "                    best_metrics = {\n",
    "                        'recall': rec,\n",
    "                        'precision': prec,\n",
    "                        'f2': f2,\n",
    "                        'high_risk_rate': high_risk_rate\n",
    "                    }\n",
    "            \n",
    "            # Fallback if no threshold in target range found\n",
    "            if best_score < 0:\n",
    "                print(f\"      âš ï¸ No threshold in target range, finding closest...\")\n",
    "                \n",
    "                # IMPROVED FALLBACK: Find threshold closest to 30% high-risk rate\n",
    "                # This is better than median which often gives 50%+ high-risk rate\n",
    "                TARGET_HR = 0.30  # Target 30% high-risk (middle of 20-40% range)\n",
    "                \n",
    "                # Search wider range for fallback\n",
    "                best_distance = float('inf')\n",
    "                for thresh in np.linspace(0.1, 0.95, 86):\n",
    "                    pred = (train_proba >= thresh).astype(int)\n",
    "                    high_risk_rate = pred.mean()\n",
    "                    \n",
    "                    # Find threshold closest to target\n",
    "                    distance = abs(high_risk_rate - TARGET_HR)\n",
    "                    if distance < best_distance:\n",
    "                        best_distance = distance\n",
    "                        best_thresh = thresh\n",
    "                        \n",
    "                        rec = recall_score(y_train, pred, zero_division=0)\n",
    "                        prec = precision_score(y_train, pred, zero_division=0)\n",
    "                        f2 = fbeta_score(y_train, pred, beta=2, zero_division=0)\n",
    "                        best_metrics = {\n",
    "                            'recall': rec,\n",
    "                            'precision': prec,\n",
    "                            'f2': f2,\n",
    "                            'high_risk_rate': high_risk_rate\n",
    "                        }\n",
    "                \n",
    "                # If still can't get close to target, find EXACT threshold for 30% HR rate\n",
    "                if best_distance > 0.10:  # More than 10% away from target\n",
    "                    # Sort probabilities descending and pick threshold that gives 30% HR\n",
    "                    sorted_proba = np.sort(train_proba)[::-1]  # Descending order\n",
    "                    target_count = int(len(sorted_proba) * TARGET_HR)  # Top 30%\n",
    "                    if target_count > 0 and target_count < len(sorted_proba):\n",
    "                        best_thresh = sorted_proba[target_count - 1]  # Threshold at 30% mark\n",
    "                    else:\n",
    "                        best_thresh = sorted_proba[len(sorted_proba) // 3]  # Fallback\n",
    "                    \n",
    "                    pred = (train_proba >= best_thresh).astype(int)\n",
    "                    actual_hr = pred.mean()\n",
    "                    print(f\"      âœ… Forced {TARGET_HR*100:.0f}% HR threshold={best_thresh:.3f} (actual HR={actual_hr*100:.1f}%)\")\n",
    "                best_metrics = {\n",
    "                    'recall': recall_score(y_train, pred, zero_division=0),\n",
    "                    'precision': precision_score(y_train, pred, zero_division=0),\n",
    "                    'f2': fbeta_score(y_train, pred, beta=2, zero_division=0),\n",
    "                    'high_risk_rate': pred.mean()\n",
    "                }\n",
    "            \n",
    "            self.threshold = best_thresh\n",
    "            \n",
    "            # Report calibration results\n",
    "            print(f\"      âœ… BNN calibrated (threshold={self.threshold:.3f})\")\n",
    "            print(f\"      High-risk rate: {best_metrics['high_risk_rate']*100:.1f}% (target: 20-40%)\")\n",
    "            print(f\"      Gray zone: Recall={best_metrics['recall']*100:.1f}%, Prec={best_metrics['precision']*100:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ BNN training failed: {str(e)[:50]}\")\n",
    "            self.is_fitted = False\n",
    "    \n",
    "    def predict(self, X, feature_names, rule_engine, n_samples=10):\n",
    "        \"\"\"\n",
    "        Predict high-risk probability with uncertainty estimation.\n",
    "        \n",
    "        Uses MC Dropout approximation: run multiple forward passes\n",
    "        and measure variance as uncertainty.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Transaction features (unscaled)\n",
    "        feature_names : list\n",
    "            Feature names\n",
    "        rule_engine : RuleBasedEngine\n",
    "            Rule engine instance\n",
    "        n_samples : int\n",
    "            Number of MC samples for uncertainty (default 10)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict with:\n",
    "            - is_high_risk: np.ndarray (bool)\n",
    "            - probability: np.ndarray (0-1)\n",
    "            - uncertainty: np.ndarray (0-1, higher = less confident)\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            # Fallback: use rule score range 60-84\n",
    "            return {\n",
    "                'is_high_risk': np.zeros(len(X), dtype=bool),\n",
    "                'probability': np.zeros(len(X)),\n",
    "                'uncertainty': np.ones(len(X))  # High uncertainty\n",
    "            }\n",
    "        \n",
    "        # Extract rule signals\n",
    "        rule_signals = self.extract_rule_signals(X, feature_names, rule_engine)\n",
    "        \n",
    "        # Get predictions\n",
    "        # For sklearn MLP, we can't do true MC dropout, so we use single prediction\n",
    "        # with confidence from predict_proba\n",
    "        probabilities = self.model.predict_proba(rule_signals)[:, 1]\n",
    "        \n",
    "        # Estimate uncertainty from probability (closer to 0.5 = higher uncertainty)\n",
    "        uncertainty = 1 - 2 * np.abs(probabilities - 0.5)\n",
    "        \n",
    "        # Decision\n",
    "        is_high_risk = probabilities >= self.threshold\n",
    "        \n",
    "        return {\n",
    "            'is_high_risk': is_high_risk,\n",
    "            'probability': probabilities,\n",
    "            'uncertainty': uncertainty\n",
    "        }\n",
    "    \n",
    "    def get_explanation(self, X_single, feature_names, rule_engine):\n",
    "        \"\"\"Get explanation for a single transaction's high-risk classification\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return \"BNN not trained\"\n",
    "        \n",
    "        rule_signals = self.extract_rule_signals(X_single.reshape(1, -1), feature_names, rule_engine)[0]\n",
    "        triggered_rules = [self.rule_names[i] for i in range(len(rule_signals)) if rule_signals[i] > 0]\n",
    "        \n",
    "        result = self.predict(X_single.reshape(1, -1), feature_names, rule_engine)\n",
    "        prob = result['probability'][0]\n",
    "        unc = result['uncertainty'][0]\n",
    "        \n",
    "        explanation = [\n",
    "            f\"High-Risk Probability: {prob*100:.1f}%\",\n",
    "            f\"Confidence: {(1-unc)*100:.1f}%\",\n",
    "            f\"Decision: {'HIGH RISK' if result['is_high_risk'][0] else 'NORMAL'}\",\n",
    "            f\"Triggered Rules: {', '.join(triggered_rules) if triggered_rules else 'None'}\"\n",
    "        ]\n",
    "        \n",
    "        return \"\\n\".join(explanation)\n",
    "\n",
    "\n",
    "# FRAMEWORK COMPONENT 2: AUTOENCODER ANOMALY DETECTOR\n",
    "# ============================================================================\n",
    "class AutoencoderAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Autoencoder for anomaly detection\n",
    "    Learns to compress and reconstruct normal transactions\n",
    "    High reconstruction error = anomaly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=50, encoding_dim=10):\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.threshold = None\n",
    "        self.scaler = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build simple autoencoder using sklearn's MLPRegressor\"\"\"\n",
    "        from sklearn.neural_network import MLPRegressor\n",
    "        \n",
    "        # We'll use MLPRegressor as a simple autoencoder\n",
    "        self.autoencoder = MLPRegressor(\n",
    "            hidden_layer_sizes=(self.input_dim // 2, self.encoding_dim, self.input_dim // 2),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.001,\n",
    "            max_iter=100,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def fit(self, X_normal):\n",
    "        \"\"\"Train autoencoder on normal (non-fraud) data only\"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        # Scale the data\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_normal)\n",
    "        \n",
    "        # Limit input dimension\n",
    "        if X_scaled.shape[1] > self.input_dim:\n",
    "            X_scaled = X_scaled[:, :self.input_dim]\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "        # Train autoencoder (input = output for reconstruction)\n",
    "        self.autoencoder.fit(X_scaled, X_scaled)\n",
    "        \n",
    "        # Calculate reconstruction errors for threshold\n",
    "        reconstructed = self.autoencoder.predict(X_scaled)\n",
    "        errors = np.mean((X_scaled - reconstructed) ** 2, axis=1)\n",
    "        \n",
    "        # Set threshold at 95th percentile of normal errors\n",
    "        self.threshold = np.percentile(errors, 95)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return anomaly scores (higher = more anomalous)\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            return np.zeros(len(X))\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        if X_scaled.shape[1] > self.input_dim:\n",
    "            X_scaled = X_scaled[:, :self.input_dim]\n",
    "        \n",
    "        reconstructed = self.autoencoder.predict(X_scaled)\n",
    "        errors = np.mean((X_scaled - reconstructed) ** 2, axis=1)\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        scores = np.clip(errors / (self.threshold + 1e-6), 0, 1)\n",
    "        return scores\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FRAMEWORK COMPONENT 3: GRAPH FEATURE EXTRACTOR\n",
    "# ============================================================================\n",
    "class GraphFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract graph-based features from device/IP/merchant relationships\n",
    "    Detects fraud rings and money laundering patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device_stats = {}\n",
    "        self.ip_stats = {}\n",
    "        self.merchant_stats = {}\n",
    "        self.user_device_map = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"Build graph statistics from training data\"\"\"\n",
    "        # Device statistics: how many users, fraud rate\n",
    "        if 'device_id' in df.columns:\n",
    "            device_groups = df.groupby('device_id').agg({\n",
    "                'user_id': 'nunique',\n",
    "                'is_fraud': ['sum', 'count', 'mean']\n",
    "            })\n",
    "            device_groups.columns = ['shared_users', 'fraud_count', 'txn_count', 'fraud_rate']\n",
    "            self.device_stats = device_groups.to_dict('index')\n",
    "        \n",
    "        # IP statistics (if available)\n",
    "        if 'ip_address' in df.columns:\n",
    "            ip_groups = df.groupby('ip_address').agg({\n",
    "                'user_id': 'nunique',\n",
    "                'is_fraud': ['sum', 'count', 'mean']\n",
    "            })\n",
    "            ip_groups.columns = ['shared_users', 'fraud_count', 'txn_count', 'fraud_rate']\n",
    "            self.ip_stats = ip_groups.to_dict('index')\n",
    "        \n",
    "        # Merchant statistics\n",
    "        if 'merchant_id' in df.columns:\n",
    "            merchant_groups = df.groupby('merchant_id').agg({\n",
    "                'is_fraud': ['sum', 'count', 'mean']\n",
    "            })\n",
    "            merchant_groups.columns = ['fraud_count', 'txn_count', 'fraud_rate']\n",
    "            self.merchant_stats = merchant_groups.to_dict('index')\n",
    "        \n",
    "        # User-device mapping for connected fraudster detection\n",
    "        if 'device_id' in df.columns and 'user_id' in df.columns:\n",
    "            fraud_devices = set(df[df['is_fraud'] == 1]['device_id'].unique())\n",
    "            self.fraud_devices = fraud_devices\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Add graph features to dataframe\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            # Default values if not fitted\n",
    "            df['device_shared_users'] = 0\n",
    "            df['device_fraud_rate'] = 0\n",
    "            df['ip_shared_users'] = 0\n",
    "            df['ip_fraud_rate'] = 0\n",
    "            df['merchant_fraud_rate'] = 0\n",
    "            df['uses_fraud_device'] = 0\n",
    "            return df\n",
    "        \n",
    "        # Device features\n",
    "        if 'device_id' in df.columns and self.device_stats:\n",
    "            df['device_shared_users'] = df['device_id'].map(\n",
    "                lambda x: self.device_stats.get(x, {}).get('shared_users', 0)\n",
    "            ).fillna(0)\n",
    "            df['device_fraud_rate'] = df['device_id'].map(\n",
    "                lambda x: self.device_stats.get(x, {}).get('fraud_rate', 0)\n",
    "            ).fillna(0)\n",
    "            df['device_fraud_count'] = df['device_id'].map(\n",
    "                lambda x: self.device_stats.get(x, {}).get('fraud_count', 0)\n",
    "            ).fillna(0)\n",
    "            # Flag if device was used in fraud before\n",
    "            df['uses_fraud_device'] = df['device_id'].isin(\n",
    "                getattr(self, 'fraud_devices', set())\n",
    "            ).astype(int)\n",
    "        else:\n",
    "            df['device_shared_users'] = 0\n",
    "            df['device_fraud_rate'] = 0\n",
    "            df['device_fraud_count'] = 0\n",
    "            df['uses_fraud_device'] = 0\n",
    "        \n",
    "        # IP features\n",
    "        if 'ip_address' in df.columns and self.ip_stats:\n",
    "            df['ip_shared_users'] = df['ip_address'].map(\n",
    "                lambda x: self.ip_stats.get(x, {}).get('shared_users', 0)\n",
    "            ).fillna(0)\n",
    "            df['ip_fraud_rate'] = df['ip_address'].map(\n",
    "                lambda x: self.ip_stats.get(x, {}).get('fraud_rate', 0)\n",
    "            ).fillna(0)\n",
    "        else:\n",
    "            df['ip_shared_users'] = 0\n",
    "            df['ip_fraud_rate'] = 0\n",
    "        \n",
    "        # Merchant features\n",
    "        if 'merchant_id' in df.columns and self.merchant_stats:\n",
    "            df['merchant_fraud_rate'] = df['merchant_id'].map(\n",
    "                lambda x: self.merchant_stats.get(x, {}).get('fraud_rate', 0)\n",
    "            ).fillna(0)\n",
    "        else:\n",
    "            df['merchant_fraud_rate'] = 0\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RISK STRATIFICATION MATRIX\n",
    "# ============================================================================\n",
    "def get_risk_tier(score):\n",
    "    \"\"\"Convert risk score (0-100) to risk tier and action\"\"\"\n",
    "    if score <= 30:\n",
    "        return 'LOW', 'Auto-approve', 'ðŸŸ¢'\n",
    "    elif score <= 50:\n",
    "        return 'MEDIUM-LOW', 'Enhanced monitoring', 'ðŸŸ¡'\n",
    "    elif score <= 70:\n",
    "        return 'MEDIUM', 'Add to watchlist', 'ðŸŸ '\n",
    "    elif score <= 90:\n",
    "        return 'HIGH', 'Manual review', 'ðŸ”´'\n",
    "    else:\n",
    "        return 'CRITICAL', 'Instant block', 'â›”'\n",
    "\n",
    "\n",
    "class UltimateFraudDetectionEngine:\n",
    "    \"\"\"\n",
    "    ULTIMATE Fraud Detection System combining:\n",
    "    - 150+ Advanced Features\n",
    "    - Multi-Model Ensemble (XGBoost + LightGBM + CatBoost)\n",
    "    - Graph Network Analysis\n",
    "    - Bayesian Optimization with Multi-Objective Search\n",
    "    - Adversarial Validation\n",
    "    - Calibrated Predictions\n",
    "    - SHAP Explainability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        print(f\"ðŸ“‚ Loading data from {data_path}\")\n",
    "        # Note: Depending on file type, you might need pd.read_csv or pd.read_excel\n",
    "        if data_path.endswith('.csv'):\n",
    "            try:\n",
    "                self.df = pd.read_csv(data_path)\n",
    "            except:\n",
    "                print(\"   âš ï¸ Using Python engine for CSV parsing...\")\n",
    "                self.df = pd.read_csv(data_path, engine='python')\n",
    "        else:\n",
    "            self.df = pd.read_excel(data_path)\n",
    "        \n",
    "        # Force datetime conversion\n",
    "        date_columns = ['timestamp', 'user_signup_date']\n",
    "        for col in date_columns:\n",
    "            if col in self.df.columns:\n",
    "                try:\n",
    "                    self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "                    print(f\"   âœ“ Converted {col} to datetime\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš  Warning: {col}: {e}\")\n",
    "        \n",
    "        # Sort by timestamp for time-aware features\n",
    "        if 'timestamp' in self.df.columns:\n",
    "            self.df = self.df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Initialize storage\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "        self.y_test = None\n",
    "        self.X_train_scaled = None\n",
    "        self.X_val_scaled = None\n",
    "        self.X_test_scaled = None\n",
    "        self.feature_names = None\n",
    "        self.scaler = None\n",
    "        self.models = {}  # Ensemble of models\n",
    "        self.optimal_threshold = 0.5\n",
    "        self.feature_importance = None\n",
    "        self.anomaly_scores = None\n",
    "        self.graph_features = None\n",
    "        \n",
    "        print(f\"âœ… Loaded: {self.df.shape}\")\n",
    "        if 'is_fraud' in self.df.columns:\n",
    "            print(f\"ðŸ’° Fraud rate: {self.df['is_fraud'].mean()*100:.2f}%\")\n",
    "    \n",
    "    def calculate_haversine_distance(self, lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Haversine distance in km\"\"\"\n",
    "        R = 6371.0\n",
    "        try:\n",
    "            lat1, lon1, lat2, lon2 = map(radians, [float(lat1), float(lon1), float(lat2), float(lon2)])\n",
    "            dlon = lon2 - lon1\n",
    "            dlat = lat2 - lat1\n",
    "            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "            c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "            return R * c\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def create_graph_features(self, df):\n",
    "        \"\"\"\n",
    "        ADVANCED GRAPH/NETWORK FEATURES\n",
    "        Creates bipartite graph: Users <-> Devices/IPs/Merchants\n",
    "        \"\"\"\n",
    "        if not NETWORKX_AVAILABLE:\n",
    "            return df\n",
    "        \n",
    "        print(\"   ðŸ•¸ï¸ Creating Graph Network features...\")\n",
    "        \n",
    "        # User-Device Graph - Optimized with vectorized operations\n",
    "        if 'user_id' in df.columns and 'device_id' in df.columns:\n",
    "            # Create edge dataframe for vectorized graph construction\n",
    "            edge_df = pd.DataFrame({\n",
    "                'source': 'U_' + df['user_id'].astype(str),\n",
    "                'target': 'D_' + df['device_id'].astype(str)\n",
    "            })\n",
    "            G_device = nx.from_pandas_edgelist(edge_df, 'source', 'target')\n",
    "            \n",
    "            # Degree centrality\n",
    "            centrality = nx.degree_centrality(G_device)\n",
    "            df['user_device_centrality'] = df['user_id'].apply(lambda x: centrality.get(f\"U_{x}\", 0))\n",
    "            \n",
    "            # Clustering coefficient\n",
    "            clustering = nx.clustering(G_device)\n",
    "            df['user_device_clustering'] = df['user_id'].apply(lambda x: clustering.get(f\"U_{x}\", 0))\n",
    "        \n",
    "        # User-IP Graph\n",
    "        if 'user_id' in df.columns and 'ip_address' in df.columns:\n",
    "            ip_counts = df.groupby('ip_address')['user_id'].nunique().to_dict()\n",
    "            df['users_per_ip'] = df['ip_address'].map(ip_counts).fillna(1)\n",
    "            df['is_suspicious_ip'] = (df['users_per_ip'] > 10).astype(int)\n",
    "        \n",
    "        # User-Merchant Graph\n",
    "        if 'user_id' in df.columns and 'merchant_id' in df.columns:\n",
    "            merchant_diversity = df.groupby('user_id')['merchant_id'].nunique().to_dict()\n",
    "            df['user_merchant_diversity'] = df['user_id'].map(merchant_diversity).fillna(1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def advanced_feature_engineering_ultimate(self):\n",
    "        \"\"\"\n",
    "        ULTIMATE FEATURE ENGINEERING - 150+ Features\n",
    "        Combines best from both systems + new innovations\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸ”§ ULTIMATE Feature Engineering (150+ features)...\")\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # ============ 1. TEMPORAL FEATURES (40+) ============\n",
    "        print(\"   â° Temporal features...\")\n",
    "        if 'timestamp' in df.columns:\n",
    "            # Basic time components\n",
    "            df['hour'] = df['timestamp'].dt.hour\n",
    "            df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "            df['day_of_month'] = df['timestamp'].dt.day\n",
    "            df['month'] = df['timestamp'].dt.month\n",
    "            df['year'] = df['timestamp'].dt.year\n",
    "            df['quarter'] = df['timestamp'].dt.quarter\n",
    "            df['week_of_year'] = df['timestamp'].dt.isocalendar().week\n",
    "            df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "            df['is_month_start'] = df['timestamp'].dt.is_month_start.astype(int)\n",
    "            df['is_month_end'] = df['timestamp'].dt.is_month_end.astype(int)\n",
    "            \n",
    "            # Advanced time categories\n",
    "            df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 5)).astype(int)\n",
    "            df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)\n",
    "            df['is_late_night'] = ((df['hour'] >= 23) | (df['hour'] <= 3)).astype(int)\n",
    "            df['is_peak_hours'] = ((df['hour'] >= 10) & (df['hour'] <= 14)).astype(int)\n",
    "            \n",
    "            # Cyclical encoding (sine/cosine)\n",
    "            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "            df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "            df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['day_of_month_sin'] = np.sin(2 * np.pi * df['day_of_month'] / 30)\n",
    "            df['day_of_month_cos'] = np.cos(2 * np.pi * df['day_of_month'] / 30)\n",
    "            \n",
    "            # Time since epoch (normalized)\n",
    "            df['timestamp_epoch'] = df['timestamp'].astype(np.int64) // 10**9\n",
    "            df['timestamp_normalized'] = (df['timestamp_epoch'] - df['timestamp_epoch'].min()) / \\\n",
    "                                         (df['timestamp_epoch'].max() - df['timestamp_epoch'].min() + 1)\n",
    "        \n",
    "        # User tenure\n",
    "        if 'timestamp' in df.columns and 'user_signup_date' in df.columns:\n",
    "            df['user_tenure_days'] = (df['timestamp'] - df['user_signup_date']).dt.total_seconds() / 86400\n",
    "            df['user_tenure_days'] = df['user_tenure_days'].fillna(0).clip(0, 10000)\n",
    "            df['user_tenure_weeks'] = df['user_tenure_days'] / 7\n",
    "            df['user_tenure_months'] = df['user_tenure_days'] / 30\n",
    "            df['user_tenure_log'] = np.log1p(df['user_tenure_days'])\n",
    "            df['user_tenure_sqrt'] = np.sqrt(df['user_tenure_days'])\n",
    "            df['is_new_user'] = (df['user_tenure_days'] < 30).astype(int)\n",
    "            df['is_very_new_user'] = (df['user_tenure_days'] < 7).astype(int)\n",
    "            df['is_established_user'] = (df['user_tenure_days'] > 365).astype(int)\n",
    "        \n",
    "        # ============ 2. AMOUNT FEATURES (30+) ============\n",
    "        print(\"   ðŸ’µ Amount features...\")\n",
    "        if 'amount' in df.columns:\n",
    "            df['amount_log'] = np.log1p(df['amount'])\n",
    "            df['amount_sqrt'] = np.sqrt(df['amount'])\n",
    "            df['amount_squared'] = df['amount'] ** 2\n",
    "            df['amount_cubed'] = df['amount'] ** 3\n",
    "            \n",
    "            # Statistical transformations\n",
    "            df['amount_percentile'] = df['amount'].rank(pct=True)\n",
    "            df['amount_percentile_bin'] = pd.cut(df['amount_percentile'], bins=10, labels=False)\n",
    "            \n",
    "            # Multiple binning strategies\n",
    "            df['amount_range_fine'] = pd.cut(df['amount'], \n",
    "                bins=[0, 25, 50, 100, 150, 200, 300, 500, 750, 1000, float('inf')],\n",
    "                labels=range(10)).astype(float)\n",
    "            \n",
    "            # Round number detection\n",
    "            df['is_round_amount'] = (df['amount'] % 10 == 0).astype(int)\n",
    "            df['is_very_round_amount'] = (df['amount'] % 100 == 0).astype(int)\n",
    "            df['is_ultra_round_amount'] = (df['amount'] % 1000 == 0).astype(int)\n",
    "            \n",
    "            # Global statistics\n",
    "            df['amount_vs_global_mean'] = df['amount'] / (df['amount'].mean() + 1e-6)\n",
    "            df['amount_vs_global_median'] = df['amount'] / (df['amount'].median() + 1e-6)\n",
    "            df['amount_zscore_global'] = (df['amount'] - df['amount'].mean()) / (df['amount'].std() + 1e-6)\n",
    "            \n",
    "            # Rolling statistics (time-aware)\n",
    "            if 'timestamp' in df.columns:\n",
    "                df['amount_rolling_mean_10'] = df['amount'].rolling(window=10, min_periods=1).mean()\n",
    "                df['amount_rolling_std_10'] = df['amount'].rolling(window=10, min_periods=1).std().fillna(0)\n",
    "                df['amount_vs_rolling_mean'] = df['amount'] / (df['amount_rolling_mean_10'] + 1e-6)\n",
    "        \n",
    "        # ============ 3. USER BEHAVIORAL FEATURES (50+) ============\n",
    "        print(\"   ðŸ‘¤ User behavioral features...\")\n",
    "        \n",
    "        required_cols = ['user_total_txn_amount', 'user_total_txn_count', 'user_past_fraud_count', 'days_since_last_txn']\n",
    "        for col in required_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "\n",
    "        df['user_avg_txn_amount'] = df['user_total_txn_amount'] / (df['user_total_txn_count'] + 1)\n",
    "        df['user_txn_frequency'] = 1 / (df['days_since_last_txn'] + 1)\n",
    "        \n",
    "        if 'amount' in df.columns:\n",
    "            df['amount_vs_user_avg'] = df['amount'] / (df['user_avg_txn_amount'] + 1e-6)\n",
    "            df['amount_deviation_user'] = np.abs(df['amount'] - df['user_avg_txn_amount'])\n",
    "            \n",
    "            # Per-user advanced statistics\n",
    "            user_stats = df.groupby('user_id').agg({\n",
    "                'amount': ['mean', 'std', 'min', 'max', 'median', 'sum', 'count', \n",
    "                          lambda x: x.quantile(0.25), lambda x: x.quantile(0.75), \n",
    "                          lambda x: skew(x, nan_policy='omit'), \n",
    "                          lambda x: kurtosis(x, nan_policy='omit')]\n",
    "            }).fillna(0)\n",
    "            user_stats.columns = ['user_amount_mean', 'user_amount_std', 'user_amount_min', \n",
    "                                  'user_amount_max', 'user_amount_median', 'user_amount_sum', \n",
    "                                  'user_txn_count_new', 'user_amount_q25', 'user_amount_q75',\n",
    "                                  'user_amount_skew', 'user_amount_kurt']\n",
    "            df = df.merge(user_stats, on='user_id', how='left')\n",
    "            \n",
    "            # Z-scores and IQR\n",
    "            df['amount_zscore'] = (df['amount'] - df['user_amount_mean']) / (df['user_amount_std'] + 1e-6)\n",
    "            df['amount_zscore'] = df['amount_zscore'].fillna(0).clip(-10, 10)\n",
    "            df['amount_zscore_abs'] = np.abs(df['amount_zscore'])\n",
    "            df['amount_is_outlier'] = (df['amount_zscore_abs'] > 3).astype(int)\n",
    "            \n",
    "            # IQR-based outlier detection\n",
    "            df['user_iqr'] = df['user_amount_q75'] - df['user_amount_q25']\n",
    "            df['amount_iqr_outlier'] = ((df['amount'] < (df['user_amount_q25'] - 1.5 * df['user_iqr'])) | \n",
    "                                     (df['amount'] > (df['user_amount_q75'] + 1.5 * df['user_iqr']))).astype(int)\n",
    "            \n",
    "            # Coefficient of variation\n",
    "            df['user_amount_cv'] = df['user_amount_std'] / (df['user_amount_mean'] + 1e-6)\n",
    "        \n",
    "        # Fraud history\n",
    "        df['user_fraud_ratio'] = df['user_past_fraud_count'] / (df['user_total_txn_count'] + 1)\n",
    "        df['user_has_fraud_history'] = (df['user_past_fraud_count'] > 0).astype(int)\n",
    "        df['user_fraud_count_log'] = np.log1p(df['user_past_fraud_count'])\n",
    "        \n",
    "        # Advanced velocity features\n",
    "        if 'user_id' in df.columns and 'timestamp' in df.columns:\n",
    "            df['time_since_prev_txn'] = df.groupby('user_id')['timestamp'].diff().dt.total_seconds().fillna(99999)\n",
    "            df['is_rapid_txn'] = (df['time_since_prev_txn'] < 60).astype(int)\n",
    "            df['is_burst_txn'] = (df['time_since_prev_txn'] < 10).astype(int)\n",
    "            df['time_since_prev_log'] = np.log1p(df['time_since_prev_txn'])\n",
    "            \n",
    "            # Running velocity metrics\n",
    "            df['user_running_velocity'] = df.groupby('user_id')['time_since_prev_txn'].transform(\n",
    "                lambda x: x.rolling(5, min_periods=1).mean()\n",
    "            )\n",
    "        \n",
    "        # ============ 4. LOCATION FEATURES (40+) ============\n",
    "        print(\"   ðŸŒ Location features...\")\n",
    "        \n",
    "        loc_cols = ['billing_country', 'shipping_country', 'user_country', 'merchant_country']\n",
    "        for col in loc_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 'UNKNOWN'\n",
    "\n",
    "        # Country matches\n",
    "        df['billing_shipping_match'] = (df['billing_country'] == df['shipping_country']).astype(int)\n",
    "        df['user_billing_match'] = (df['user_country'] == df['billing_country']).astype(int)\n",
    "        df['user_merchant_match'] = (df['user_country'] == df['merchant_country']).astype(int)\n",
    "        df['all_countries_match'] = ((df['billing_shipping_match']) & \n",
    "                                     (df['user_billing_match']) & \n",
    "                                     (df['user_merchant_match'])).astype(int)\n",
    "        \n",
    "        df['country_mismatch_count'] = 4 - (df['billing_shipping_match'] + \n",
    "                                            df['user_billing_match'] + \n",
    "                                            df['user_merchant_match'] + \n",
    "                                            (df['billing_country'] == df['merchant_country']).astype(int))\n",
    "        \n",
    "        # High-risk countries\n",
    "        high_risk_countries = ['BR', 'NG', 'RU', 'CN', 'VE', 'IR', 'KP', 'SD', 'SY', 'PK']\n",
    "        for col in ['billing_country', 'shipping_country', 'merchant_country', 'user_country']:\n",
    "            df[f'is_high_risk_{col}'] = df[col].isin(high_risk_countries).astype(int)\n",
    "        \n",
    "        df['high_risk_country_count'] = sum([df[f'is_high_risk_{col}'] for col in \n",
    "                                             ['billing_country', 'shipping_country', \n",
    "                                              'merchant_country', 'user_country']])\n",
    "        \n",
    "        # Country-level statistics\n",
    "        country_stats = df.groupby('billing_country').agg({\n",
    "            'amount': ['mean', 'median', 'std'],\n",
    "            'is_fraud': ['mean', 'sum', 'count']\n",
    "        }).fillna(0)\n",
    "        country_stats.columns = ['country_avg_amount', 'country_median_amount', \n",
    "                                 'country_std_amount', 'country_fraud_rate', \n",
    "                                 'country_fraud_count', 'country_txn_count']\n",
    "        df = df.merge(country_stats, on='billing_country', how='left')\n",
    "        \n",
    "        df['amount_vs_country_avg'] = df['amount'] / (df['country_avg_amount'] + 1e-6)\n",
    "        df['country_is_high_fraud'] = (df['country_fraud_rate'] > 0.05).astype(int)\n",
    "        \n",
    "        # ============ 5. MERCHANT FEATURES (25+) ============\n",
    "        print(\"   ðŸª Merchant features...\")\n",
    "        \n",
    "        if 'merchant_id' in df.columns:\n",
    "            merchant_stats = df.groupby('merchant_id').agg({\n",
    "                'amount': ['mean', 'std', 'min', 'max', 'median', 'count'],\n",
    "                'is_fraud': ['mean', 'sum']\n",
    "            }).fillna(0)\n",
    "            merchant_stats.columns = ['merchant_avg_amount', 'merchant_std_amount', \n",
    "                                      'merchant_min_amount', 'merchant_max_amount',\n",
    "                                      'merchant_median_amount', 'merchant_txn_count',\n",
    "                                      'merchant_fraud_rate', 'merchant_fraud_count']\n",
    "            df = df.merge(merchant_stats, on='merchant_id', how='left')\n",
    "            \n",
    "            df['amount_vs_merchant_avg'] = df['amount'] / (df['merchant_avg_amount'] + 1e-6)\n",
    "            df['merchant_risk_score'] = df['merchant_fraud_rate'] * 100\n",
    "            df['merchant_is_high_risk'] = (df['merchant_fraud_rate'] > 0.1).astype(int)\n",
    "            \n",
    "            # Merchant Z-score\n",
    "            df['amount_zscore_merchant'] = ((df['amount'] - df['merchant_avg_amount']) / \n",
    "                                            (df['merchant_std_amount'] + 1e-6)).fillna(0).clip(-10, 10)\n",
    "        \n",
    "        # ============ 6. DEVICE & CHANNEL FEATURES (20+) ============\n",
    "        print(\"   ðŸ“± Device & Channel features...\")\n",
    "        \n",
    "        if 'device_type' in df.columns:\n",
    "            df['is_mobile'] = (df['device_type'] == 'mobile').astype(int)\n",
    "            df['is_desktop'] = (df['device_type'] == 'desktop').astype(int)\n",
    "        \n",
    "        if 'channel' in df.columns:\n",
    "            channel_stats = df.groupby('channel').agg({\n",
    "                'amount': ['mean', 'median'],\n",
    "                'is_fraud': 'mean'\n",
    "            }).fillna(0)\n",
    "            channel_stats.columns = ['channel_avg_amount', 'channel_median_amount', 'channel_fraud_rate']\n",
    "            df = df.merge(channel_stats, on='channel', how='left')\n",
    "            \n",
    "            df['amount_vs_channel_avg'] = df['amount'] / (df['channel_avg_amount'] + 1e-6)\n",
    "            df['is_high_risk_channel'] = (df['channel_fraud_rate'] > 0.05).astype(int)\n",
    "        \n",
    "        # ============ 7. GRAPH NETWORK FEATURES (10+) ============\n",
    "        df = self.create_graph_features(df)\n",
    "        \n",
    "        # ============ 8. RISK FLAGS & COMPOSITE SCORES (30+) ============\n",
    "        print(\"   âš ï¸ Risk flags & composite scores...\")\n",
    "        \n",
    "        if 'kyc_status' in df.columns:\n",
    "            df['kyc_risk_flag'] = (df['kyc_status'] != 'verified').astype(int)\n",
    "        \n",
    "        # Multi-factor risk score\n",
    "        df['composite_risk_score'] = (\n",
    "            df.get('high_risk_country_count', 0) * 2 +\n",
    "            df.get('kyc_risk_flag', 0) * 3 +\n",
    "            df.get('user_fraud_ratio', 0) * 10 +\n",
    "            df.get('is_rapid_txn', 0) * 2 +\n",
    "            df.get('is_night', 0) * 1 +\n",
    "            (df['amount'] > df['amount'].quantile(0.9)).astype(int) * 2\n",
    "        )\n",
    "        \n",
    "        # Suspicious patterns\n",
    "        df['suspicious_new_user_high_amount'] = (\n",
    "            (df.get('user_tenure_days', 999) < 30) * (df['amount'] > df['amount'].quantile(0.75)).astype(int)\n",
    "        )\n",
    "        \n",
    "        df['night_international_high_amount'] = (\n",
    "            df.get('is_night', 0) * (df.get('country_mismatch_count', 0) > 0) *\n",
    "            (df['amount'] > df['amount'].quantile(0.7)).astype(int)\n",
    "        )\n",
    "        \n",
    "        # ============ 9. ENHANCED FEATURES FOR BALANCE (NEW) ============\n",
    "        print(\"   ðŸŽ¯ Enhanced balance features...\")\n",
    "        \n",
    "        # === 9.1 Percentile-Based Features ===\n",
    "        if 'amount' in df.columns and 'user_id' in df.columns:\n",
    "            # Percentile rank of amount within user's transactions\n",
    "            df['amount_user_percentile'] = df.groupby('user_id')['amount'].rank(pct=True)\n",
    "            \n",
    "            # Is amount in user's top 5%, 10%, 25%?\n",
    "            df['amount_user_top5pct'] = (df['amount_user_percentile'] > 0.95).astype(int)\n",
    "            df['amount_user_top10pct'] = (df['amount_user_percentile'] > 0.90).astype(int)\n",
    "            df['amount_user_top25pct'] = (df['amount_user_percentile'] > 0.75).astype(int)\n",
    "        \n",
    "        # === 9.2 Enhanced Behavioral Stability ===\n",
    "        if 'user_amount_std' in df.columns and 'user_amount_mean' in df.columns:\n",
    "            # Coefficient of variation (already exists, but add more)\n",
    "            df['user_spending_stability'] = 1 / (df['user_amount_cv'] + 1)  # Higher = more stable\n",
    "            \n",
    "            # Amount range relative to mean\n",
    "            if 'user_amount_max' in df.columns and 'user_amount_min' in df.columns:\n",
    "                df['user_amount_range_ratio'] = ((df['user_amount_max'] - df['user_amount_min']) / \n",
    "                                                 (df['user_amount_mean'] + 1e-6))\n",
    "        \n",
    "        # === 9.3 Enhanced Domain-Specific Risk Signals ===\n",
    "        if 'amount' in df.columns:\n",
    "            # Just-below-threshold amounts (common fraud pattern)\n",
    "            df['is_just_below_100'] = ((df['amount'] >= 95) & (df['amount'] < 100)).astype(int)\n",
    "            df['is_just_below_500'] = ((df['amount'] >= 490) & (df['amount'] < 500)).astype(int)\n",
    "            df['is_just_below_1000'] = ((df['amount'] >= 990) & (df['amount'] < 1000)).astype(int)\n",
    "            df['is_just_below_threshold'] = (df['is_just_below_100'] | \n",
    "                                             df['is_just_below_500'] | \n",
    "                                             df['is_just_below_1000']).astype(int)\n",
    "            \n",
    "            # Amount spike detection (more sensitive)\n",
    "            if 'user_amount_mean' in df.columns and 'user_amount_std' in df.columns:\n",
    "                df['amount_spike_2std'] = (df['amount'] > (df['user_amount_mean'] + 2 * df['user_amount_std'])).astype(int)\n",
    "                df['amount_spike_3std'] = (df['amount'] > (df['user_amount_mean'] + 3 * df['user_amount_std'])).astype(int)\n",
    "        \n",
    "        # === 9.4 First-Transaction Flags ===\n",
    "        if 'merchant_id' in df.columns and 'user_id' in df.columns:\n",
    "            # First transaction with this merchant for this user\n",
    "            df['user_merchant_pair'] = df['user_id'].astype(str) + '_' + df['merchant_id'].astype(str)\n",
    "            df['is_first_merchant_txn'] = (~df.duplicated(subset='user_merchant_pair', keep='first')).astype(int)\n",
    "            \n",
    "        # ============ 10. ENHANCED FEATURE INTERACTIONS ============\n",
    "        print(\"   ðŸ”— Feature interactions...\")\n",
    "        \n",
    "        if 'amount' in df.columns and 'user_tenure_days' in df.columns:\n",
    "            df['amount_x_tenure'] = df['amount'] * df['user_tenure_days']\n",
    "            df['amount_div_tenure'] = df['amount'] / (df['user_tenure_days'] + 1)\n",
    "        \n",
    "        if 'merchant_fraud_rate' in df.columns and 'country_fraud_rate' in df.columns:\n",
    "            df['merchant_x_country_risk'] = df['merchant_fraud_rate'] * df['country_fraud_rate']\n",
    "        \n",
    "        # === NEW: High-Impact Interactions ===\n",
    "        # Amount Ã— user fraud history (powerful interaction)\n",
    "        if 'amount' in df.columns and 'user_fraud_ratio' in df.columns:\n",
    "            df['amount_x_fraud_ratio'] = df['amount'] * df['user_fraud_ratio']\n",
    "            df['amount_x_has_fraud_hist'] = df['amount'] * df.get('user_has_fraud_history', 0)\n",
    "        \n",
    "        # New user Ã— high risk indicators\n",
    "        if 'is_new_user' in df.columns:\n",
    "            df['new_user_x_high_risk_country'] = df['is_new_user'] * df.get('high_risk_country_count', 0)\n",
    "            df['new_user_x_night'] = df['is_new_user'] * df.get('is_night', 0)\n",
    "            df['new_user_x_country_mismatch'] = df['is_new_user'] * df.get('country_mismatch_count', 0)\n",
    "            df['new_user_x_high_amount'] = df['is_new_user'] * (df['amount'] > df['amount'].quantile(0.75)).astype(int)\n",
    "        \n",
    "        # Night Ã— country mismatch (common fraud pattern)\n",
    "        if 'is_night' in df.columns and 'country_mismatch_count' in df.columns:\n",
    "            df['night_x_country_mismatch'] = df['is_night'] * df['country_mismatch_count']\n",
    "        \n",
    "        # Amount zscore Ã— tenure (new users with unusual amounts)\n",
    "        if 'amount_zscore' in df.columns and 'user_tenure_days' in df.columns:\n",
    "            df['zscore_div_tenure'] = df['amount_zscore'] / (df['user_tenure_log'] + 1)\n",
    "        \n",
    "        # ============ 11. PATTERN-ALIGNED FEATURES (NEW - Maps to Dataset Fraud Patterns) ============\n",
    "        print(\"   ðŸŽ¯ Pattern-aligned features (matching fraud injection logic)...\")\n",
    "        \n",
    "        # === Pattern 10 & 14: New User / First Transaction Detection ===\n",
    "        if 'user_signup_date' in df.columns and 'timestamp' in df.columns:\n",
    "            try:\n",
    "                # Convert if needed\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df['user_signup_date']):\n",
    "                    df['user_signup_date'] = pd.to_datetime(df['user_signup_date'], errors='coerce')\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "                    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "                \n",
    "                # Days since signup\n",
    "                df['days_since_signup'] = (df['timestamp'] - df['user_signup_date']).dt.days.fillna(365)\n",
    "                df['is_very_new_user'] = (df['days_since_signup'] < 7).astype(int)\n",
    "                df['is_new_user_7_30'] = ((df['days_since_signup'] >= 7) & (df['days_since_signup'] < 30)).astype(int)\n",
    "                \n",
    "                # Pattern 10: New user + high value (fraud injection: days < 7 AND amount > 500)\n",
    "                df['is_new_user_high_value'] = (\n",
    "                    (df['days_since_signup'] < 7) & (df['amount'] > 500)\n",
    "                ).astype(int)\n",
    "            except Exception as e:\n",
    "                print(f\"      âš ï¸ Days since signup calculation failed: {e}\")\n",
    "        \n",
    "        # Pattern 14: First transaction ever (fraud injection: total_transactions == 0 AND amount > 1000)\n",
    "        if 'user_total_txn_count' in df.columns:\n",
    "            df['is_first_ever_txn'] = (df['user_total_txn_count'] == 0).astype(int)\n",
    "            df['is_first_txn_high_amount'] = (\n",
    "                (df['user_total_txn_count'] == 0) & (df['amount'] > 1000)\n",
    "            ).astype(int)\n",
    "            df['is_first_txn_medium_amount'] = (\n",
    "                (df['user_total_txn_count'] == 0) & (df['amount'] > 500)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # === Pattern 13: Late Night High Value ===\n",
    "        # Fraud injection: 2 <= hour <= 5 AND amount > 2000\n",
    "        if 'hour' in df.columns:\n",
    "            df['is_late_night'] = ((df['hour'] >= 2) & (df['hour'] <= 5)).astype(int)\n",
    "            df['is_late_night_high_value'] = (\n",
    "                ((df['hour'] >= 2) & (df['hour'] <= 5)) & (df['amount'] > 2000)\n",
    "            ).astype(int)\n",
    "            df['is_late_night_medium_value'] = (\n",
    "                ((df['hour'] >= 2) & (df['hour'] <= 5)) & (df['amount'] > 1000)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # === Pattern 8: Geo Mismatch Combined ===\n",
    "        # Fraud injection: billing_country IN HIGH_RISK AND billing_country != user_country\n",
    "        if 'is_high_risk_billing_country' in df.columns and 'user_billing_match' in df.columns:\n",
    "            df['is_geo_mismatch_fraud'] = (\n",
    "                (df['is_high_risk_billing_country'] == 1) & \n",
    "                (df['user_billing_match'] == 0)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # === Pattern 1: Stolen Card ===\n",
    "        # Fraud injection: amount > 800 AND billing_country IN HIGH_RISK AND NOT in user's visited countries\n",
    "        if 'is_high_risk_billing_country' in df.columns:\n",
    "            df['is_stolen_card_pattern'] = (\n",
    "                (df['amount'] > 800) & \n",
    "                (df.get('is_high_risk_billing_country', 0) == 1) &\n",
    "                (df.get('user_billing_match', 1) == 0)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # === Pattern 16: Micro Testing ===\n",
    "        # Fraud injection: amount < 5 AND total_transactions < 3\n",
    "        if 'user_total_txn_count' in df.columns:\n",
    "            df['is_micro_txn'] = (df['amount'] < 5).astype(int)\n",
    "            df['is_micro_new_user'] = (\n",
    "                (df['amount'] < 5) & (df['user_total_txn_count'] < 3)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # === Pattern 3 & 15: High-Risk Merchant Categories ===\n",
    "        # Fraud injection: category IN [Gift Cards, Crypto, Money Transfer, Gambling]\n",
    "        if 'merchant_category' in df.columns:\n",
    "            high_risk_cats = ['Gift Cards', 'Crypto', 'Money Transfer', 'Gambling', 'Adult']\n",
    "            df['is_high_risk_merchant_cat'] = df['merchant_category'].isin(high_risk_cats).astype(int)\n",
    "            \n",
    "            # Pattern 15: Gift card + new user or high amount\n",
    "            df['is_gift_card_risky'] = (\n",
    "                (df['merchant_category'].isin(['Gift Cards', 'Crypto', 'Money Transfer'])) &\n",
    "                ((df['amount'] > 300) | (df.get('user_total_txn_count', 100) < 5))\n",
    "            ).astype(int)\n",
    "        \n",
    "        # === Pattern 12: Device Sharing ===\n",
    "        # Fraud injection: len(device.users_sharing) > 3\n",
    "        if 'device_id' in df.columns and 'user_id' in df.columns:\n",
    "            device_user_counts = df.groupby('device_id')['user_id'].transform('nunique')\n",
    "            df['device_user_count'] = device_user_counts\n",
    "            df['is_shared_device'] = (df['device_user_count'] > 3).astype(int)\n",
    "            df['is_very_shared_device'] = (df['device_user_count'] > 5).astype(int)\n",
    "        \n",
    "        # === Pattern 2: Velocity Attack (Enhanced) ===\n",
    "        # Fraud injection: days_since_last_txn < 0.007 (< 10 minutes)\n",
    "        if 'days_since_last_txn' in df.columns:\n",
    "            df['is_velocity_attack'] = (df['days_since_last_txn'] < 0.007).astype(int)  # < 10 min\n",
    "            df['is_very_rapid'] = (df['days_since_last_txn'] < 0.0035).astype(int)  # < 5 min\n",
    "        \n",
    "        # === Pattern 17: Card Testing Sequence ===\n",
    "        # Fraud injection: last 2 txns < $15 AND current > $300\n",
    "        if 'user_id' in df.columns and 'timestamp' in df.columns:\n",
    "            # Sort by user and time to get previous amounts\n",
    "            df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "            df['prev_amount_1'] = df.groupby('user_id')['amount'].shift(1)\n",
    "            df['prev_amount_2'] = df.groupby('user_id')['amount'].shift(2)\n",
    "            \n",
    "            # Card testing pattern\n",
    "            df['is_card_testing'] = (\n",
    "                (df['prev_amount_1'].fillna(999) < 15) & \n",
    "                (df['prev_amount_2'].fillna(999) < 15) & \n",
    "                (df['amount'] > 300)\n",
    "            ).astype(int)\n",
    "            \n",
    "            # Amount spike after small transactions\n",
    "            df['amount_spike_ratio'] = df['amount'] / (df['prev_amount_1'].fillna(1) + 1)\n",
    "            df['is_amount_spike_20x'] = (df['amount_spike_ratio'] > 20).astype(int)\n",
    "            df['is_amount_spike_50x'] = (df['amount_spike_ratio'] > 50).astype(int)\n",
    "        \n",
    "        # === Pattern 11: Impossible Travel ===\n",
    "        # Fraud injection: speed > 900 km/h\n",
    "        if all(col in df.columns for col in ['latitude', 'longitude', 'user_id', 'timestamp']):\n",
    "            try:\n",
    "                df = df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
    "                df['prev_lat'] = df.groupby('user_id')['latitude'].shift(1)\n",
    "                df['prev_lon'] = df.groupby('user_id')['longitude'].shift(1)\n",
    "                df['prev_time'] = df.groupby('user_id')['timestamp'].shift(1)\n",
    "                \n",
    "                # Approximate distance using Haversine formula\n",
    "                def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "                    R = 6371  # Earth's radius in km\n",
    "                    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "                    dlat = lat2 - lat1\n",
    "                    dlon = lon2 - lon1\n",
    "                    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "                    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "                    return R * c\n",
    "                \n",
    "                mask = df['prev_lat'].notna() & df['prev_lon'].notna()\n",
    "                df.loc[mask, 'travel_distance_km'] = haversine_vectorized(\n",
    "                    df.loc[mask, 'prev_lat'], df.loc[mask, 'prev_lon'],\n",
    "                    df.loc[mask, 'latitude'], df.loc[mask, 'longitude']\n",
    "                )\n",
    "                df['travel_distance_km'] = df['travel_distance_km'].fillna(0)\n",
    "                \n",
    "                # Time difference in hours\n",
    "                df['time_diff_hours'] = (df['timestamp'] - df['prev_time']).dt.total_seconds() / 3600\n",
    "                df['time_diff_hours'] = df['time_diff_hours'].fillna(999)\n",
    "                \n",
    "                # Speed in km/h\n",
    "                df['travel_speed_kmh'] = df['travel_distance_km'] / (df['time_diff_hours'] + 0.001)\n",
    "                df['is_impossible_travel'] = (df['travel_speed_kmh'] > 900).astype(int)\n",
    "                df['is_suspicious_travel'] = (df['travel_speed_kmh'] > 500).astype(int)\n",
    "                \n",
    "                # Cleanup temp columns\n",
    "                df.drop(['prev_lat', 'prev_lon', 'prev_time'], axis=1, inplace=True, errors='ignore')\n",
    "            except Exception as e:\n",
    "                print(f\"      âš ï¸ Impossible travel calculation failed: {e}\")\n",
    "                df['is_impossible_travel'] = 0\n",
    "                df['is_suspicious_travel'] = 0\n",
    "        \n",
    "        # === Pattern 4: Account Takeover Score ===\n",
    "        # Fraud injection: device != primary_device AND failed_logins > 2 AND late night\n",
    "        if 'failed_login_attempts_last_24h' in df.columns:\n",
    "            df['account_takeover_score'] = (\n",
    "                (df['failed_login_attempts_last_24h'] > 2).astype(int) * 3 +\n",
    "                df.get('is_late_night', 0) * 2 +\n",
    "                df.get('is_night', 0) * 1\n",
    "            )\n",
    "            df['is_account_takeover_risk'] = (df['account_takeover_score'] >= 4).astype(int)\n",
    "        \n",
    "        # === COMPOSITE PATTERN SCORES ===\n",
    "        print(\"   ðŸ“Š Creating composite pattern scores...\")\n",
    "        \n",
    "        # Master fraud pattern score (combines all pattern indicators)\n",
    "        pattern_cols = [\n",
    "            'is_new_user_high_value', 'is_first_txn_high_amount', 'is_late_night_high_value',\n",
    "            'is_geo_mismatch_fraud', 'is_stolen_card_pattern', 'is_micro_new_user',\n",
    "            'is_gift_card_risky', 'is_shared_device', 'is_velocity_attack',\n",
    "            'is_card_testing', 'is_impossible_travel', 'is_account_takeover_risk'\n",
    "        ]\n",
    "        \n",
    "        # Sum of all pattern flags (each pattern adds 1)\n",
    "        df['pattern_match_count'] = 0\n",
    "        for col in pattern_cols:\n",
    "            if col in df.columns:\n",
    "                df['pattern_match_count'] += df[col].fillna(0)\n",
    "        \n",
    "        # Weighted pattern score\n",
    "        pattern_weights = {\n",
    "            'is_impossible_travel': 10,\n",
    "            'is_card_testing': 8,\n",
    "            'is_account_takeover_risk': 7,\n",
    "            'is_stolen_card_pattern': 6,\n",
    "            'is_first_txn_high_amount': 5,\n",
    "            'is_new_user_high_value': 4,\n",
    "            'is_late_night_high_value': 4,\n",
    "            'is_geo_mismatch_fraud': 4,\n",
    "            'is_velocity_attack': 3,\n",
    "            'is_gift_card_risky': 3,\n",
    "            'is_shared_device': 2,\n",
    "            'is_micro_new_user': 2\n",
    "        }\n",
    "        \n",
    "        df['weighted_pattern_score'] = 0\n",
    "        for col, weight in pattern_weights.items():\n",
    "            if col in df.columns:\n",
    "                df['weighted_pattern_score'] += df[col].fillna(0) * weight\n",
    "        \n",
    "        # High risk flag based on pattern score\n",
    "        df['is_multi_pattern_risk'] = (df['pattern_match_count'] >= 2).astype(int)\n",
    "        df['is_high_pattern_score'] = (df['weighted_pattern_score'] >= 8).astype(int)\n",
    "        \n",
    "        # ============ 12. ONE-HOT ENCODING ============\n",
    "        print(\"   ðŸ”¤ One-hot encoding...\")\n",
    "        \n",
    "        low_card_categoricals = [\n",
    "            'currency', 'transaction_type', 'channel', 'payment_method',\n",
    "            'device_type', 'browser', 'merchant_category', 'kyc_status'\n",
    "        ]\n",
    "        \n",
    "        for col in low_card_categoricals:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    dummies = pd.get_dummies(df[col], prefix=col, drop_first=True, dtype=int)\n",
    "                    df = pd.concat([df, dummies], axis=1)\n",
    "                except Exception as e:\n",
    "                    print(f\"      âœ— {col}: {e}\")\n",
    "        \n",
    "        # ============ 12. FEATURE SELECTION ============\n",
    "        print(\"   âœ‚ï¸ Feature selection...\")\n",
    "        \n",
    "        high_card_categoricals = [\n",
    "            'user_id', 'merchant_id', 'device_id', 'ip_address',\n",
    "            'email', 'billing_city', 'shipping_city', 'device_os',\n",
    "            'user_agent', 'browser_version', 'merchant_name',\n",
    "            'user_merchant_pair'  # Added from new features\n",
    "        ]\n",
    "        \n",
    "        exclude_cols = [\n",
    "            'transaction_id', 'timestamp', 'user_signup_date',\n",
    "            'is_fraud', 'fraud_type', 'latitude', 'longitude'\n",
    "        ]\n",
    "        exclude_cols.extend(low_card_categoricals)\n",
    "        exclude_cols.extend(high_card_categoricals)\n",
    "        \n",
    "        feature_cols = [col for col in df.columns \n",
    "                        if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]\n",
    "        \n",
    "        print(f\"   âœ… Total features: {len(feature_cols)}\")\n",
    "        \n",
    "        return df, feature_cols\n",
    "    \n",
    "    def prepare_data_with_time_aware_split(self):\n",
    "        \"\"\"Time-aware train/val/test split (70:15:15)\"\"\"\n",
    "        print(\"\\nðŸ“Š Preparing time-aware data splits (70:15:15)...\")\n",
    "        \n",
    "        df_engineered, feature_cols = self.advanced_feature_engineering_ultimate()\n",
    "        self.feature_names = feature_cols\n",
    "        \n",
    "        X = df_engineered[feature_cols].copy()\n",
    "        y = df_engineered['is_fraud'].copy()\n",
    "        \n",
    "        X = X.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Feature matrix: {X.shape}\")\n",
    "        print(f\"ðŸ’° Fraud rate: {y.mean()*100:.2f}%\")\n",
    "        \n",
    "        # Time-aware split (70:15:15)\n",
    "        n = len(X)\n",
    "        train_end = int(n * 0.70)\n",
    "        val_end = int(n * 0.85)\n",
    "        \n",
    "        X_train = X.iloc[:train_end]\n",
    "        X_val = X.iloc[train_end:val_end]\n",
    "        X_test = X.iloc[val_end:]\n",
    "        y_train = y.iloc[:train_end]\n",
    "        y_val = y.iloc[train_end:val_end]\n",
    "        y_test = y.iloc[val_end:]\n",
    "        \n",
    "        self.X_train, self.X_val, self.X_test = X_train, X_val, X_test\n",
    "        self.y_train, self.y_val, self.y_test = y_train, y_val, y_test\n",
    "        \n",
    "        # Multi-stage scaling: RobustScaler + QuantileTransformer\n",
    "        print(\"\\nðŸ”„ Applying hybrid scaling (Robust + Quantile)...\")\n",
    "        self.scaler = RobustScaler()\n",
    "        X_train_robust = self.scaler.fit_transform(self.X_train)\n",
    "        X_val_robust = self.scaler.transform(self.X_val)\n",
    "        X_test_robust = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        # Apply QuantileTransformer for Gaussian-like distribution\n",
    "        self.quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=RANDOM_STATE)\n",
    "        self.X_train_scaled = self.quantile_transformer.fit_transform(X_train_robust)\n",
    "        self.X_val_scaled = self.quantile_transformer.transform(X_val_robust)\n",
    "        self.X_test_scaled = self.quantile_transformer.transform(X_test_robust)\n",
    "        \n",
    "        print(f\"\\nâœ… Data splits:\")\n",
    "        print(f\"   Train: {self.X_train.shape} (fraud: {self.y_train.mean()*100:.2f}%)\")\n",
    "        print(f\"   Val:   {self.X_val.shape} (fraud: {self.y_val.mean()*100:.2f}%)\")\n",
    "        print(f\"   Test:  {self.X_test.shape} (fraud: {self.y_test.mean()*100:.2f}%)\")\n",
    "        \n",
    "        return self.X_train_scaled, self.X_val_scaled, self.X_test_scaled\n",
    "    \n",
    "    def verify_class_distribution(self):\n",
    "        \"\"\"\n",
    "        Verify class distribution after time-aware split.\n",
    "        \n",
    "        Note: No resampling is performed because:\n",
    "        1. The dataset has ~24% fraud rate (already reasonably balanced)\n",
    "        2. Time-aware split preserves natural distribution across train/val/test\n",
    "        3. Class imbalance is handled via scale_pos_weight in boosting models\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ“Š Class Distribution Verification...\")\n",
    "        print(f\"   Data: {self.X_train_scaled.shape}, Fraud: {self.y_train.mean()*100:.2f}%\")\n",
    "        print(\"   âœ“ Using existing distribution (balanced via model weights)\")\n",
    "    \n",
    "    # REMOVED: Isolation Forest anomaly detection - no longer used in pipeline\n",
    "    # def detect_anomalies(self):\n",
    "    #     \"\"\"Isolation Forest for anomaly reporting (No Filtering)\"\"\"\n",
    "    #     print(\"\\\\nðŸ” Running Isolation Forest for anomaly detection...\")\n",
    "    #     \n",
    "    #     iso_forest = IsolationForest(\n",
    "    #         contamination=0.1,\n",
    "    #         random_state=RANDOM_STATE,\n",
    "    #         n_jobs=-1\n",
    "    #     )\n",
    "    #     \n",
    "    #     # Fit on training data\n",
    "    #     anomaly_labels = iso_forest.fit_predict(self.X_train_scaled)\n",
    "    #     self.anomaly_scores = iso_forest.score_samples(self.X_train_scaled)\n",
    "    #     \n",
    "    #     n_anomalies = (anomaly_labels == -1).sum()\n",
    "    #     print(f\"   Detected {n_anomalies} anomalies in training set (Reporting Only)\")\n",
    "    \n",
    "    def train_ensemble_models(self):\n",
    "        \"\"\"Train ensemble of XGBoost, LightGBM, CatBoost\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸŽ¯ TRAINING ENSEMBLE MODELS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        fraud_ratio = np.sum(self.y_train == 0) / np.sum(self.y_train == 1)\n",
    "        \n",
    "        # === 1. XGBoost ===\n",
    "        print(\"\\n[1/3] Training XGBoost...\")\n",
    "        xgb_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'aucpr',\n",
    "            'scale_pos_weight': fraud_ratio,\n",
    "            'n_estimators': 1600,\n",
    "            'learning_rate': 0.062,\n",
    "            'max_depth': 12,\n",
    "            'min_child_weight': 6,\n",
    "            'subsample': 0.715,\n",
    "            'colsample_bytree': 0.707,\n",
    "            'gamma': 0.971,\n",
    "            'reg_alpha': 1.003,\n",
    "            'reg_lambda': 2.545,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'device': 'cuda',           # ADD THIS (Forces NVIDIA GPU)\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        \n",
    "        self.models['xgboost'] = xgb.XGBClassifier(**xgb_params)\n",
    "        self.models['xgboost'].fit(\n",
    "            self.X_train_scaled, self.y_train,\n",
    "            eval_set=[(self.X_val_scaled, self.y_val)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        print(\"   âœ“ XGBoost trained\")\n",
    "        \n",
    "        # === 2. LightGBM ===\n",
    "        if LIGHTGBM_AVAILABLE:\n",
    "            print(\"\\n[2/3] Training LightGBM...\")\n",
    "            lgb_params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'scale_pos_weight': fraud_ratio,\n",
    "                'n_estimators': 1500,\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 10,\n",
    "                'num_leaves': 31,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "                'verbosity': -1\n",
    "            }\n",
    "            \n",
    "            self.models['lightgbm'] = lgb.LGBMClassifier(**lgb_params)\n",
    "            self.models['lightgbm'].fit(\n",
    "                self.X_train_scaled, self.y_train,\n",
    "                eval_set=[(self.X_val_scaled, self.y_val)],\n",
    "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            print(\"   âœ“ LightGBM trained\")\n",
    "        \n",
    "        # === 3. CatBoost ===\n",
    "        if CATBOOST_AVAILABLE:\n",
    "            print(\"\\n[3/3] Training CatBoost...\")\n",
    "            cat_params = {\n",
    "                'loss_function': 'Logloss',\n",
    "                'eval_metric': 'AUC',\n",
    "                'scale_pos_weight': fraud_ratio,\n",
    "                'iterations': 1500,\n",
    "                'learning_rate': 0.05,\n",
    "                'depth': 10,\n",
    "                'random_seed': RANDOM_STATE,\n",
    "                'task_type': 'CPU',  # Changed from GPU to CPU\n",
    "                'verbose': False\n",
    "            }\n",
    "            \n",
    "            self.models['catboost'] = cb.CatBoostClassifier(**cat_params)\n",
    "            self.models['catboost'].fit(\n",
    "                self.X_train_scaled, self.y_train,\n",
    "                eval_set=(self.X_val_scaled, self.y_val),\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "            print(\"   âœ“ CatBoost trained\")\n",
    "        \n",
    "        print(f\"\\nâœ… Trained {len(self.models)} models in ensemble\")\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        if 'xgboost' in self.models:\n",
    "            self.feature_importance = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': self.models['xgboost'].feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    def optimize_ensemble_weights_bayesian(self, n_trials=100):\n",
    "        \"\"\"\n",
    "        BAYESIAN OPTIMIZATION FOR ENSEMBLE WEIGHTS\n",
    "        \n",
    "        Objective: Maximize F2 Score (recall-weighted) with Precision >= 60%\n",
    "        \n",
    "        F2 Score Formula: F2 = (1 + 4) Ã— (P Ã— R) / (4P + R)\n",
    "        - Recall is weighted 4x more than precision\n",
    "        - Precision floor of 60% prevents excessive false positives\n",
    "        \n",
    "        Uses known optimal weights as warm start for faster convergence.\n",
    "        \"\"\"\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            print(\"âŒ Optuna not installed - using equal weights\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ§¬ BAYESIAN OPTIMIZATION FOR ENSEMBLE WEIGHTS\")\n",
    "        print(\"   Objective: Maximize F2 Score | Constraint: Precision >= 60%\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ============= WARM START WEIGHTS =============\n",
    "        # Best weights from previous F1-focused optimization (used as starting point)\n",
    "        WARM_START_WEIGHTS = {\n",
    "            'xgboost': 0.5329,\n",
    "            'lightgbm': 0.1890,\n",
    "            'catboost': 0.2781\n",
    "        }\n",
    "        \n",
    "        # Get base predictions from all models\n",
    "        base_predictions = {}\n",
    "        for name, model in self.models.items():\n",
    "            base_predictions[name] = model.predict_proba(self.X_val_scaled)[:, 1]\n",
    "        \n",
    "        model_names = list(base_predictions.keys())\n",
    "        \n",
    "        # Precision floor constraint\n",
    "        PRECISION_FLOOR = 0.60\n",
    "        \n",
    "        def objective(trial):\n",
    "            \"\"\"\n",
    "            Objective: Maximize F2 Score with Precision >= 60% constraint\n",
    "            \n",
    "            F2 heavily weights recall (catches more fraud) while the\n",
    "            precision floor prevents flagging too many legitimate transactions.\n",
    "            \"\"\"\n",
    "            # Suggest weights that sum to 1\n",
    "            if len(model_names) == 1:\n",
    "                weights = [1.0]\n",
    "            elif len(model_names) == 2:\n",
    "                w1 = trial.suggest_float('weight_0', 0.1, 0.9)\n",
    "                weights = [w1, 1 - w1]\n",
    "            else:\n",
    "                # For 3 models, sample 2 weights and derive the third\n",
    "                w1 = trial.suggest_float('weight_0', 0.1, 0.8)\n",
    "                w2 = trial.suggest_float('weight_1', 0.1, 0.8)\n",
    "                # Normalize to sum to 1\n",
    "                total = w1 + w2\n",
    "                if total >= 0.95:\n",
    "                    w1 = w1 / (total + 0.1)\n",
    "                    w2 = w2 / (total + 0.1)\n",
    "                w3 = max(0.05, 1.0 - w1 - w2)\n",
    "                weights = [w1, w2, w3][:len(model_names)]\n",
    "            \n",
    "            # Normalize weights to sum exactly to 1\n",
    "            weights = np.array(weights)\n",
    "            weights = weights / weights.sum()\n",
    "            \n",
    "            # Compute weighted ensemble prediction\n",
    "            ensemble_pred = np.zeros(len(self.y_val))\n",
    "            for i, name in enumerate(model_names):\n",
    "                ensemble_pred += weights[i] * base_predictions[name]\n",
    "            \n",
    "            # Try multiple thresholds for robustness\n",
    "            best_score = -1.0\n",
    "            for thresh in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "                y_pred = (ensemble_pred > thresh).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                f2 = fbeta_score(self.y_val, y_pred, beta=2)\n",
    "                prec = precision_score(self.y_val, y_pred, zero_division=0)\n",
    "                \n",
    "                # ============= F2 + PRECISION CONSTRAINT =============\n",
    "                # If precision is below floor, apply harsh penalty\n",
    "                if prec < PRECISION_FLOOR:\n",
    "                    # Penalty scales with how far below the floor we are\n",
    "                    penalty = (PRECISION_FLOOR - prec) * 2.0\n",
    "                    score = f2 - penalty\n",
    "                else:\n",
    "                    # Pure F2 optimization when precision is acceptable\n",
    "                    # Small bonus for precision above floor (up to 5%)\n",
    "                    precision_bonus = min(0.05, (prec - PRECISION_FLOOR) * 0.1)\n",
    "                    score = f2 + precision_bonus\n",
    "                \n",
    "                best_score = max(best_score, score)\n",
    "            \n",
    "            return best_score\n",
    "        \n",
    "        print(f\"ðŸ” Optimizing weights for {len(model_names)} models...\")\n",
    "        print(f\"   Models: {', '.join(model_names)}\")\n",
    "        print(f\"   Precision Floor: {PRECISION_FLOOR*100:.0f}%\")\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=RANDOM_STATE, n_startup_trials=20)\n",
    "        )\n",
    "        \n",
    "        # ============= WARM START =============\n",
    "        # Enqueue trial with known good weights for faster convergence\n",
    "        if len(model_names) >= 2:\n",
    "            try:\n",
    "                warm_params = {}\n",
    "                if 'xgboost' in WARM_START_WEIGHTS and 'lightgbm' in WARM_START_WEIGHTS:\n",
    "                    warm_params['weight_0'] = WARM_START_WEIGHTS.get('xgboost', 0.33)\n",
    "                    warm_params['weight_1'] = WARM_START_WEIGHTS.get('lightgbm', 0.33)\n",
    "                    study.enqueue_trial(warm_params)\n",
    "                    print(f\"   ðŸ”¥ Warm start: XGB={warm_params['weight_0']:.4f}, LGB={warm_params['weight_1']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ Warm start skipped: {e}\")\n",
    "        \n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        # Extract optimal weights\n",
    "        optimal_weights = []\n",
    "        if len(model_names) == 1:\n",
    "            optimal_weights = [1.0]\n",
    "        elif len(model_names) == 2:\n",
    "            w1 = study.best_params['weight_0']\n",
    "            optimal_weights = [w1, 1 - w1]\n",
    "        else:\n",
    "            w1 = study.best_params['weight_0']\n",
    "            w2 = study.best_params['weight_1']\n",
    "            w3 = max(0.05, 1.0 - w1 - w2)\n",
    "            optimal_weights = [w1, w2, w3][:len(model_names)]\n",
    "        \n",
    "        # Normalize\n",
    "        optimal_weights = np.array(optimal_weights)\n",
    "        optimal_weights = optimal_weights / optimal_weights.sum()\n",
    "        \n",
    "        print(f\"\\nðŸ† Optimal Ensemble Weights (F2 + Precision >= 60%):\")\n",
    "        for name, weight in zip(model_names, optimal_weights):\n",
    "            print(f\"   {name:12s}: {weight:.4f} ({weight*100:.2f}%)\")\n",
    "        print(f\"\\n   Best Score: {study.best_value:.6f}\")\n",
    "        \n",
    "        # Store optimal weights\n",
    "        self.optimal_weights = dict(zip(model_names, optimal_weights))\n",
    "        \n",
    "        # Validate on validation set with optimal weights\n",
    "        ensemble_pred = np.zeros(len(self.y_val))\n",
    "        for name, weight in self.optimal_weights.items():\n",
    "            ensemble_pred += weight * base_predictions[name]\n",
    "        \n",
    "        # Find best threshold with optimal weights (optimizing for F2)\n",
    "        best_f2 = 0\n",
    "        best_thresh = 0.5\n",
    "        for thresh in np.linspace(0.2, 0.7, 51):\n",
    "            y_pred = (ensemble_pred > thresh).astype(int)\n",
    "            f2 = fbeta_score(self.y_val, y_pred, beta=2)\n",
    "            prec = precision_score(self.y_val, y_pred, zero_division=0)\n",
    "            # Only consider thresholds with acceptable precision\n",
    "            if prec >= PRECISION_FLOOR and f2 > best_f2:\n",
    "                best_f2 = f2\n",
    "                best_thresh = thresh\n",
    "        \n",
    "        y_pred_optimal = (ensemble_pred > best_thresh).astype(int)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Validation Set Performance with Optimized Weights:\")\n",
    "        print(f\"   Threshold: {best_thresh:.2f}\")\n",
    "        print(f\"   Accuracy:  {accuracy_score(self.y_val, y_pred_optimal)*100:.2f}%\")\n",
    "        print(f\"   Precision: {precision_score(self.y_val, y_pred_optimal, zero_division=0)*100:.2f}%\")\n",
    "        print(f\"   Recall:    {recall_score(self.y_val, y_pred_optimal)*100:.2f}%\")\n",
    "        print(f\"   F1-Score:  {f1_score(self.y_val, y_pred_optimal)*100:.2f}%\")\n",
    "        print(f\"   F2-Score:  {fbeta_score(self.y_val, y_pred_optimal, beta=2)*100:.2f}%\")\n",
    "        \n",
    "        return self.optimal_weights\n",
    "    \n",
    "    def get_ensemble_predictions(self, X, mode='optimized'):\n",
    "        \"\"\"\n",
    "        Get ensemble predictions with multiple modes\n",
    "        Uses calibrated models when available.\n",
    "        \n",
    "        Modes:\n",
    "        - 'bouncer': Rule Engine as early-exit filter (FASTEST, catches obvious fraud)\n",
    "        - 'optimized': Use Bayesian-optimized weights (best)\n",
    "        - 'enhanced': Use all framework components (Rule+Autoencoder+ML)\n",
    "        - 'weighted': Use predefined weights based on model complexity\n",
    "        - 'average': Simple average\n",
    "        - 'voting': Hard voting (majority)\n",
    "        - 'stacking': Meta-learning approach\n",
    "        \"\"\"\n",
    "        \n",
    "        # ========= BOUNCER MODE: Soft Cascade with Dynamic Boost =========\n",
    "        if mode == 'bouncer':\n",
    "            \"\"\"\n",
    "            SOFT CASCADE with DYNAMIC BOOST (ML Engineer's Formula):\n",
    "            \n",
    "            1. Rule Engine computes risk score (0-100) using UNSCALED data\n",
    "            2. Score >= 85: HARD BLOCK (definite fraud)\n",
    "            3. Score 60-84: HIGH RISK â†’ 35% rule + 65% ML blend\n",
    "            4. Score < 60: LOW RISK â†’ 10% rule + 90% ML blend\n",
    "            \"\"\"\n",
    "            final_predictions = np.zeros(len(X))\n",
    "            HARD_BLOCK_THRESHOLD = 0.85  # Score >= 85 = definite fraud\n",
    "            HIGH_RISK_THRESHOLD = 0.60   # Score >= 60 = high risk\n",
    "            \n",
    "            if hasattr(self, 'rule_engine') and hasattr(self, 'feature_names'):\n",
    "                # CRITICAL: Rule Engine needs UNSCALED data (original feature values)\n",
    "                # Determine which unscaled dataset matches the input shape\n",
    "                X_unscaled = None\n",
    "                if hasattr(self, 'X_val') and len(X) == len(self.X_val):\n",
    "                    X_unscaled = self.X_val\n",
    "                elif hasattr(self, 'X_test') and len(X) == len(self.X_test):\n",
    "                    X_unscaled = self.X_test\n",
    "                elif hasattr(self, 'X_train') and len(X) == len(self.X_train):\n",
    "                    X_unscaled = self.X_train\n",
    "                \n",
    "                if X_unscaled is not None:\n",
    "                    rule_scores = self.rule_engine.predict_batch(X_unscaled, self.feature_names)\n",
    "                else:\n",
    "                    # Fallback: try inverse transform if scaler exists\n",
    "                    if hasattr(self, 'scaler') and self.scaler is not None:\n",
    "                        try:\n",
    "                            X_unscaled = self.scaler.inverse_transform(X)\n",
    "                            rule_scores = self.rule_engine.predict_batch(X_unscaled, self.feature_names)\n",
    "                        except:\n",
    "                            # Last resort: use scaled data (rules may not fire correctly)\n",
    "                            rule_scores = self.rule_engine.predict_batch(X, self.feature_names)\n",
    "                    else:\n",
    "                        rule_scores = self.rule_engine.predict_batch(X, self.feature_names)\n",
    "                \n",
    "                # ============= HARD BLOCK (Score >= 85) =============\n",
    "                hard_block_mask = rule_scores >= HARD_BLOCK_THRESHOLD\n",
    "                n_hard_blocked = hard_block_mask.sum()\n",
    "                final_predictions[hard_block_mask] = 1.0\n",
    "                \n",
    "                # ============= SOFT CASCADE (Score < 98) =============\n",
    "                soft_mask = ~hard_block_mask\n",
    "                n_soft = soft_mask.sum()\n",
    "                \n",
    "                if n_soft > 0:\n",
    "                    X_soft = X[soft_mask]\n",
    "                    soft_rule_scores = rule_scores[soft_mask]\n",
    "                    \n",
    "                    # Get ML predictions for soft cascade transactions\n",
    "                    models_to_use = getattr(self, 'calibrated_models', self.models)\n",
    "                    \n",
    "                    if hasattr(self, 'optimal_weights') and self.optimal_weights:\n",
    "                        ml_preds = np.zeros(len(X_soft))\n",
    "                        for name, model in models_to_use.items():\n",
    "                            weight = self.optimal_weights.get(name, 0)\n",
    "                            if weight > 0:\n",
    "                                ml_preds += weight * model.predict_proba(X_soft)[:, 1]\n",
    "                    else:\n",
    "                        pred_list = [model.predict_proba(X_soft)[:, 1] for model in models_to_use.values()]\n",
    "                        ml_preds = np.mean(pred_list, axis=0) if pred_list else np.zeros(len(X_soft))\n",
    "                    \n",
    "                    # Add autoencoder boost if available\n",
    "                    if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "                        ae_scores = self.autoencoder.predict(X_soft)\n",
    "                        ml_preds = ml_preds * 0.85 + ae_scores * 0.15\n",
    "                    \n",
    "                    # ============= DYNAMIC BOOST FORMULA (BNN-Enhanced) =============\n",
    "                    # Use BNN to identify high-risk transactions (replaces hardcoded threshold)\n",
    "                    # High risk (BNN predicts fraud): 35% rule, 65% ML\n",
    "                    # Low risk (BNN predicts legit): 10% rule, 90% ML\n",
    "                    \n",
    "                    if hasattr(self, 'high_risk_identifier') and self.high_risk_identifier.is_fitted:\n",
    "                        # Use BNN for high-risk identification\n",
    "                        X_soft_unscaled = X_unscaled[soft_mask] if X_unscaled is not None else None\n",
    "                        \n",
    "                        if X_soft_unscaled is not None:\n",
    "                            bnn_result = self.high_risk_identifier.predict(\n",
    "                                X_soft_unscaled, \n",
    "                                self.feature_names, \n",
    "                                self.rule_engine\n",
    "                            )\n",
    "                            high_risk_mask = bnn_result['is_high_risk']\n",
    "                            high_risk_proba = bnn_result['probability']\n",
    "                            high_risk_uncertainty = bnn_result['uncertainty']\n",
    "                        else:\n",
    "                            # Fallback to hardcoded threshold\n",
    "                            high_risk_mask = soft_rule_scores >= HIGH_RISK_THRESHOLD\n",
    "                    else:\n",
    "                        # Fallback: use hardcoded threshold\n",
    "                        high_risk_mask = soft_rule_scores >= HIGH_RISK_THRESHOLD\n",
    "                    \n",
    "                    blended = np.zeros(len(X_soft))\n",
    "                    \n",
    "                    # High risk transactions (BNN says fraud-like patterns)\n",
    "                    if high_risk_mask.sum() > 0:\n",
    "                        boost_weight = 0.35\n",
    "                        blended[high_risk_mask] = (\n",
    "                            ml_preds[high_risk_mask] * (1 - boost_weight) +\n",
    "                            soft_rule_scores[high_risk_mask] * boost_weight\n",
    "                        )\n",
    "                    \n",
    "                    # Low risk transactions\n",
    "                    low_risk_mask = ~high_risk_mask\n",
    "                    if low_risk_mask.sum() > 0:\n",
    "                        boost_weight = 0.10\n",
    "                        blended[low_risk_mask] = (\n",
    "                            ml_preds[low_risk_mask] * (1 - boost_weight) +\n",
    "                            soft_rule_scores[low_risk_mask] * boost_weight\n",
    "                        )\n",
    "                    \n",
    "                    final_predictions[soft_mask] = np.clip(blended, 0, 1)\n",
    "                \n",
    "                # Store cascade statistics\n",
    "                n_high_risk = high_risk_mask.sum() if 'high_risk_mask' in dir() else 0\n",
    "                self._cascade_stats = {\n",
    "                    'total': len(X),\n",
    "                    'hard_blocked': int(n_hard_blocked),\n",
    "                    'high_risk_soft': int(n_high_risk),\n",
    "                    'low_risk': int(len(X) - n_hard_blocked - n_high_risk),\n",
    "                    'hard_block_rate': n_hard_blocked / len(X) * 100\n",
    "                }\n",
    "                \n",
    "                # Log stats\n",
    "                if not getattr(self, '_bouncer_stats_logged', False):\n",
    "                    print(f\"   ðŸš¨ Hard Blocked (score>=85): {n_hard_blocked}/{len(X)} ({n_hard_blocked/len(X)*100:.1f}%)\")\n",
    "                    print(f\"   âš¡ High Risk Blend (35% rule, BNN): {n_high_risk}\")\n",
    "                    print(f\"   ðŸ“Š Low Risk Blend (10% rule): {len(X) - n_hard_blocked - n_high_risk}\")\n",
    "                    self._bouncer_stats_logged = True\n",
    "                \n",
    "                return final_predictions\n",
    "            else:\n",
    "                print(\"   âš ï¸ Rule engine not available, falling back to optimized mode\")\n",
    "                mode = 'optimized'\n",
    "        \n",
    "        # ========= Standard Modes =========\n",
    "        predictions = {}\n",
    "        \n",
    "        # Use calibrated models if available, otherwise raw models\n",
    "        models_to_use = getattr(self, 'calibrated_models', self.models)\n",
    "        \n",
    "        for name, model in models_to_use.items():\n",
    "            pred_proba = model.predict_proba(X)[:, 1]\n",
    "            predictions[name] = pred_proba\n",
    "        \n",
    "        # Always add Autoencoder predictions (for 'optimized' mode with 5-model weights)\n",
    "        if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "            predictions['autoencoder'] = self.autoencoder.predict(X)\n",
    "        \n",
    "        # Add framework component scores if mode is enhanced\n",
    "        if mode == 'enhanced':\n",
    "            # Get autoencoder anomaly scores\n",
    "            if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "                autoencoder_scores = self.autoencoder.predict(X)\n",
    "                predictions['autoencoder'] = autoencoder_scores\n",
    "            \n",
    "            # Get rule engine scores\n",
    "            if hasattr(self, 'rule_engine') and hasattr(self, 'feature_names'):\n",
    "                rule_scores = self.rule_engine.predict_batch(X, self.feature_names)\n",
    "                predictions['rule_engine'] = rule_scores\n",
    "            \n",
    "            # Weighted combination: ML models (80%) + Framework (20%)\n",
    "            ml_weight = 0.80\n",
    "            framework_weight = 0.20\n",
    "            \n",
    "            ml_models = ['xgboost', 'lightgbm', 'catboost', 'extratrees']\n",
    "            framework_models = ['autoencoder', 'rule_engine']\n",
    "            \n",
    "            ml_preds = [predictions[m] for m in ml_models if m in predictions]\n",
    "            fw_preds = [predictions[m] for m in framework_models if m in predictions]\n",
    "            \n",
    "            ml_avg = np.mean(ml_preds, axis=0) if ml_preds else np.zeros(len(X))\n",
    "            fw_avg = np.mean(fw_preds, axis=0) if fw_preds else np.zeros(len(X))\n",
    "            \n",
    "            return ml_weight * ml_avg + framework_weight * fw_avg\n",
    "        \n",
    "        if mode == 'optimized' and hasattr(self, 'optimal_weights'):\n",
    "            # Use Bayesian-optimized weights\n",
    "            ensemble_pred = np.zeros(len(X))\n",
    "            for name, weight in self.optimal_weights.items():\n",
    "                if name in predictions:\n",
    "                    ensemble_pred += weight * predictions[name]\n",
    "            return ensemble_pred\n",
    "        \n",
    "        elif mode == 'weighted':\n",
    "            # Predefined weights for 4-model ensemble\n",
    "            weight_map = {\n",
    "                'xgboost': 0.30,     # Strong baseline\n",
    "                'lightgbm': 0.30,    # Fast and accurate  \n",
    "                'catboost': 0.20,    # Good with categorical\n",
    "                'extratrees': 0.20   # Diversity\n",
    "            }\n",
    "            ensemble_pred = np.zeros(len(X))\n",
    "            total_weight = 0\n",
    "            for name, pred in predictions.items():\n",
    "                weight = weight_map.get(name, 1.0 / len(predictions))\n",
    "                ensemble_pred += weight * pred\n",
    "                total_weight += weight\n",
    "            return ensemble_pred / total_weight\n",
    "        \n",
    "        elif mode == 'average':\n",
    "            # Simple average\n",
    "            return np.mean(list(predictions.values()), axis=0)\n",
    "        \n",
    "        elif mode == 'voting':\n",
    "            # Hard voting (majority)\n",
    "            votes = np.array([pred > 0.5 for pred in predictions.values()])\n",
    "            return np.mean(votes, axis=0)\n",
    "        \n",
    "        elif mode == 'stacking':\n",
    "            # Stacking: use predictions as features for meta-model\n",
    "            if hasattr(self, 'meta_model'):\n",
    "                stacked_features = np.column_stack(list(predictions.values()))\n",
    "                return self.meta_model.predict_proba(stacked_features)[:, 1]\n",
    "            else:\n",
    "                # Fallback to weighted average\n",
    "                return self.get_ensemble_predictions(X, mode='weighted')\n",
    "        \n",
    "        else:\n",
    "            # Default to average\n",
    "            return np.mean(list(predictions.values()), axis=0)\n",
    "    \n",
    "    def train_stacking_meta_model(self):\n",
    "        \"\"\"\n",
    "        STACKING META-LEARNER\n",
    "        Train a meta-model on base model predictions\n",
    "        Uses Logistic Regression for interpretability\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸŽ“ Training Stacking Meta-Model...\")\n",
    "        \n",
    "        # Get base model predictions on validation set\n",
    "        base_predictions_val = []\n",
    "        for name, model in self.models.items():\n",
    "            pred_proba = model.predict_proba(self.X_val_scaled)[:, 1]\n",
    "            base_predictions_val.append(pred_proba)\n",
    "        \n",
    "        X_meta_val = np.column_stack(base_predictions_val)\n",
    "        \n",
    "        # Train meta-model with class weights\n",
    "        fraud_ratio = np.sum(self.y_val == 0) / np.sum(self.y_val == 1)\n",
    "        \n",
    "        self.meta_model = LogisticRegression(\n",
    "            class_weight={0: 1, 1: fraud_ratio},\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "        self.meta_model.fit(X_meta_val, self.y_val)\n",
    "        \n",
    "        # Evaluate meta-model\n",
    "        y_meta_pred = self.meta_model.predict(X_meta_val)\n",
    "        meta_f1 = f1_score(self.y_val, y_meta_pred) * 100\n",
    "        \n",
    "        print(f\"   âœ“ Meta-model trained\")\n",
    "        print(f\"   Validation F1: {meta_f1:.2f}%\")\n",
    "        print(f\"\\n   Meta-model weights (how much each base model contributes):\")\n",
    "        for i, name in enumerate(self.models.keys()):\n",
    "            coef = self.meta_model.coef_[0][i]\n",
    "            print(f\"      {name:12s}: {coef:+.4f}\")\n",
    "        \n",
    "        return self.meta_model\n",
    "    \n",
    "    def evaluate_individual_models(self, X=None, y=None, dataset_name=\"Validation\"):\n",
    "        \"\"\"\n",
    "        INDIVIDUAL MODEL EVALUATION\n",
    "        Print metrics for each model separately to understand their strengths\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            X = self.X_val_scaled\n",
    "            y = self.y_val\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ðŸ“Š INDIVIDUAL MODEL PERFORMANCE ({dataset_name} Set)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\n{'Model':<15} {'Acc%':>8} {'Prec%':>8} {'Rec%':>8} {'F1%':>8} {'F2%':>8} {'AUC%':>8}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "                \n",
    "                # Find optimal threshold for this model\n",
    "                best_thresh = 0.5\n",
    "                best_f1 = 0\n",
    "                for thresh in np.linspace(0.3, 0.7, 41):\n",
    "                    y_pred = (y_pred_proba > thresh).astype(int)\n",
    "                    local_f1 = f1_score(y, y_pred)\n",
    "                    if local_f1 > best_f1:\n",
    "                        best_f1 = local_f1\n",
    "                        best_thresh = thresh\n",
    "                \n",
    "                y_pred = (y_pred_proba > best_thresh).astype(int)\n",
    "                \n",
    "                acc = accuracy_score(y, y_pred) * 100\n",
    "                prec = precision_score(y, y_pred, zero_division=0) * 100\n",
    "                rec = recall_score(y, y_pred) * 100\n",
    "                f1 = f1_score(y, y_pred) * 100\n",
    "                f2 = fbeta_score(y, y_pred, beta=2) * 100\n",
    "                auc = roc_auc_score(y, y_pred_proba) * 100\n",
    "                \n",
    "                model_results[name] = {\n",
    "                    'accuracy': acc, 'precision': prec, 'recall': rec,\n",
    "                    'f1': f1, 'f2': f2, 'auc': auc, 'threshold': best_thresh\n",
    "                }\n",
    "                \n",
    "                print(f\"{name:<15} {acc:>8.2f} {prec:>8.2f} {rec:>8.2f} {f1:>8.2f} {f2:>8.2f} {auc:>8.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{name:<15} - Failed: {str(e)}\")\n",
    "        \n",
    "        # Add Autoencoder if available\n",
    "        if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "            try:\n",
    "                ae_scores = self.autoencoder.predict(X)\n",
    "                \n",
    "                best_thresh = 0.5\n",
    "                best_f1 = 0\n",
    "                for thresh in np.linspace(0.3, 0.9, 31):\n",
    "                    y_pred = (ae_scores > thresh).astype(int)\n",
    "                    local_f1 = f1_score(y, y_pred)\n",
    "                    if local_f1 > best_f1:\n",
    "                        best_f1 = local_f1\n",
    "                        best_thresh = thresh\n",
    "                \n",
    "                y_pred = (ae_scores > best_thresh).astype(int)\n",
    "                \n",
    "                acc = accuracy_score(y, y_pred) * 100\n",
    "                prec = precision_score(y, y_pred, zero_division=0) * 100\n",
    "                rec = recall_score(y, y_pred) * 100\n",
    "                f1 = f1_score(y, y_pred) * 100\n",
    "                f2 = fbeta_score(y, y_pred, beta=2) * 100\n",
    "                auc = roc_auc_score(y, ae_scores) * 100\n",
    "                \n",
    "                model_results['autoencoder'] = {\n",
    "                    'accuracy': acc, 'precision': prec, 'recall': rec,\n",
    "                    'f1': f1, 'f2': f2, 'auc': auc, 'threshold': best_thresh\n",
    "                }\n",
    "                \n",
    "                print(f\"{'autoencoder':<15} {acc:>8.2f} {prec:>8.2f} {rec:>8.2f} {f1:>8.2f} {f2:>8.2f} {auc:>8.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{'autoencoder':<15} - Failed: {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Find best individual model\n",
    "        best_model = max(model_results.items(), key=lambda x: x[1]['f1'])\n",
    "        print(f\"\\nðŸ† Best Individual Model: {best_model[0].upper()} (F1: {best_model[1]['f1']:.2f}%)\")\n",
    "        \n",
    "        self.individual_model_results = model_results\n",
    "        return model_results\n",
    "    \n",
    "    def _strategy_tiered_threshold(self, X, threshold=0.5, consensus_min=3, high_conf_thresh=0.9):\n",
    "        \"\"\"\n",
    "        STRATEGY 1: Tiered Threshold Voting\n",
    "        Layer 1: Rule Engine first\n",
    "        Layer 2: Consensus (if â‰¥N models agree)\n",
    "        Layer 3: High-confidence override (if ANY model is very sure)\n",
    "        Layer 4: Weighted average fallback\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # Get all model predictions\n",
    "        for name, model in self.models.items():\n",
    "            predictions[name] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "            predictions['autoencoder'] = self.autoencoder.predict(X)\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        final_pred = np.zeros(n_samples)\n",
    "        \n",
    "        # Stack all predictions\n",
    "        all_preds = np.array(list(predictions.values()))  # (n_models, n_samples)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            sample_preds = all_preds[:, i]\n",
    "            \n",
    "            # Layer 2: Consensus check\n",
    "            votes_above_threshold = np.sum(sample_preds > threshold)\n",
    "            \n",
    "            if votes_above_threshold >= consensus_min:\n",
    "                # Strong consensus -> high probability\n",
    "                final_pred[i] = np.mean(sample_preds[sample_preds > threshold])\n",
    "            \n",
    "            # Layer 3: High-confidence override\n",
    "            elif np.max(sample_preds) > high_conf_thresh:\n",
    "                final_pred[i] = np.max(sample_preds)\n",
    "            \n",
    "            # Layer 4: Weighted average fallback\n",
    "            else:\n",
    "                if hasattr(self, 'optimal_weights'):\n",
    "                    for name, weight in self.optimal_weights.items():\n",
    "                        if name in predictions:\n",
    "                            final_pred[i] += weight * predictions[name][i]\n",
    "                else:\n",
    "                    final_pred[i] = np.mean(sample_preds)\n",
    "        \n",
    "        return final_pred\n",
    "    \n",
    "    def _strategy_confidence_weighted(self, X):\n",
    "        \"\"\"\n",
    "        STRATEGY 2: Confidence-Weighted Voting\n",
    "        Models that are more \"certain\" (further from 0.5) get more weight\n",
    "        \"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            predictions[name] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "            predictions['autoencoder'] = self.autoencoder.predict(X)\n",
    "        \n",
    "        # Base weights from Bayesian optimization (or equal)\n",
    "        base_weights = getattr(self, 'optimal_weights', {name: 1.0/len(predictions) for name in predictions})\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        final_pred = np.zeros(n_samples)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            weighted_sum = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            for name, preds in predictions.items():\n",
    "                p = preds[i]\n",
    "                # Confidence = how far from 0.5 (uncertain)\n",
    "                confidence = np.abs(p - 0.5) * 2  # 0 if p=0.5, 1 if p=0 or p=1\n",
    "                \n",
    "                base_w = base_weights.get(name, 1.0/len(predictions))\n",
    "                effective_weight = base_w * (0.5 + confidence)  # At least 50% of base weight\n",
    "                \n",
    "                weighted_sum += effective_weight * p\n",
    "                total_weight += effective_weight\n",
    "            \n",
    "            final_pred[i] = weighted_sum / (total_weight + 1e-9)\n",
    "        \n",
    "        return final_pred\n",
    "    \n",
    "    def _strategy_anomaly_boosted(self, X, alpha=0.3):\n",
    "        \"\"\"\n",
    "        STRATEGY 3: Anomaly-Boosted Ensemble\n",
    "        Use the Autoencoder as a MULTIPLIER, not a voter\n",
    "        Increases scores for transactions the autoencoder finds \"weird\"\n",
    "        \"\"\"\n",
    "        # Get ML model average\n",
    "        ml_preds = []\n",
    "        for name, model in self.models.items():\n",
    "            ml_preds.append(model.predict_proba(X)[:, 1])\n",
    "        \n",
    "        ml_avg = np.mean(ml_preds, axis=0)\n",
    "        \n",
    "        # Get autoencoder boost\n",
    "        if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "            ae_scores = self.autoencoder.predict(X)\n",
    "            ae_boost = 1 + alpha * ae_scores  # 1.0 to 1.3 multiplier\n",
    "            \n",
    "            final_pred = ml_avg * ae_boost\n",
    "            # Clip to [0, 1]\n",
    "            final_pred = np.clip(final_pred, 0, 1)\n",
    "        else:\n",
    "            final_pred = ml_avg\n",
    "        \n",
    "        return final_pred\n",
    "    \n",
    "    def _strategy_stacking_enhanced(self, X):\n",
    "        \"\"\"\n",
    "        STRATEGY 4: Bayesian-Optimized Enhanced Stacking\n",
    "        Uses base predictions (with optimized input weights) + summary statistics\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'enhanced_meta_model'):\n",
    "            # Train enhanced meta-model if not exists\n",
    "            self._train_enhanced_meta_model()\n",
    "        \n",
    "        # Get base predictions\n",
    "        base_preds = []\n",
    "        for name, model in self.models.items():\n",
    "            base_preds.append(model.predict_proba(X)[:, 1])\n",
    "        \n",
    "        if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "            base_preds.append(self.autoencoder.predict(X))\n",
    "        \n",
    "        base_preds = np.array(base_preds).T  # (n_samples, n_models)\n",
    "        n_base_models = base_preds.shape[1]\n",
    "        \n",
    "        # Apply Bayesian-optimized input weights\n",
    "        if hasattr(self, 'meta_input_weights') and len(self.meta_input_weights) == n_base_models:\n",
    "            for i in range(n_base_models):\n",
    "                base_preds[:, i] *= self.meta_input_weights[i]\n",
    "        \n",
    "        # Add meta-features\n",
    "        meta_features = np.column_stack([\n",
    "            base_preds,\n",
    "            np.max(base_preds, axis=1),      # Max across models\n",
    "            np.min(base_preds, axis=1),      # Min across models\n",
    "            np.std(base_preds, axis=1),      # Disagreement\n",
    "            np.sum(base_preds > 0.5, axis=1) # Consensus count\n",
    "        ])\n",
    "        \n",
    "        return self.enhanced_meta_model.predict_proba(meta_features)[:, 1]\n",
    "    \n",
    "    def _train_enhanced_meta_model(self):\n",
    "        \"\"\"\n",
    "        DIRECT ENHANCED META-MODEL (NO BAYESIAN OPTIMIZATION)\n",
    "        \n",
    "        Uses optimal parameters from previous optimization Trial 296:\n",
    "        - C: 0.08521596812289807\n",
    "        - class_weight_ratio: 2.8291961956998812\n",
    "        - input_weight_0 (xgboost): 0.35247629370537314\n",
    "        - input_weight_1 (lightgbm): 0.5673794299635898\n",
    "        - input_weight_2 (catboost): 1.894993092888986\n",
    "        - threshold: 0.478701698309106\n",
    "        \n",
    "        Performance achieved: F2=72.32%\n",
    "        \"\"\"\n",
    "        print(\"   ðŸš€ Training enhanced stacking meta-model (Using Bayesian Optimization backed optimal weights)...\")\n",
    "        print(\"      Using optimal parameters from previous optimization\")\n",
    "        \n",
    "        # Prepare base predictions\n",
    "        base_preds_val = []\n",
    "        model_names = []\n",
    "        for name, model in self.models.items():\n",
    "            base_preds_val.append(model.predict_proba(self.X_val_scaled)[:, 1])\n",
    "            model_names.append(name)\n",
    "        \n",
    "        if hasattr(self, 'autoencoder') and self.autoencoder.is_fitted:\n",
    "            base_preds_val.append(self.autoencoder.predict(self.X_val_scaled))\n",
    "            model_names.append('autoencoder')\n",
    "        \n",
    "        base_preds_val = np.array(base_preds_val).T  # (n_samples, n_models)\n",
    "        n_base_models = base_preds_val.shape[1]\n",
    "        \n",
    "        # Create meta-features\n",
    "        meta_features = np.column_stack([\n",
    "            base_preds_val,\n",
    "            np.max(base_preds_val, axis=1),\n",
    "            np.min(base_preds_val, axis=1),\n",
    "            np.std(base_preds_val, axis=1),\n",
    "            np.sum(base_preds_val > 0.5, axis=1)\n",
    "        ])\n",
    "        \n",
    "        # Optimal parameters from Trial 296 (F2: 0.7232)\n",
    "        best_params = {\n",
    "            'C': 0.08521596812289807,\n",
    "            'class_weight_ratio': 2.8291961956998812,\n",
    "            'input_weight_0': 0.35247629370537314,   # xgboost\n",
    "            'input_weight_1': 0.5673794299635898,    # lightgbm\n",
    "            'input_weight_2': 1.894993092888986,     # catboost\n",
    "            'threshold': 0.478701698309106\n",
    "        }\n",
    "        \n",
    "        # Build input weights for current number of models\n",
    "        input_weights = []\n",
    "        for i in range(n_base_models):\n",
    "            input_weights.append(best_params.get(f'input_weight_{i}', 1.0))\n",
    "        \n",
    "        # Apply input weights to base predictions\n",
    "        meta_features_weighted = meta_features.copy()\n",
    "        for i in range(n_base_models):\n",
    "            meta_features_weighted[:, i] *= input_weights[i]\n",
    "        \n",
    "        # Train final meta-model with optimal parameters\n",
    "        self.enhanced_meta_model = LogisticRegression(\n",
    "            C=best_params['C'],\n",
    "            class_weight={0: 1, 1: best_params['class_weight_ratio']},\n",
    "            random_state=RANDOM_STATE,\n",
    "            max_iter=1000,\n",
    "            solver='lbfgs'\n",
    "        )\n",
    "        self.enhanced_meta_model.fit(meta_features_weighted, self.y_val)\n",
    "        \n",
    "        # Store optimized parameters for inference\n",
    "        self.meta_input_weights = input_weights\n",
    "        self.meta_threshold = best_params['threshold']\n",
    "        \n",
    "        print(f\"      âœ… Meta-model configured with optimal parameters\")\n",
    "        print(f\"      Meta-model input weights:\")\n",
    "        for i, name in enumerate(model_names):\n",
    "            if i < len(input_weights):\n",
    "                print(f\"         {name:12s}: {input_weights[i]:.4f}\")\n",
    "        print(f\"      Optimized threshold: {self.meta_threshold:.3f}\")\n",
    "        print(\"   âœ“ Enhanced meta-model ready (Using Bayesian Optimization backed optimal weights)\")\n",
    "    \n",
    "    \n",
    "    def _strategy_delphi_consensus(self, X, n_rounds=3, convergence_threshold=0.01):\n",
    "        \"\"\"\n",
    "        STRATEGY 5: Delphi Consensus Voting\n",
    "        \n",
    "        Inspired by the Delphi Method (expert panel technique):\n",
    "        1. Each model predicts independently (Round 1)\n",
    "        2. Calculate group consensus and disagreement\n",
    "        3. Down-weight models that strongly disagree with majority\n",
    "        4. Repeat until convergence or max rounds\n",
    "        \n",
    "        Key insight: When models disagree, trust the \"wisdom of the crowd\"\n",
    "        and discount outlier predictions.\n",
    "        \"\"\"\n",
    "        # Round 1: Get initial predictions from all models\n",
    "        predictions = {}\n",
    "        for name, model in self.models.items():\n",
    "            predictions[name] = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        model_names = list(predictions.keys())\n",
    "        n_models = len(model_names)\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Initialize weights equally\n",
    "        weights = {name: 1.0 / n_models for name in model_names}\n",
    "        \n",
    "        prev_ensemble = None\n",
    "        \n",
    "        for round_num in range(n_rounds):\n",
    "            # Calculate weighted ensemble prediction\n",
    "            ensemble_pred = np.zeros(n_samples)\n",
    "            for name in model_names:\n",
    "                ensemble_pred += weights[name] * predictions[name]\n",
    "            \n",
    "            # Check convergence\n",
    "            if prev_ensemble is not None:\n",
    "                change = np.mean(np.abs(ensemble_pred - prev_ensemble))\n",
    "                if change < convergence_threshold:\n",
    "                    break\n",
    "            \n",
    "            prev_ensemble = ensemble_pred.copy()\n",
    "            \n",
    "            # Calculate per-sample disagreement for each model\n",
    "            # Models that deviate from consensus get down-weighted\n",
    "            new_weights = {}\n",
    "            for name in model_names:\n",
    "                # Disagreement = how far this model is from the ensemble\n",
    "                disagreement = np.abs(predictions[name] - ensemble_pred)\n",
    "                avg_disagreement = np.mean(disagreement)\n",
    "                \n",
    "                # More disagreement = lower weight (but never zero)\n",
    "                # Agreement score: 1 = perfect agreement, 0 = max disagreement\n",
    "                agreement_score = 1 - avg_disagreement\n",
    "                \n",
    "                # New weight: base weight adjusted by agreement\n",
    "                # Models in agreement get boosted, outliers get penalized\n",
    "                new_weights[name] = max(0.05, weights[name] * (0.5 + agreement_score))\n",
    "            \n",
    "            # Normalize weights to sum to 1\n",
    "            total_weight = sum(new_weights.values())\n",
    "            weights = {name: w / total_weight for name, w in new_weights.items()}\n",
    "        \n",
    "        # Final prediction with converged weights\n",
    "        final_pred = np.zeros(n_samples)\n",
    "        for name in model_names:\n",
    "            final_pred += weights[name] * predictions[name]\n",
    "        \n",
    "        return final_pred\n",
    "    \n",
    "    def compare_all_strategies(self, X=None, y=None, dataset_name=\"Validation\"):\n",
    "        \"\"\"\n",
    "        COMPREHENSIVE STRATEGY COMPARISON\n",
    "        Compare all voting strategies - selects based on BALANCED F1\n",
    "        \"\"\"\n",
    "        if X is None:\n",
    "            X = self.X_val_scaled\n",
    "            y = self.y_val\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ðŸ”¬ COMPARING ALL VOTING STRATEGIES ({dataset_name} Set)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        strategies = {\n",
    "            'average': lambda x: self.get_ensemble_predictions(x, mode='average'),\n",
    "            'weighted': lambda x: self.get_ensemble_predictions(x, mode='weighted'),\n",
    "            'bayesian_opt': lambda x: self.get_ensemble_predictions(x, mode='optimized'),\n",
    "            'tiered_vote': lambda x: self._strategy_tiered_threshold(x),\n",
    "            'conf_weighted': lambda x: self._strategy_confidence_weighted(x),\n",
    "            'delphi': lambda x: self._strategy_delphi_consensus(x),\n",
    "            'stack_enhanced': lambda x: self._strategy_stacking_enhanced(x),\n",
    "            'bouncer': lambda x: self.get_ensemble_predictions(x, mode='bouncer'),  # Rule Engine + ML Cascade\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(f\"\\n{'Strategy':<15} {'Acc%':>8} {'Prec%':>8} {'Rec%':>8} {'F1%':>8} {'F2%':>8} {'AUC%':>8}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        best_strategy = None\n",
    "        best_score = -float('inf')  # Score incorporating F2 + precision floor\n",
    "        best_metrics = (0, 0, 0)    # (F2, AUC, Precision) for display\n",
    "        \n",
    "        PRECISION_FLOOR = 0.60\n",
    "        \n",
    "        for name, strategy_fn in strategies.items():\n",
    "            try:\n",
    "                y_pred_proba = strategy_fn(X)\n",
    "                \n",
    "                # Find optimal threshold based on F2 with precision constraint\n",
    "                best_thresh = 0.5\n",
    "                best_local_score = -float('inf')\n",
    "                for thresh in np.linspace(0.3, 0.7, 41):\n",
    "                    y_pred = (y_pred_proba > thresh).astype(int)\n",
    "                    local_f2 = fbeta_score(y, y_pred, beta=2)\n",
    "                    local_prec = precision_score(y, y_pred, zero_division=0)\n",
    "                    \n",
    "                    # Apply same F2 + precision floor scoring as Bayesian optimization\n",
    "                    if local_prec < PRECISION_FLOOR:\n",
    "                        penalty = (PRECISION_FLOOR - local_prec) * 2.0\n",
    "                        local_score = local_f2 - penalty\n",
    "                    else:\n",
    "                        precision_bonus = min(0.05, (local_prec - PRECISION_FLOOR) * 0.1)\n",
    "                        local_score = local_f2 + precision_bonus\n",
    "                    \n",
    "                    if local_score > best_local_score:\n",
    "                        best_local_score = local_score\n",
    "                        best_thresh = thresh\n",
    "                \n",
    "                y_pred = (y_pred_proba > best_thresh).astype(int)\n",
    "                \n",
    "                acc = accuracy_score(y, y_pred) * 100\n",
    "                prec = precision_score(y, y_pred, zero_division=0) * 100\n",
    "                rec = recall_score(y, y_pred) * 100\n",
    "                f1 = f1_score(y, y_pred) * 100\n",
    "                f2 = fbeta_score(y, y_pred, beta=2) * 100\n",
    "                auc = roc_auc_score(y, y_pred_proba) * 100\n",
    "                \n",
    "                results[name] = {\n",
    "                    'accuracy': acc, 'precision': prec, 'recall': rec,\n",
    "                    'f1': f1, 'f2': f2, 'auc': auc, 'threshold': best_thresh,\n",
    "                    'score': best_local_score\n",
    "                }\n",
    "                \n",
    "                # Check if this strategy is better (using F2 + precision constraint score)\n",
    "                is_better = best_local_score > best_score\n",
    "                \n",
    "                marker = \"â­\" if is_better else \"  \"\n",
    "                prec_marker = \"âœ“\" if prec >= 60 else \"âœ—\"\n",
    "                print(f\"{name:<15} {acc:>8.2f} {prec:>8.2f}{prec_marker} {rec:>8.2f} {f1:>8.2f} {f2:>8.2f} {auc:>8.2f} {marker}\")\n",
    "                \n",
    "                if is_better:\n",
    "                    best_score = best_local_score\n",
    "                    best_metrics = (f2, auc, prec)\n",
    "                    best_strategy = name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{name:<15} - Failed: {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"\\nðŸ† BEST STRATEGY: {best_strategy.upper()} (F2 + Precision â‰¥ 60%)\")\n",
    "        print(f\"   F2: {best_metrics[0]:.2f}%, AUC: {best_metrics[1]:.2f}%, Precision: {best_metrics[2]:.2f}%\")\n",
    "        print(f\"   Optimal Threshold: {results[best_strategy]['threshold']:.3f}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.best_voting_strategy = best_strategy\n",
    "        self.strategy_comparison_results = results\n",
    "        \n",
    "        return best_strategy, results\n",
    "\n",
    "\n",
    "    def find_optimal_threshold_multi_objective(self):\n",
    "        \"\"\"Multi-objective threshold optimization\"\"\"\n",
    "        print(\"\\nðŸŽ¯ Multi-Objective Threshold Optimization...\")\n",
    "        \n",
    "        # Use the best ensemble strategy found\n",
    "        strategy = getattr(self, 'best_ensemble_strategy', 'weighted')\n",
    "        y_pred_proba = self.get_ensemble_predictions(self.X_val_scaled, mode=strategy)\n",
    "        \n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        \n",
    "        results = []\n",
    "        print(f\"\\n{'Thresh':>8} {'Acc%':>8} {'Prec%':>8} {'Rec%':>8} {'F1%':>8} {'F2%':>8} {'MCC':>8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        best_balanced = None\n",
    "        best_f2 = None\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            y_pred = (y_pred_proba > thresh).astype(int)\n",
    "            \n",
    "            acc = accuracy_score(self.y_val, y_pred) * 100\n",
    "            prec = precision_score(self.y_val, y_pred, zero_division=0) * 100\n",
    "            rec = recall_score(self.y_val, y_pred) * 100\n",
    "            f1 = f1_score(self.y_val, y_pred) * 100\n",
    "            f2 = fbeta_score(self.y_val, y_pred, beta=2) * 100\n",
    "            mcc = matthews_corrcoef(self.y_val, y_pred)\n",
    "            \n",
    "            balanced_score = (acc + prec + rec + f1) / 4\n",
    "            \n",
    "            res = {\n",
    "                'threshold': float(thresh),\n",
    "                'accuracy': float(acc),\n",
    "                'precision': float(prec),\n",
    "                'recall': float(rec),\n",
    "                'f1': float(f1),\n",
    "                'f2': float(f2),\n",
    "                'mcc': float(mcc),\n",
    "                'balanced_score': float(balanced_score)\n",
    "            }\n",
    "            results.append(res)\n",
    "            \n",
    "            # Track best candidates\n",
    "            if prec >= 80.0 and rec >= 80.0:\n",
    "                if best_balanced is None or balanced_score > best_balanced['balanced_score']:\n",
    "                    best_balanced = res\n",
    "            \n",
    "            if f2 > 0:\n",
    "                if best_f2 is None or f2 > best_f2['f2']:\n",
    "                    best_f2 = res\n",
    "            \n",
    "            if thresh % 0.1 < 0.025:  # Print every 10th\n",
    "                print(f\"{thresh:>8.3f} {acc:>8.2f} {prec:>8.2f} {rec:>8.2f} {f1:>8.2f} {f2:>8.2f} {mcc:>8.4f}\")\n",
    "        \n",
    "        # Selection logic\n",
    "        if best_balanced:\n",
    "            optimal = best_balanced\n",
    "            print(f\"\\nðŸ† SELECTED: Balanced Optimum (Precâ‰¥80%, Recâ‰¥80%)\")\n",
    "        elif best_f2:\n",
    "            optimal = best_f2\n",
    "            print(f\"\\nðŸ¥ˆ SELECTED: F2-Optimized (Recall Priority)\")\n",
    "        else:\n",
    "            optimal = max(results, key=lambda x: x['balanced_score'])\n",
    "            print(f\"\\nðŸ¥‰ SELECTED: Max Balanced Score\")\n",
    "        \n",
    "        print(f\"\\nâœ… Optimal Threshold: {optimal['threshold']:.3f}\")\n",
    "        print(f\"   Accuracy:  {optimal['accuracy']:.2f}%\")\n",
    "        print(f\"   Precision: {optimal['precision']:.2f}%\")\n",
    "        print(f\"   Recall:    {optimal['recall']:.2f}%\")\n",
    "        print(f\"   F1-Score:  {optimal['f1']:.2f}%\")\n",
    "        print(f\"   F2-Score:  {optimal['f2']:.2f}%\")\n",
    "        print(f\"   MCC:       {optimal['mcc']:.4f}\")\n",
    "        \n",
    "        self.optimal_threshold = optimal['threshold']\n",
    "        return results, optimal\n",
    "    \n",
    "    def evaluate_final(self):\n",
    "        \"\"\"Comprehensive final evaluation using HYBRID: Bouncer + Best ML Strategy\n",
    "        \n",
    "        Uses same cascade logic as threshold analysis with best_voting_strategy\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ“Š FINAL CASCADED SYSTEM EVALUATION ON TEST SET\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Use best_voting_strategy for consistency with threshold analysis\n",
    "        ml_strategy = getattr(self, 'best_voting_strategy', 'average')\n",
    "        if ml_strategy == 'bouncer':\n",
    "            ml_strategy = 'average'  # Fallback if bouncer was selected\n",
    "        \n",
    "        print(f\"   ðŸš¨ HYBRID MODE: Rule Engine (Bouncer) + {ml_strategy.upper()}\")\n",
    "        \n",
    "        # ============= STEP 1: Rule Engine Evaluation (Hard Block) =============\n",
    "        final_predictions = np.zeros(len(self.X_test_scaled))\n",
    "        HARD_BLOCK_THRESHOLD = 0.85  # Scores >= 85 = definite fraud\n",
    "        \n",
    "        if hasattr(self, 'rule_engine') and hasattr(self, 'feature_names') and hasattr(self, 'X_test'):\n",
    "            # CRITICAL: Use UNSCALED X_test for Rule Engine (thresholds need original values)\n",
    "            rule_scores = self.rule_engine.predict_batch(self.X_test, self.feature_names)\n",
    "            \n",
    "            # Hard block obvious frauds\n",
    "            hard_block_mask = rule_scores >= HARD_BLOCK_THRESHOLD\n",
    "            n_hard_blocked = hard_block_mask.sum()\n",
    "            final_predictions[hard_block_mask] = 1.0  # Flag as fraud\n",
    "            \n",
    "            print(f\"   ðŸš¨ Rule Engine Hard Blocked: {n_hard_blocked}/{len(self.X_test_scaled)} ({n_hard_blocked/len(self.X_test_scaled)*100:.2f}%)\")\n",
    "            \n",
    "            # ============= STEP 2: ML Strategy for Non-Blocked Transactions =============\n",
    "            soft_mask = ~hard_block_mask\n",
    "            n_soft = soft_mask.sum()\n",
    "            \n",
    "            if n_soft > 0:\n",
    "                X_soft = self.X_test_scaled[soft_mask]\n",
    "                \n",
    "                # Get ML predictions using best strategy\n",
    "                if ml_strategy == 'stack_enhanced':\n",
    "                    ml_preds = self._strategy_stacking_enhanced(X_soft)\n",
    "                elif ml_strategy == 'tiered_vote':\n",
    "                    ml_preds = self._strategy_tiered_threshold(X_soft)\n",
    "                elif ml_strategy == 'conf_weighted':\n",
    "                    ml_preds = self._strategy_confidence_weighted(X_soft)\n",
    "                elif ml_strategy == 'delphi':\n",
    "                    ml_preds = self._strategy_delphi_consensus(X_soft)\n",
    "                else:\n",
    "                    mode_map = {'average': 'average', 'weighted': 'weighted', 'bayesian_opt': 'optimized'}\n",
    "                    ml_preds = self.get_ensemble_predictions(X_soft, mode=mode_map.get(ml_strategy, 'average'))\n",
    "                \n",
    "                # Apply soft cascade: blend rule scores with ML predictions (BNN-Enhanced)\n",
    "                soft_rule_scores = rule_scores[soft_mask]\n",
    "                X_soft_unscaled = self.X_test[soft_mask] if hasattr(self, 'X_test') else None\n",
    "                \n",
    "                # Use BNN to identify high-risk transactions\n",
    "                if hasattr(self, 'high_risk_identifier') and self.high_risk_identifier.is_fitted and X_soft_unscaled is not None:\n",
    "                    bnn_result = self.high_risk_identifier.predict(\n",
    "                        X_soft_unscaled,\n",
    "                        self.feature_names,\n",
    "                        self.rule_engine\n",
    "                    )\n",
    "                    high_risk_mask = bnn_result['is_high_risk']\n",
    "                else:\n",
    "                    # Fallback to hardcoded threshold\n",
    "                    HIGH_RISK_THRESHOLD = 0.60\n",
    "                    high_risk_mask = soft_rule_scores >= HIGH_RISK_THRESHOLD\n",
    "                \n",
    "                # FIX: Also use HIGH RISK blend when rule score >= 60%\n",
    "                # This ensures strong rule signals (like velocity attacks) are not diluted by BNN\n",
    "                strong_rule_mask = soft_rule_scores >= 0.60\n",
    "                use_high_risk_blend = high_risk_mask | strong_rule_mask\n",
    "                \n",
    "                low_risk_mask = ~use_high_risk_blend\n",
    "                blended = np.zeros(n_soft)\n",
    "                \n",
    "                # High risk (BNN OR strong rules): 35% rule + 65% ML\n",
    "                if use_high_risk_blend.sum() > 0:\n",
    "                    blended[use_high_risk_blend] = (\n",
    "                        ml_preds[use_high_risk_blend] * 0.65 +\n",
    "                        soft_rule_scores[use_high_risk_blend] * 0.35\n",
    "                    )\n",
    "                \n",
    "                # Low risk (BNN says legit AND rules < 60%): 10% rule + 90% ML\n",
    "                if low_risk_mask.sum() > 0:\n",
    "                    blended[low_risk_mask] = (\n",
    "                        ml_preds[low_risk_mask] * 0.90 +\n",
    "                        soft_rule_scores[low_risk_mask] * 0.10\n",
    "                    )\n",
    "                \n",
    "                final_predictions[soft_mask] = np.clip(blended, 0, 1)\n",
    "                \n",
    "                n_high_risk = use_high_risk_blend.sum()\n",
    "                n_strong_rules_only = (strong_rule_mask & ~high_risk_mask).sum()\n",
    "                bnn_status = \"BNN\" if (hasattr(self, 'high_risk_identifier') and self.high_risk_identifier.is_fitted) else \"threshold\"\n",
    "                print(f\"   âš¡ High Risk Blend (35% rule, {bnn_status}+Rules): {n_high_risk} ({n_strong_rules_only} by rules only)\")\n",
    "                print(f\"   ðŸ“Š Low Risk Blend (10% rule): {low_risk_mask.sum()}\")\n",
    "        else:\n",
    "            # Fallback: No rule engine, use pure ML strategy\n",
    "            print(f\"   âš ï¸ Rule Engine not available, using pure {ml_strategy.upper()}\")\n",
    "            if ml_strategy == 'stack_enhanced':\n",
    "                final_predictions = self._strategy_stacking_enhanced(self.X_test_scaled)\n",
    "            elif ml_strategy == 'tiered_vote':\n",
    "                final_predictions = self._strategy_tiered_threshold(self.X_test_scaled)\n",
    "            elif ml_strategy == 'conf_weighted':\n",
    "                final_predictions = self._strategy_confidence_weighted(self.X_test_scaled)\n",
    "            elif ml_strategy == 'delphi':\n",
    "                final_predictions = self._strategy_delphi_consensus(self.X_test_scaled)\n",
    "            else:\n",
    "                mode_map = {'average': 'average', 'weighted': 'weighted', 'bayesian_opt': 'optimized'}\n",
    "                final_predictions = self.get_ensemble_predictions(self.X_test_scaled, mode=mode_map.get(ml_strategy, 'average'))\n",
    "        \n",
    "        y_pred_proba = final_predictions\n",
    "        y_pred = (y_pred_proba > self.optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        acc = accuracy_score(self.y_test, y_pred) * 100\n",
    "        prec = precision_score(self.y_test, y_pred, zero_division=0) * 100\n",
    "        rec = recall_score(self.y_test, y_pred) * 100\n",
    "        f1 = f1_score(self.y_test, y_pred) * 100\n",
    "        f2 = fbeta_score(self.y_test, y_pred, beta=2) * 100\n",
    "        roc_auc = roc_auc_score(self.y_test, y_pred_proba) * 100\n",
    "        pr_auc = average_precision_score(self.y_test, y_pred_proba) * 100\n",
    "        mcc = matthews_corrcoef(self.y_test, y_pred)\n",
    "        kappa = cohen_kappa_score(self.y_test, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(self.y_test, y_pred) * 100\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ METRICS (Threshold={self.optimal_threshold:.3f}):\")\n",
    "        print(f\"   Accuracy:            {acc:.2f}% {'âœ…' if acc >= 80 else 'âš ï¸'}\")\n",
    "        print(f\"   Balanced Accuracy:   {balanced_acc:.2f}%\")\n",
    "        print(f\"   Precision:           {prec:.2f}% {'âœ…' if prec >= 80 else 'âš ï¸'}\")\n",
    "        print(f\"   Recall:              {rec:.2f}% {'âœ…' if rec >= 80 else 'âš ï¸'}\")\n",
    "        print(f\"   F1-Score:            {f1:.2f}% {'âœ…' if f1 >= 80 else 'âš ï¸'}\")\n",
    "        print(f\"   F2-Score:            {f2:.2f}%\")\n",
    "        print(f\"   ROC-AUC:             {roc_auc:.2f}%\")\n",
    "        print(f\"   PR-AUC:              {pr_auc:.2f}%\")\n",
    "        print(f\"   MCC:                 {mcc:.4f}\")\n",
    "        print(f\"   Cohen's Kappa:       {kappa:.4f}\")\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(f\"\\nðŸ”¢ Confusion Matrix:\")\n",
    "        print(f\"   TN: {cm[0,0]:6d} | FP: {cm[0,1]:6d}\")\n",
    "        print(f\"   FN: {cm[1,0]:6d} | TP: {cm[1,1]:6d}\")\n",
    "        \n",
    "        # Per-model performance\n",
    "        print(f\"\\nðŸ“Š Individual Model Performance:\")\n",
    "        for name, model in self.models.items():\n",
    "            model_proba = model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "            model_pred = (model_proba > self.optimal_threshold).astype(int)\n",
    "            model_f1 = f1_score(self.y_test, model_pred) * 100\n",
    "            model_auc = roc_auc_score(self.y_test, model_proba) * 100\n",
    "            print(f\"   {name:12s}: F1={model_f1:.2f}%, AUC={model_auc:.2f}%\")\n",
    "        \n",
    "        # Compare ALL ensemble strategies on test set (matching compare_all_strategies)\n",
    "        print(f\"\\nðŸ“Š Ensemble Strategy Comparison (Test Set):\")\n",
    "        strategies_to_compare = [\n",
    "            ('average', 'average'),\n",
    "            ('weighted', 'weighted'),\n",
    "            ('bayesian_opt', 'optimized'),\n",
    "            ('conf_weighted', None),  # Custom strategy\n",
    "            ('stack_enhanced', None),  # Custom strategy (Bayesian stacking)\n",
    "        ]\n",
    "        \n",
    "        for strategy_name, mode in strategies_to_compare:\n",
    "            try:\n",
    "                if strategy_name == 'stack_enhanced':\n",
    "                    strat_proba = self._strategy_stacking_enhanced(self.X_test_scaled)\n",
    "                elif strategy_name == 'conf_weighted':\n",
    "                    strat_proba = self._strategy_confidence_weighted(self.X_test_scaled)\n",
    "                else:\n",
    "                    strat_proba = self.get_ensemble_predictions(self.X_test_scaled, mode=mode)\n",
    "                \n",
    "                strat_pred = (strat_proba > self.optimal_threshold).astype(int)\n",
    "                strat_f1 = f1_score(self.y_test, strat_pred) * 100\n",
    "                strat_f2 = fbeta_score(self.y_test, strat_pred, beta=2) * 100\n",
    "                strat_auc = roc_auc_score(self.y_test, strat_proba) * 100\n",
    "                strat_rec = recall_score(self.y_test, strat_pred) * 100\n",
    "                \n",
    "                # Show which strategy is being used\n",
    "                marker = 'ðŸ†' if strategy_name == ml_strategy else '  '\n",
    "                print(f\"   {marker} {strategy_name:15s}: F2={strat_f2:.2f}%, F1={strat_f1:.2f}%, Rec={strat_rec:.2f}%, AUC={strat_auc:.2f}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ {strategy_name:15s}: Failed - {str(e)[:30]}\")\n",
    "        \n",
    "        # Show optimized weights if available\n",
    "        if hasattr(self, 'optimal_weights'):\n",
    "            print(f\"\\nðŸŽ¯ Optimized Ensemble Weights:\")\n",
    "            for name, weight in self.optimal_weights.items():\n",
    "                print(f\"   {name:12s}: {weight:.4f} ({weight*100:.2f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': acc, 'balanced_accuracy': balanced_acc,\n",
    "            'precision': prec, 'recall': rec, \n",
    "            'f1': f1, 'f2': f2,\n",
    "            'roc_auc': roc_auc, 'pr_auc': pr_auc, \n",
    "            'mcc': mcc, 'kappa': kappa,\n",
    "            'ensemble_mode': ml_strategy\n",
    "        }\n",
    "    \n",
    "    def evaluate_cascaded_pipeline(self, X_test=None, y_test=None):\n",
    "        \"\"\"\n",
    "        END-TO-END EVALUATION OF CASCADED FRAUD DETECTION PIPELINE\n",
    "        \n",
    "        Matches inference_demo.py cascade logic:\n",
    "        1. Rule Engine (â‰¥85% â†’ HARD BLOCK, skip ML)\n",
    "        2. BNN High-Risk Identification\n",
    "        3. ML Ensemble with weighted predictions\n",
    "        4. Cascaded Blending:\n",
    "           - HIGH RISK (BNN=True OR Rulesâ‰¥60%): 65%ML + 35%Rule\n",
    "           - LOW RISK: 90%ML + 10%Rule\n",
    "        5. Final threshold comparison\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ”„ CASCADED PIPELINE EVALUATION ON TEST SET\")\n",
    "        print(\"   (Matches inference_demo.py pipeline)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Use stored test data if not provided\n",
    "        if X_test is None:\n",
    "            X_test_scaled = self.X_test_scaled\n",
    "            X_test_unscaled = self.X_test if hasattr(self, 'X_test') else None\n",
    "        else:\n",
    "            X_test_scaled = X_test\n",
    "            X_test_unscaled = None\n",
    "        if y_test is None:\n",
    "            y_test = self.y_test\n",
    "        \n",
    "        n_total = len(y_test)\n",
    "        n_actual_fraud = int(y_test.sum())\n",
    "        n_actual_legit = n_total - n_actual_fraud\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Test Set: {n_total} transactions ({n_actual_fraud} fraud, {n_actual_legit} legit)\")\n",
    "        \n",
    "        # Thresholds matching inference_demo.py\n",
    "        HARD_BLOCK_THRESHOLD = 0.85  # Rule score â‰¥85% = definite fraud\n",
    "        HIGH_RISK_THRESHOLD = 0.60   # For blending decision\n",
    "        \n",
    "        # =========================\n",
    "        # STAGE 1: RULE-BASED MODEL (HARD BLOCK)\n",
    "        # =========================\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"STAGE 1: RULE ENGINE (Hard Block â‰¥85%)\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        final_predictions = np.zeros(n_total)\n",
    "        rule_scores_normalized = np.zeros(n_total)\n",
    "        \n",
    "        if hasattr(self, 'rule_engine') and hasattr(self, 'feature_names') and X_test_unscaled is not None:\n",
    "            # Use UNSCALED data for rule engine\n",
    "            rule_scores = self.rule_engine.predict_batch(X_test_unscaled, self.feature_names)\n",
    "            rule_scores_normalized = rule_scores / 100.0  # Normalize to 0-1\n",
    "            \n",
    "            # Hard block obvious frauds\n",
    "            hard_block_mask = rule_scores_normalized >= HARD_BLOCK_THRESHOLD\n",
    "            n_hard_blocked = hard_block_mask.sum()\n",
    "            final_predictions[hard_block_mask] = 1.0\n",
    "            \n",
    "            # Metrics for hard-blocked\n",
    "            rule_tp = ((hard_block_mask) & (y_test == 1)).sum()\n",
    "            rule_fp = ((hard_block_mask) & (y_test == 0)).sum()\n",
    "            \n",
    "            print(f\"   Hard Block Threshold: â‰¥{int(HARD_BLOCK_THRESHOLD*100)}%\")\n",
    "            print(f\"   Transactions blocked: {n_hard_blocked}/{n_total} ({n_hard_blocked/n_total*100:.1f}%)\")\n",
    "            print(f\"   âœ“ True Positives (fraud caught): {rule_tp}\")\n",
    "            print(f\"   âœ— False Positives (legit blocked): {rule_fp}\")\n",
    "            if n_actual_fraud > 0:\n",
    "                print(f\"   Stage 1 Recall: {rule_tp/n_actual_fraud*100:.2f}%\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ Rule engine not available or X_test unscaled not found\")\n",
    "            hard_block_mask = np.zeros(n_total, dtype=bool)\n",
    "            rule_tp = 0\n",
    "            rule_fp = 0\n",
    "            n_hard_blocked = 0\n",
    "        \n",
    "        # =========================\n",
    "        # STAGE 2: BNN + ML ENSEMBLE (Non-blocked transactions)\n",
    "        # =========================\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"STAGE 2: BNN + ML ENSEMBLE (with Cascaded Blending)\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        soft_mask = ~hard_block_mask\n",
    "        n_to_ml = soft_mask.sum()\n",
    "        \n",
    "        ml_tp = 0\n",
    "        ml_fp = 0\n",
    "        n_high_risk_blend = 0\n",
    "        n_low_risk_blend = 0\n",
    "        \n",
    "        if n_to_ml > 0:\n",
    "            X_soft = X_test_scaled[soft_mask]\n",
    "            y_soft = y_test[soft_mask] if isinstance(y_test, np.ndarray) else y_test.values[soft_mask]\n",
    "            soft_rule_scores = rule_scores_normalized[soft_mask]\n",
    "            \n",
    "            # Get ML predictions using optimal weights\n",
    "            models_to_use = getattr(self, 'calibrated_models', self.models)\n",
    "            ml_probs = np.zeros(len(X_soft))\n",
    "            \n",
    "            if hasattr(self, 'optimal_weights') and self.optimal_weights:\n",
    "                for name, model in models_to_use.items():\n",
    "                    weight = self.optimal_weights.get(name, 0)\n",
    "                    if weight > 0:\n",
    "                        ml_probs += weight * model.predict_proba(X_soft)[:, 1]\n",
    "            else:\n",
    "                ml_preds_list = [model.predict_proba(X_soft)[:, 1] for model in models_to_use.values()]\n",
    "                ml_probs = np.mean(ml_preds_list, axis=0)\n",
    "            \n",
    "            # BNN High-Risk Detection\n",
    "            if hasattr(self, 'high_risk_identifier') and self.high_risk_identifier.is_fitted and X_test_unscaled is not None:\n",
    "                X_soft_unscaled = X_test_unscaled[soft_mask]\n",
    "                bnn_result = self.high_risk_identifier.predict(\n",
    "                    X_soft_unscaled,\n",
    "                    self.feature_names,\n",
    "                    self.rule_engine\n",
    "                )\n",
    "                bnn_high_risk_mask = bnn_result['is_high_risk']\n",
    "                bnn_status = \"BNN\"\n",
    "            else:\n",
    "                # Fallback: use rule threshold for high-risk classification\n",
    "                bnn_high_risk_mask = np.zeros(len(X_soft), dtype=bool)\n",
    "                bnn_status = \"threshold-only\"\n",
    "            \n",
    "            # CASCADED BLENDING (matching inference_demo.py)\n",
    "            # High Risk = BNN says high risk OR strong rule signals (â‰¥60%)\n",
    "            strong_rule_mask = soft_rule_scores >= HIGH_RISK_THRESHOLD\n",
    "            use_high_risk_blend = bnn_high_risk_mask | strong_rule_mask\n",
    "            low_risk_mask = ~use_high_risk_blend\n",
    "            \n",
    "            blended_scores = np.zeros(len(X_soft))\n",
    "            \n",
    "            # High Risk Blend: 65% ML + 35% Rule\n",
    "            if use_high_risk_blend.sum() > 0:\n",
    "                blended_scores[use_high_risk_blend] = (\n",
    "                    ml_probs[use_high_risk_blend] * 0.65 +\n",
    "                    soft_rule_scores[use_high_risk_blend] * 0.35\n",
    "                )\n",
    "            \n",
    "            # Low Risk Blend: 90% ML + 10% Rule\n",
    "            if low_risk_mask.sum() > 0:\n",
    "                blended_scores[low_risk_mask] = (\n",
    "                    ml_probs[low_risk_mask] * 0.90 +\n",
    "                    soft_rule_scores[low_risk_mask] * 0.10\n",
    "                )\n",
    "            \n",
    "            blended_scores = np.clip(blended_scores, 0, 1)\n",
    "            \n",
    "            # Apply threshold\n",
    "            ml_flags = (blended_scores >= self.optimal_threshold).astype(int)\n",
    "            \n",
    "            # Assign to final predictions\n",
    "            final_predictions[soft_mask] = blended_scores\n",
    "            \n",
    "            # Metrics\n",
    "            ml_tp = ((ml_flags == 1) & (y_soft == 1)).sum()\n",
    "            ml_fp = ((ml_flags == 1) & (y_soft == 0)).sum()\n",
    "            n_fraud_in_soft = (y_soft == 1).sum()\n",
    "            n_high_risk_blend = use_high_risk_blend.sum()\n",
    "            n_low_risk_blend = low_risk_mask.sum()\n",
    "            \n",
    "            print(f\"   Transactions received: {n_to_ml}/{n_total} ({n_to_ml/n_total*100:.1f}%)\")\n",
    "            print(f\"   High-Risk Classification: {bnn_status}\")\n",
    "            print(f\"   âš¡ High Risk Blend (65%ML+35%Rule): {n_high_risk_blend} txns\")\n",
    "            print(f\"   ðŸ“Š Low Risk Blend (90%ML+10%Rule): {n_low_risk_blend} txns\")\n",
    "            print(f\"   Fraud in soft zone: {n_fraud_in_soft}\")\n",
    "            print(f\"   âœ“ Additional fraud caught (ML): {ml_tp}\")\n",
    "            print(f\"   âœ— False positives (ML): {ml_fp}\")\n",
    "            if n_fraud_in_soft > 0:\n",
    "                print(f\"   Stage 2 Recall (on soft zone): {ml_tp/n_fraud_in_soft*100:.2f}%\")\n",
    "        \n",
    "        # =========================\n",
    "        # FINAL COMBINED RESULTS\n",
    "        # =========================\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"FINAL CASCADED RESULTS\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        y_pred = (final_predictions >= self.optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        final_tp = ((y_pred == 1) & (y_test == 1)).sum()\n",
    "        final_fp = ((y_pred == 1) & (y_test == 0)).sum()\n",
    "        final_tn = ((y_pred == 0) & (y_test == 0)).sum()\n",
    "        final_fn = ((y_pred == 0) & (y_test == 1)).sum()\n",
    "        \n",
    "        accuracy = (final_tp + final_tn) / n_total\n",
    "        precision = final_tp / (final_tp + final_fp) if (final_tp + final_fp) > 0 else 0\n",
    "        recall = final_tp / (final_tp + final_fn) if (final_tp + final_fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f2 = 5 * precision * recall / (4 * precision + recall) if (4 * precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n   ðŸ“ˆ FINAL CASCADED METRICS (threshold={self.optimal_threshold:.3f}):\")\n",
    "        print(f\"   â”Œ{'â”€'*50}â”\")\n",
    "        print(f\"   â”‚ Accuracy:   {accuracy*100:6.2f}% {'âœ…' if accuracy >= 0.85 else 'ðŸ“Š':<20}â”‚\")\n",
    "        print(f\"   â”‚ Precision:  {precision*100:6.2f}% {'âœ…' if precision >= 0.50 else 'ðŸ“Š':<20}â”‚\")\n",
    "        print(f\"   â”‚ Recall:     {recall*100:6.2f}% {'âœ…' if recall >= 0.80 else 'ðŸ“Š':<20}â”‚\")\n",
    "        print(f\"   â”‚ F1-Score:   {f1*100:6.2f}% {'âœ…' if f1 >= 0.65 else 'ðŸ“Š':<20}â”‚\")\n",
    "        print(f\"   â”‚ F2-Score:   {f2*100:6.2f}% (recall-weighted)      â”‚\")\n",
    "        print(f\"   â””{'â”€'*50}â”˜\")\n",
    "        \n",
    "        print(f\"\\n   ðŸ”¢ Confusion Matrix:\")\n",
    "        print(f\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "        print(f\"   â”‚        â”‚ Pred 0 â”‚ Pred 1 â”‚\")\n",
    "        print(f\"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "        print(f\"   â”‚ Act 0  â”‚ {final_tn:6d} â”‚ {final_fp:6d} â”‚\")\n",
    "        print(f\"   â”‚ Act 1  â”‚ {final_fn:6d} â”‚ {final_tp:6d} â”‚\")\n",
    "        print(f\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "        \n",
    "        print(f\"\\n   ðŸ“Š FRAUD DETECTION BREAKDOWN:\")\n",
    "        print(f\"   â€¢ Total fraud cases: {n_actual_fraud}\")\n",
    "        print(f\"   â€¢ Caught by Rules (Stage 1, Hard Block): {rule_tp} ({rule_tp/n_actual_fraud*100:.1f}%)\")\n",
    "        print(f\"   â€¢ Caught by ML (Stage 2, Blended): {ml_tp} ({ml_tp/n_actual_fraud*100:.1f}%)\")\n",
    "        print(f\"   â€¢ Total caught: {final_tp} ({recall*100:.1f}%)\")\n",
    "        print(f\"   â€¢ Missed: {final_fn} ({final_fn/n_actual_fraud*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n   ðŸ”§ PIPELINE CONFIGURATION:\")\n",
    "        print(f\"   â€¢ Hard Block Threshold: â‰¥{int(HARD_BLOCK_THRESHOLD*100)}%\")\n",
    "        print(f\"   â€¢ High-Risk Blend Threshold: â‰¥{int(HIGH_RISK_THRESHOLD*100)}%\")\n",
    "        print(f\"   â€¢ ML Decision Threshold: {self.optimal_threshold:.3f}\")\n",
    "        print(f\"   â€¢ Blending: HIGH=65%ML+35%Rule, LOW=90%ML+10%Rule\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy * 100,\n",
    "            'precision': precision * 100,\n",
    "            'recall': recall * 100,\n",
    "            'f1': f1 * 100,\n",
    "            'f2': f2 * 100,\n",
    "            'rule_catches': int(rule_tp),\n",
    "            'ml_catches': int(ml_tp),\n",
    "            'total_catches': int(final_tp),\n",
    "            'missed': int(final_fn),\n",
    "            'high_risk_blend_count': int(n_high_risk_blend),\n",
    "            'low_risk_blend_count': int(n_low_risk_blend),\n",
    "            'confusion_matrix': {\n",
    "                'tp': int(final_tp), 'fp': int(final_fp),\n",
    "                'tn': int(final_tn), 'fn': int(final_fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def plot_comprehensive_analysis(self):\n",
    "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
    "        print(\"\\nðŸ“Š Generating comprehensive analysis plots...\")\n",
    "        \n",
    "        ensemble_mode = getattr(self, 'best_ensemble_strategy', 'optimized')\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # 1. Feature Importance\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        if self.feature_importance is not None:\n",
    "            top_features = self.feature_importance.head(20)\n",
    "            sns.barplot(data=top_features, x='importance', y='feature', palette='viridis', ax=ax1)\n",
    "            ax1.set_title('Top 20 Feature Importance (XGBoost)', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel('Importance')\n",
    "        \n",
    "        # 2. ROC Curve (Multiple Strategies)\n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        strategies = ['average', 'weighted', 'optimized', 'stacking']\n",
    "        colors = ['blue', 'orange', 'green', 'red']\n",
    "        \n",
    "        for strategy, color in zip(strategies, colors):\n",
    "            try:\n",
    "                y_pred_proba = self.get_ensemble_predictions(self.X_test_scaled, mode=strategy)\n",
    "                fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
    "                roc_auc = roc_auc_score(self.y_test, y_pred_proba)\n",
    "                label = f'{strategy.upper()}: AUC={roc_auc:.3f}'\n",
    "                if strategy == ensemble_mode:\n",
    "                    ax2.plot(fpr, tpr, color=color, linewidth=3, label=label + ' â˜…')\n",
    "                else:\n",
    "                    ax2.plot(fpr, tpr, color=color, linewidth=2, alpha=0.6, label=label)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        ax2.plot([0, 1], [0, 1], 'navy', linestyle='--', alpha=0.5)\n",
    "        ax2.set_title('ROC Curves (Ensemble Strategies)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('False Positive Rate')\n",
    "        ax2.set_ylabel('True Positive Rate')\n",
    "        ax2.legend(loc='lower right', fontsize=9)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Precision-Recall Curve\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        y_pred_proba = self.get_ensemble_predictions(self.X_test_scaled, mode=ensemble_mode)\n",
    "        precision, recall, _ = precision_recall_curve(self.y_test, y_pred_proba)\n",
    "        pr_auc = average_precision_score(self.y_test, y_pred_proba)\n",
    "        ax3.plot(recall, precision, 'green', linewidth=2, label=f'AP={pr_auc:.3f}')\n",
    "        ax3.set_title(f'Precision-Recall ({ensemble_mode.upper()})', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Recall')\n",
    "        ax3.set_ylabel('Precision')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Confusion Matrix\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        y_pred = (y_pred_proba > self.optimal_threshold).astype(int)\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4, \n",
    "                   xticklabels=['Legitimate', 'Fraud'],\n",
    "                   yticklabels=['Legitimate', 'Fraud'])\n",
    "        ax4.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "        ax4.set_ylabel('True Label')\n",
    "        ax4.set_xlabel('Predicted Label')\n",
    "        \n",
    "        # 5. Ensemble Weights Visualization\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        if hasattr(self, 'optimal_weights'):\n",
    "            model_names = list(self.optimal_weights.keys())\n",
    "            weights = list(self.optimal_weights.values())\n",
    "            colors_weights = ['#1f77b4', '#ff7f0e', '#2ca02c'][:len(weights)]\n",
    "            \n",
    "            bars = ax5.barh(model_names, weights, color=colors_weights, alpha=0.8)\n",
    "            ax5.set_title('Bayesian-Optimized Ensemble Weights', fontsize=14, fontweight='bold')\n",
    "            ax5.set_xlabel('Weight')\n",
    "            ax5.set_xlim([0, 1])\n",
    "            \n",
    "            # Add percentage labels\n",
    "            for bar, weight in zip(bars, weights):\n",
    "                width = bar.get_width()\n",
    "                ax5.text(width + 0.02, bar.get_y() + bar.get_height()/2.,\n",
    "                        f'{weight*100:.1f}%', ha='left', va='center', fontweight='bold')\n",
    "        else:\n",
    "            ax5.text(0.5, 0.5, 'Weights not optimized', ha='center', va='center', \n",
    "                    transform=ax5.transAxes, fontsize=12)\n",
    "            ax5.set_title('Ensemble Weights', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 6. Model Comparison with Strategy Comparison\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        model_scores = []\n",
    "        model_labels = []\n",
    "        \n",
    "        # Individual models\n",
    "        for name, model in self.models.items():\n",
    "            model_proba = model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "            model_pred = (model_proba > self.optimal_threshold).astype(int)\n",
    "            model_scores.append(f1_score(self.y_test, model_pred) * 100)\n",
    "            model_labels.append(name.upper())\n",
    "        \n",
    "        # Ensemble strategies\n",
    "        for strategy in ['average', 'weighted', 'optimized', 'stacking']:\n",
    "            try:\n",
    "                strat_proba = self.get_ensemble_predictions(self.X_test_scaled, mode=strategy)\n",
    "                strat_pred = (strat_proba > self.optimal_threshold).astype(int)\n",
    "                strat_f1 = f1_score(self.y_test, strat_pred) * 100\n",
    "                model_scores.append(strat_f1)\n",
    "                label = f'ENS-{strategy[:3].upper()}'\n",
    "                if strategy == ensemble_mode:\n",
    "                    label += ' â˜…'\n",
    "                model_labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        colors_bars = ['#1f77b4', '#ff7f0e', '#2ca02c'] + ['#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "        colors_bars = colors_bars[:len(model_scores)]\n",
    "        \n",
    "        bars = ax6.bar(range(len(model_labels)), model_scores, color=colors_bars, alpha=0.8)\n",
    "        ax6.set_xticks(range(len(model_labels)))\n",
    "        ax6.set_xticklabels(model_labels, rotation=45, ha='right')\n",
    "        ax6.set_title('F1-Score Comparison (Models + Ensembles)', fontsize=14, fontweight='bold')\n",
    "        ax6.set_ylabel('F1-Score (%)')\n",
    "        ax6.set_ylim([0, 100])\n",
    "        ax6.axhline(y=80, color='red', linestyle='--', alpha=0.5, label='Target (80%)')\n",
    "        ax6.legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('ultimate_fraud_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"   âœ“ Saved: ultimate_fraud_analysis.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_complete_system(self, prefix='fraud_system'):\n",
    "        \"\"\"\n",
    "        Save complete cascaded fraud detection system as 2 separate files:\n",
    "        1. Stage 1 (Joblib) - Rule Engine + BNN High-Risk Identifier\n",
    "        2. Stage 2 (Joblib) - ML Ensemble Models\n",
    "        \n",
    "        This matches the cascaded architecture:\n",
    "        Transaction â†’ [Stage 1: Rules + BNN] â†’ [Stage 2: ML Ensemble]\n",
    "        \n",
    "        THRESHOLD SELECTION:\n",
    "        - Rule Engine: ALGORITHM-CHOSEN (score >= 85 for hard block)\n",
    "        - Ensemble ML: MANUALLY CHOSEN (0.390 for real-world compliance)\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸ’¾ Saving cascaded fraud detection system...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # =====================================================\n",
    "        # FILE 1: STAGE 1 - BNN MODEL COMPONENTS (JOBLIB)\n",
    "        # Saves BNN's sklearn model + threshold separately (not the class)\n",
    "        # This allows inference to load without needing the class definition\n",
    "        # =====================================================\n",
    "        \n",
    "        # Extract BNN components (model + threshold) if available\n",
    "        bnn = getattr(self, 'high_risk_identifier', None)\n",
    "        if bnn is not None and bnn.is_fitted:\n",
    "            bnn_model = bnn.model  # sklearn MLPClassifier (can be pickled)\n",
    "            bnn_threshold = bnn.threshold\n",
    "            bnn_rule_names = bnn.rule_names\n",
    "        else:\n",
    "            bnn_model = None\n",
    "            bnn_threshold = 0.5\n",
    "            bnn_rule_names = []\n",
    "        \n",
    "        stage1_bundle = {\n",
    "            # Rule Engine Configuration (for reference - engine is recreated at inference)\n",
    "            'rule_weights': RULE_WEIGHTS,\n",
    "            'hard_block_threshold': 85,  # Score >= 85 = definite fraud\n",
    "            'high_risk_threshold': 60,   # Score >= 60 = high risk blend\n",
    "            # Note: rule_engine NOT included (lambdas can't be pickled)\n",
    "            # Recreate at inference: rule_engine = RuleBasedEngine()\n",
    "            \n",
    "            # BNN Components (NOT the class, just the model + config)\n",
    "            'bnn_model': bnn_model,              # sklearn MLPClassifier\n",
    "            'bnn_threshold': bnn_threshold,       # Decision threshold\n",
    "            'bnn_rule_names': bnn_rule_names,     # Rule names for interpretability\n",
    "            'bnn_is_fitted': bnn_model is not None,\n",
    "            \n",
    "            # Metadata\n",
    "            'threshold_selection': 'ALGORITHM-CHOSEN',\n",
    "            'threshold_justification': 'Optimized by Bayesian calibration for fraud pattern detection',\n",
    "            'calibration_date': datetime.now().isoformat(),\n",
    "            'version': '2.1',\n",
    "            'description': 'Stage 1: BNN Model Components (sklearn MLP + threshold)'\n",
    "        }\n",
    "        \n",
    "        stage1_file = f'{prefix}_stage1.joblib'\n",
    "        joblib.dump(stage1_bundle, stage1_file)\n",
    "        print(f\"   âœ… Stage 1: {stage1_file}\")\n",
    "        print(f\"      â””â”€â”€ {len(RULE_WEIGHTS)} rules (config) + BNN model, hard_blockâ‰¥85\")\n",
    "        \n",
    "        # Also save JSON for quick reference (without BNN model)\n",
    "        # NOW INCLUDES RULE_CONDITIONS for inference scripts to load rule logic\n",
    "        rule_json = {\n",
    "            'rule_weights': RULE_WEIGHTS,\n",
    "            'rule_conditions': RULE_CONDITIONS,  # NEW: Declarative rule logic\n",
    "            'hard_block_threshold': 85,\n",
    "            'high_risk_threshold': 60,\n",
    "            'threshold_selection': 'ALGORITHM-CHOSEN',\n",
    "            'calibration_date': datetime.now().isoformat(),\n",
    "            'version': '3.0',  # Bumped version for new format\n",
    "            'description': 'Rule engine config with declarative conditions (can be loaded by inference scripts)'\n",
    "        }\n",
    "        rule_json_file = f'{prefix}_rules.json'\n",
    "        with open(rule_json_file, 'w') as f:\n",
    "            json.dump(convert_to_serializable(rule_json), f, indent=2)\n",
    "        print(f\"   âœ… Stage 1 Config (JSON): {rule_json_file}\")\n",
    "        print(f\"      â””â”€â”€ Includes {len(RULE_CONDITIONS)} declarative rule conditions\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # FILE 2: STAGE 2 - ML ENSEMBLE + SCALERS (JOBLIB)\n",
    "        # Contains: XGBoost, LightGBM, CatBoost, Scalers, Weights\n",
    "        # =====================================================\n",
    "        \n",
    "        # Get quantile transformer if available\n",
    "        quantile_transformer = getattr(self, 'quantile_transformer', None)\n",
    "        \n",
    "        # Get rule specs for BNN (rule names in exact order)\n",
    "        rule_specs = [(r[0], r[2]) for r in self.rule_engine.rules] if hasattr(self, 'rule_engine') else []\n",
    "        \n",
    "        # Convert XGBoost to CPU for portable inference (avoids GPU warning at inference)\n",
    "        if 'xgboost' in self.models:\n",
    "            try:\n",
    "                # For XGBClassifier, access the underlying booster to set params\n",
    "                booster = self.models['xgboost'].get_booster()\n",
    "                booster.set_param({'device': 'cpu', 'predictor': 'cpu_predictor'})\n",
    "                print(\"   ðŸ”„ XGBoost converted to CPU for portable inference\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ XGBoost CPU conversion failed (will still work): {e}\")\n",
    "        \n",
    "        stage2_bundle = {\n",
    "            # ML Models\n",
    "            'models': self.models,\n",
    "            'optimal_weights': getattr(self, 'optimal_weights', None),\n",
    "            'meta_model': getattr(self, 'meta_model', None),\n",
    "            \n",
    "            # Scalers (BOTH needed for proper inference)\n",
    "            'scaler': self.scaler,  # RobustScaler\n",
    "            'quantile_transformer': quantile_transformer,  # QuantileTransformer\n",
    "            \n",
    "            # Feature Engineering Config\n",
    "            'feature_names': self.feature_names,\n",
    "            \n",
    "            # Rule specs for BNN input (names + weights in exact order)\n",
    "            'rule_specs': rule_specs,  # [(rule_name, weight), ...]\n",
    "            \n",
    "            # Thresholds\n",
    "            'optimal_threshold': self.optimal_threshold,\n",
    "            'threshold_selection': 'MANUALLY-CHOSEN',\n",
    "            'threshold_justification': 'Selected 0.390 for hackathon + bank compliance (Recallâ‰¥80%, Precisionâ‰¥50%, Best F2)',\n",
    "            'best_ensemble_strategy': getattr(self, 'best_ensemble_strategy', 'bouncer'),\n",
    "            \n",
    "            # Metadata\n",
    "            'calibration_date': datetime.now().isoformat(),\n",
    "            'version': '2.1',\n",
    "            'description': 'Stage 2: ML Ensemble + Scalers (complete inference bundle)'\n",
    "        }\n",
    "        \n",
    "        stage2_file = f'{prefix}_ensemble.joblib'\n",
    "        joblib.dump(stage2_bundle, stage2_file)\n",
    "        print(f\"   âœ… Stage 2: {stage2_file}\")\n",
    "        print(f\"      â””â”€â”€ {len(self.models)} models + scalers, threshold={self.optimal_threshold:.3f}\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # OPTIONAL: Feature importance CSV\n",
    "        # =====================================================\n",
    "        if self.feature_importance is not None:\n",
    "            importance_file = f'{prefix}_feature_importance.csv'\n",
    "            self.feature_importance.to_csv(importance_file, index=False)\n",
    "            print(f\"   âœ… Feature Importance: {importance_file}\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # OPTIONAL: Ensemble weights JSON (for quick reference)\n",
    "        # =====================================================\n",
    "        if hasattr(self, 'optimal_weights') and self.optimal_weights:\n",
    "            weights_file = f'{prefix}_ensemble_weights.json'\n",
    "            with open(weights_file, 'w') as f:\n",
    "                json.dump(convert_to_serializable(self.optimal_weights), f, indent=2)\n",
    "            print(f\"   âœ… Ensemble Weights: {weights_file}\")\n",
    "        \n",
    "        # =====================================================\n",
    "        # FILE 3: FEATURE ENGINEERING CONFIG (JSON + JOBLIB)\n",
    "        # Required for FeatureEngineer class in inference\n",
    "        # =====================================================\n",
    "        \n",
    "        # Compute user and merchant statistics from training data\n",
    "        user_stats = {}\n",
    "        merchant_stats = {}\n",
    "        global_stats = {}\n",
    "        \n",
    "        if hasattr(self, 'df'):\n",
    "            df = self.df\n",
    "            \n",
    "            # Global amount statistics\n",
    "            if 'amount' in df.columns:\n",
    "                global_stats['amount_mean'] = float(df['amount'].mean())\n",
    "                global_stats['amount_std'] = float(df['amount'].std())\n",
    "                global_stats['amount_q90'] = float(df['amount'].quantile(0.9))\n",
    "            \n",
    "            # User statistics (for user behavioral features)\n",
    "            if 'user_id' in df.columns and 'amount' in df.columns:\n",
    "                user_groups = df.groupby('user_id')['amount']\n",
    "                for user_id, group in user_groups:\n",
    "                    user_stats[user_id] = {\n",
    "                        'mean': float(group.mean()),\n",
    "                        'std': float(group.std()) if len(group) > 1 else 0,\n",
    "                        'min': float(group.min()),\n",
    "                        'max': float(group.max()),\n",
    "                        'median': float(group.median()),\n",
    "                        'sum': float(group.sum()),\n",
    "                        'count': int(len(group))\n",
    "                    }\n",
    "            \n",
    "            # Merchant statistics (for merchant features)\n",
    "            if 'merchant_id' in df.columns and 'amount' in df.columns:\n",
    "                merchant_groups = df.groupby('merchant_id')\n",
    "                for merchant_id, group in merchant_groups:\n",
    "                    fraud_rate = group['is_fraud'].mean() if 'is_fraud' in group.columns else 0\n",
    "                    merchant_stats[merchant_id] = {\n",
    "                        'mean': float(group['amount'].mean()),\n",
    "                        'std': float(group['amount'].std()) if len(group) > 1 else 0,\n",
    "                        'median': float(group['amount'].median()),\n",
    "                        'count': int(len(group)),\n",
    "                        'fraud_rate': float(fraud_rate)\n",
    "                    }\n",
    "        \n",
    "        # Get categorical column values (for one-hot encoding)\n",
    "        categorical_columns = {}\n",
    "        low_card_categoricals = ['currency', 'transaction_type', 'channel', 'payment_method',\n",
    "                                  'device_type', 'browser', 'merchant_category', 'kyc_status']\n",
    "        if hasattr(self, 'df'):\n",
    "            for col in low_card_categoricals:\n",
    "                if col in self.df.columns:\n",
    "                    categorical_columns[col] = list(self.df[col].dropna().unique())\n",
    "        \n",
    "        # Save feature config JSON\n",
    "        feature_config = {\n",
    "            'global_stats': global_stats,\n",
    "            'categorical_columns': categorical_columns,\n",
    "            'training_columns': self.feature_names,\n",
    "            'high_risk_countries': ['BR', 'NG', 'RU', 'CN', 'VE', 'IR', 'KP', 'SD', 'SY', 'PK']\n",
    "        }\n",
    "        \n",
    "        config_file = f'{prefix}_feature_config.json'\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(convert_to_serializable(feature_config), f, indent=2)\n",
    "        print(f\"   âœ… Feature Config: {config_file}\")\n",
    "        print(f\"      â””â”€â”€ {len(self.feature_names)} training columns, {len(categorical_columns)} categorical cols\")\n",
    "        \n",
    "        # Save user/merchant stats (for behavioral features)\n",
    "        stats_file = f'{prefix}_user_merchant_stats.joblib'\n",
    "        stats_bundle = {\n",
    "            'user_stats': user_stats,\n",
    "            'merchant_stats': merchant_stats\n",
    "        }\n",
    "        joblib.dump(stats_bundle, stats_file)\n",
    "        print(f\"   âœ… User/Merchant Stats: {stats_file}\")\n",
    "        print(f\"      â””â”€â”€ {len(user_stats)} users, {len(merchant_stats)} merchants\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"ðŸŽ‰ CASCADED SYSTEM SAVED SUCCESSFULLY!\")\n",
    "        print(f\"   ðŸ“Œ ARCHITECTURE:\")\n",
    "        print(f\"      Stage 1: Rules + BNN â†’ {stage1_file}\")\n",
    "        print(f\"      Stage 2: ML Ensemble â†’ {stage2_file}\")\n",
    "        print(f\"      Feature Eng: {config_file} + {stats_file}\")\n",
    "        print(f\"   ðŸ“Œ THRESHOLDS:\")\n",
    "        print(f\"      Rule Engine:  hard_blockâ‰¥85 (ALGORITHM-CHOSEN)\")\n",
    "        print(f\"      Ensemble ML:  {self.optimal_threshold:.3f} (ALGORITHM-CHOSEN)\")\n",
    "        print(f\"   To load in production:\")\n",
    "        print(f\"   â€¢ Stage 1: joblib.load('{stage1_file}')\")\n",
    "        print(f\"   â€¢ Stage 2: joblib.load('{stage2_file}')\")\n",
    "        print(f\"   â€¢ Feature Eng: FeatureEngineer('{config_file}', '{stats_file}')\")\n",
    "\n",
    "\n",
    "class RecallOptimizedFraudDetectionEngine(UltimateFraudDetectionEngine):\n",
    "    \"\"\"\n",
    "    ENHANCED VERSION WITH RECALL-FOCUSED OPTIMIZATION\n",
    "    Now with option to use pre-tuned hyperparameters (skips Bayesian opt)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        super().__init__(data_path)\n",
    "        self.optimization_history = {}\n",
    "        self.recall_focused_weights = None\n",
    "        self.recall_target = 0.80  # 80% recall target for fraud detection\n",
    "\n",
    "    def train_with_bayesian_optimization(self):\n",
    "        \"\"\"\n",
    "        BAYESIAN OPTIMIZED TRAINING with F2 + Precision >= 60% objective.\n",
    "        \n",
    "        Delegates to train_ensemble_models_with_practical_bayesian which performs:\n",
    "        - XGBoost:  60 trials (GPU)\n",
    "        - LightGBM: 60 trials (CPU)\n",
    "        - CatBoost: 25 trials (CPU)\n",
    "        \n",
    "        Objective: Maximize F2 Score with Precision >= 60%\n",
    "        (Aligned with problem statement: reduce false positives while detecting fraud)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the real Bayesian optimization function\n",
    "        self.train_ensemble_models_with_practical_bayesian()\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        if 'xgboost' in self.models:\n",
    "            self.feature_importance = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': self.models['xgboost'].feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    def train_framework_components(self):\n",
    "        \n",
    "        # === Framework Components ===\n",
    "        print(\"\\nðŸ”§ Training Framework Components...\")\n",
    "        \n",
    "        # 1. Rule-Based Engine\n",
    "        print(\"   [1/2] Initializing Rule Engine (10 expert rules)...\")\n",
    "        self.rule_engine = RuleBasedEngine()\n",
    "        print(\"   âœ“ Rule Engine ready\")\n",
    "        \n",
    "        # 2. Autoencoder - DISABLED BY USER REQUEST (23.81% precision = adds noise)\n",
    "        # print(\"   [2/3] Training Autoencoder on normal transactions...\")\n",
    "        # normal_mask = self.y_train == 0\n",
    "        # X_normal = self.X_train_scaled[normal_mask]\n",
    "        # self.autoencoder = AutoencoderAnomalyDetector(input_dim=min(50, self.X_train_scaled.shape[1]))\n",
    "        # self.autoencoder.fit(X_normal)\n",
    "        # print(f\"   âœ“ Autoencoder trained on {len(X_normal)} normal samples\")\n",
    "        \n",
    "        # 2. Bayesian High-Risk Identifier (learns gray zone patterns)\n",
    "        print(\"   [2/2] Training Bayesian High-Risk Identifier...\")\n",
    "        \n",
    "\n",
    "        self.high_risk_identifier = BayesianHighRiskIdentifier(n_rules=len(self.rule_engine.rules))\n",
    "        self.high_risk_identifier.fit(\n",
    "            self.X_train,  # Unscaled data\n",
    "            self.feature_names,\n",
    "            self.y_train,\n",
    "            self.rule_engine\n",
    "        )\n",
    "        \n",
    "        print(\"   âœ“ Framework components ready\")\n",
    " \n",
    "    def _apply_self_training(self, confidence_threshold=0.95, max_iterations=3):\n",
    "        \"\"\"Self-training with pseudo-labels on high-confidence predictions\"\"\"\n",
    "        for iteration in range(max_iterations):\n",
    "            # Get ensemble predictions on validation set\n",
    "            preds = {}\n",
    "            for name, model in self.models.items():\n",
    "                preds[name] = model.predict_proba(self.X_val_scaled)[:, 1]\n",
    "            \n",
    "            # Average predictions\n",
    "            avg_pred = np.mean(list(preds.values()), axis=0)\n",
    "            \n",
    "            # Find high-confidence predictions\n",
    "            high_conf_fraud = avg_pred >= confidence_threshold\n",
    "            high_conf_legit = avg_pred <= (1 - confidence_threshold)\n",
    "            high_conf_mask = high_conf_fraud | high_conf_legit\n",
    "            \n",
    "            n_high_conf = high_conf_mask.sum()\n",
    "            if n_high_conf < 100:\n",
    "                print(f\"   Iteration {iteration+1}: Only {n_high_conf} high-confidence samples, stopping\")\n",
    "                break\n",
    "            \n",
    "            # Create pseudo-labels\n",
    "            pseudo_labels = (avg_pred >= 0.5).astype(int)\n",
    "            \n",
    "            # Add high-confidence samples to training\n",
    "            X_pseudo = self.X_val_scaled[high_conf_mask]\n",
    "            y_pseudo = pseudo_labels[high_conf_mask]\n",
    "            \n",
    "            # Retrain only XGBoost with augmented data (fastest)\n",
    "            X_augmented = np.vstack([self.X_train_scaled, X_pseudo])\n",
    "            y_augmented = np.concatenate([self.y_train, y_pseudo])\n",
    "            \n",
    "            self.models['xgboost'].fit(\n",
    "                X_augmented, y_augmented,\n",
    "                eval_set=[(self.X_val_scaled, self.y_val)],\n",
    "                early_stopping_rounds=30,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            print(f\"   Iteration {iteration+1}: Added {n_high_conf} pseudo-labeled samples\")\n",
    "        \n",
    "        print(\"   âœ“ Self-training complete\")\n",
    "    \n",
    "    def train_ensemble_models_with_practical_bayesian(self, n_trials_per_model=50):\n",
    "        \"\"\"\n",
    "        DIRECT TRAINING WITH OPTIMAL HYPERPARAMETERS (NO BAYESIAN OPTIMIZATION)\n",
    "        \n",
    "        Uses best hyperparameters found from previous Bayesian optimization:\n",
    "        - XGBoost:  Trial 58 (F2: 0.7085)\n",
    "        - LightGBM: Trial 22 (F2: 0.7145)\n",
    "        - CatBoost: Trial 23 (F2: 0.7096)\n",
    "        \n",
    "        This skips the time-consuming Bayesian search and directly trains with optimal params.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸš€ TRAINING WITH OPTIMAL HYPERPARAMETERS (FOUND USING BAYESIAN OPTIMIZATION)\")\n",
    "        print(\"   Using best parameters from previous optimization runs\")\n",
    "        \n",
    "        # === 1. XGBoost with optimal params from Trial 58 (F2: 0.7085) ===\n",
    "        print(f\"\\n[1/3] Training XGBoost with optimal hyperparameters...\")\n",
    "        \n",
    "        xgb_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'aucpr',\n",
    "            \n",
    "            # Best params from Trial 58 (F2: 0.7085)\n",
    "            'scale_pos_weight': 3.604751337893015,\n",
    "            'n_estimators': 1600,\n",
    "            'learning_rate': 0.01264923836032133,\n",
    "            'max_depth': 8,\n",
    "            'min_child_weight': 8,\n",
    "            'subsample': 0.7971623302216809,\n",
    "            'colsample_bytree': 0.7702707561076475,\n",
    "            'gamma': 0.3182485411673384,\n",
    "            'reg_alpha': 0.5721343259406985,\n",
    "            'reg_lambda': 2.8676192049842437,\n",
    "            'early_stopping_rounds': 50,  # Moved to constructor for XGBoost 2.0+ compatibility\n",
    "            \n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': 2,\n",
    "            \n",
    "            # GPU configuration\n",
    "            'tree_method': 'hist',\n",
    "            'device': 'cuda',\n",
    "            'predictor': 'gpu_predictor',\n",
    "            'max_bin': 256,\n",
    "            'grow_policy': 'lossguide',\n",
    "            'sampling_method': 'gradient_based'\n",
    "        }\n",
    "        \n",
    "        self.models['xgboost'] = xgb.XGBClassifier(**xgb_params)\n",
    "        self.models['xgboost'].fit(\n",
    "            self.X_train_scaled, self.y_train,\n",
    "            eval_set=[(self.X_val_scaled, self.y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        print(f\"   âœ… XGBoost trained (n_estimators={xgb_params['n_estimators']}, lr={xgb_params['learning_rate']:.4f})\")\n",
    "        \n",
    "        # === 2. LightGBM with optimal params from Trial 22 (F2: 0.7145) ===\n",
    "        if LIGHTGBM_AVAILABLE:\n",
    "            print(f\"\\n[2/3] Training LightGBM with optimal hyperparameters...\")\n",
    "            \n",
    "            lgb_params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                \n",
    "                # Best params from Trial 22 (F2: 0.7145)\n",
    "                'scale_pos_weight': 3.6979249328227004,\n",
    "                'n_estimators': 1511,\n",
    "                'learning_rate': 0.014462382942984102,\n",
    "                'max_depth': 9,\n",
    "                'num_leaves': 26,\n",
    "                'subsample': 0.8661931380997505,\n",
    "                'colsample_bytree': 0.7391783907773977,\n",
    "                'reg_alpha': 0.2057923932601703,\n",
    "                'reg_lambda': 0.5098277669321674,\n",
    "                'min_child_samples': 18,\n",
    "                \n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            \n",
    "            self.models['lightgbm'] = lgb.LGBMClassifier(**lgb_params)\n",
    "            self.models['lightgbm'].fit(\n",
    "                self.X_train_scaled, self.y_train,\n",
    "                eval_set=[(self.X_val_scaled, self.y_val)],\n",
    "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            print(f\"   âœ… LightGBM trained (n_estimators={lgb_params['n_estimators']}, lr={lgb_params['learning_rate']:.4f})\")\n",
    "        \n",
    "        # === 3. CatBoost with optimal params from Trial 23 (F2: 0.7096) ===\n",
    "        if CATBOOST_AVAILABLE:\n",
    "            print(f\"\\n[3/3] Training CatBoost with optimal hyperparameters...\")\n",
    "            \n",
    "            cat_params = {\n",
    "                'loss_function': 'Logloss',\n",
    "                'eval_metric': 'AUC',\n",
    "                \n",
    "                # Best params from Trial 23 (F2: 0.7096)\n",
    "                'scale_pos_weight': 3.6124435025354416,\n",
    "                'iterations': 1481,\n",
    "                'learning_rate': 0.0686923604666307,\n",
    "                'depth': 6,\n",
    "                'l2_leaf_reg': 1.4658666408948338,\n",
    "                'border_count': 166,\n",
    "                \n",
    "                'random_seed': RANDOM_STATE,\n",
    "                'verbose': 0,\n",
    "                \n",
    "                # GPU configuration\n",
    "                'task_type': 'GPU',\n",
    "                'devices': '0:0',\n",
    "                'used_ram_limit': '10GB',\n",
    "                'bootstrap_type': 'Bernoulli',\n",
    "                'allow_writing_files': False\n",
    "            }\n",
    "            \n",
    "            self.models['catboost'] = cb.CatBoostClassifier(**cat_params)\n",
    "            self.models['catboost'].fit(\n",
    "                self.X_train_scaled, self.y_train,\n",
    "                eval_set=(self.X_val_scaled, self.y_val),\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "            print(f\"   âœ… CatBoost trained (iterations={cat_params['iterations']}, lr={cat_params['learning_rate']:.4f})\")\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        if 'xgboost' in self.models:\n",
    "            self.feature_importance = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': self.models['xgboost'].feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nâœ… All models trained with optimal hyperparameters (Using Bayesian Optimization backed optimal weights)\")\n",
    "        print(f\"   Total models: {len(self.models)}\")\n",
    "        \n",
    "        return self.models\n",
    "    \n",
    "    \n",
    "    def optimize_ensemble_weights_practical_recall_focused(self, n_trials=300):\n",
    "        \"\"\"\n",
    "        DIRECT ENSEMBLE WEIGHT ASSIGNMENT (NO BAYESIAN OPTIMIZATION)\n",
    "        \n",
    "        Uses optimal weights from previous Bayesian optimization Trial 256:\n",
    "        - XGBoost:  24.16%\n",
    "        - LightGBM: 68.47%\n",
    "        - CatBoost:  7.37%\n",
    "        \n",
    "        Performance achieved: F2=73.25%, Score=0.7325\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸš€ ENSEMBLE WEIGHT ASSIGNMENT (Using Bayesian Optimization backed optimal weights)\")\n",
    "        print(\"   Using optimal weights from previous optimization\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Get model names for weight mapping\n",
    "        model_names = list(self.models.keys())\n",
    "        print(f\"   ðŸ“Š Setting weights for {len(model_names)} models: {model_names}\")\n",
    "        \n",
    "        # Optimal weights from Trial 256 (Score: 0.7325)\n",
    "        # weight_0 (xgboost): 0.24161051662086008\n",
    "        # weight_1 (lightgbm): 0.6846536032579759\n",
    "        # catboost: 1 - 0.24161051662086008 - 0.6846536032579759 = 0.0737358801211640\n",
    "        optimal_weight_map = {\n",
    "            'xgboost': 0.24161051662086008,   # 24.16%\n",
    "            'lightgbm': 0.6846536032579759,   # 68.47%\n",
    "            'catboost': 0.0737358801211640    # 7.37%\n",
    "        }\n",
    "        \n",
    "        # Build weights array in model order\n",
    "        optimal_weights = []\n",
    "        for name in model_names:\n",
    "            weight = optimal_weight_map.get(name, 1.0 / len(model_names))\n",
    "            optimal_weights.append(weight)\n",
    "        \n",
    "        # Normalize to ensure sum = 1\n",
    "        optimal_weights = np.array(optimal_weights)\n",
    "        optimal_weights = optimal_weights / optimal_weights.sum()\n",
    "        \n",
    "        print(f\"\\nðŸ† OPTIMAL WEIGHTS (from Trial 256, Score=0.7325):\")\n",
    "        for name, weight in zip(model_names, optimal_weights):\n",
    "            print(f\"   {name:12s}: {weight:.4f} ({weight*100:.2f}%)\")\n",
    "        \n",
    "        # Store weights\n",
    "        self.balanced_weights = dict(zip(model_names, optimal_weights))\n",
    "        self.optimal_weights = self.balanced_weights\n",
    "        \n",
    "        print(f\"\\nâœ… Ensemble weights set (Using Bayesian Optimization backed optimal weights)\")\n",
    "        \n",
    "        return self.balanced_weights\n",
    "    \n",
    "    def train_advanced_stacking_ensemble(self):\n",
    "        \"\"\"\n",
    "        ADVANCED STACKING WITH MULTIPLE META-LEARNERS\n",
    "        Trains and selects the best meta-model for stacking\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸŽ“ TRAINING ADVANCED STACKING ENSEMBLE...\")\n",
    "        \n",
    "        # Get base model predictions\n",
    "        base_predictions_train = []\n",
    "        base_predictions_val = []\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            train_pred = model.predict_proba(self.X_train_scaled)[:, 1]\n",
    "            val_pred = model.predict_proba(self.X_val_scaled)[:, 1]\n",
    "            \n",
    "            base_predictions_train.append(train_pred)\n",
    "            base_predictions_val.append(val_pred)\n",
    "        \n",
    "        X_meta_train = np.column_stack(base_predictions_train)\n",
    "        X_meta_val = np.column_stack(base_predictions_val)\n",
    "        \n",
    "        # Define meta-model with optimal params from Trial 296\n",
    "        # C: 0.08521596812289807, class_weight_ratio: 2.8291961956998812\n",
    "        meta_models = {\n",
    "            'logistic': LogisticRegression(\n",
    "                class_weight={0: 1.0, 1: 2.8291961956998812},  # Optimal class weight ratio\n",
    "                random_state=RANDOM_STATE,\n",
    "                max_iter=1000,\n",
    "                C=0.08521596812289807,  # Optimal C from Trial 296\n",
    "                solver='liblinear'\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Evaluate each meta-model\n",
    "        best_meta_model = None\n",
    "        best_meta_score = -float('inf')\n",
    "        meta_results = {}\n",
    "        \n",
    "        print(\"\\n   Meta-Model Evaluation:\")\n",
    "        print(f\"   {'Model':<20} {'Recall%':>10} {'F1%':>10} {'F2%':>10}\")\n",
    "        print(f\"   {'-'*20} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "        \n",
    "        for name, model in meta_models.items():\n",
    "            try:\n",
    "                model.fit(X_meta_train, self.y_train)\n",
    "                y_meta_pred = model.predict(X_meta_val)\n",
    "                \n",
    "                recall = recall_score(self.y_val, y_meta_pred) * 100\n",
    "                f1 = f1_score(self.y_val, y_meta_pred) * 100\n",
    "                f2 = fbeta_score(self.y_val, y_meta_pred, beta=2) * 100\n",
    "                \n",
    "                # Score with emphasis on recall\n",
    "                score = recall * 0.7 + f1 * 0.3\n",
    "                \n",
    "                meta_results[name] = {\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'f2': f2,\n",
    "                    'score': score\n",
    "                }\n",
    "                \n",
    "                print(f\"   {name:<20} {recall:>10.2f} {f1:>10.2f} {f2:>10.2f}\")\n",
    "                \n",
    "                if score > best_meta_score:\n",
    "                    best_meta_score = score\n",
    "                    best_meta_model = (name, model)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   {name:<20} Failed: {str(e)[:30]}...\")\n",
    "        \n",
    "        # Train best meta-model on combined data\n",
    "        if best_meta_model:\n",
    "            meta_name, meta_model = best_meta_model\n",
    "            \n",
    "            # Combine train and val for final training\n",
    "            X_meta_combined = np.vstack([X_meta_train, X_meta_val])\n",
    "            y_combined = np.concatenate([self.y_train, self.y_val])\n",
    "            \n",
    "            meta_model.fit(X_meta_combined, y_combined)\n",
    "            self.meta_model = meta_model\n",
    "            \n",
    "            print(f\"\\n   âœ… Selected meta-model: {meta_name}\")\n",
    "            print(f\"   ðŸ“Š Performance: Recall={meta_results[meta_name]['recall']:.2f}%, \"\n",
    "                  f\"F1={meta_results[meta_name]['f1']:.2f}%\")\n",
    "        else:\n",
    "            # Fallback to logistic regression\n",
    "            print(\"\\n   âš ï¸  No meta-model selected, using logistic regression as fallback\")\n",
    "            self.meta_model = LogisticRegression(\n",
    "                class_weight='balanced',\n",
    "                random_state=RANDOM_STATE,\n",
    "                max_iter=1000\n",
    "            )\n",
    "            X_meta_combined = np.vstack([X_meta_train, X_meta_val])\n",
    "            y_combined = np.concatenate([self.y_train, self.y_val])\n",
    "            self.meta_model.fit(X_meta_combined, y_combined)\n",
    "        \n",
    "        return self.meta_model\n",
    "    \n",
    "    def compare_ensemble_strategies_recall_focused(self):\n",
    "        \"\"\"\n",
    "        ENSEMBLE STRATEGY COMPARISON WITH RECALL PRIORITY\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ”¬ RECALL-FOCUSED ENSEMBLE STRATEGY COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        strategies = ['bouncer', 'enhanced', 'average', 'weighted', 'optimized', 'stacking']\n",
    "        results = {}\n",
    "        \n",
    "        print(f\"\\n{'Strategy':>12} {'Acc%':>8} {'Prec%':>8} {'Rec%':>8} {'F1%':>8} {'F2%':>8} {'R-F Score':>10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        best_strategy = None\n",
    "        best_recall_f1_score = 0\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            try:\n",
    "                y_pred_proba = self.get_ensemble_predictions(self.X_val_scaled, mode=strategy)\n",
    "                \n",
    "                # Optimize threshold for recall-F1 balance\n",
    "                best_thresh = 0.5\n",
    "                best_recall_f1 = 0\n",
    "                \n",
    "                # Search thresholds with bias toward higher recall\n",
    "                for thresh in np.linspace(0.2, 0.6, 41):  # Lower range for higher recall\n",
    "                    y_pred = (y_pred_proba > thresh).astype(int)\n",
    "                    recall = recall_score(self.y_val, y_pred)\n",
    "                    f1 = f1_score(self.y_val, y_pred)\n",
    "                    \n",
    "                    # RECALL-F1 BALANCED SCORE (weights recall more)\n",
    "                    recall_f1_score = recall * 0.6 + f1 * 0.4\n",
    "                    \n",
    "                    if recall_f1_score > best_recall_f1:\n",
    "                        best_recall_f1 = recall_f1_score\n",
    "                        best_thresh = thresh\n",
    "                \n",
    "                y_pred = (y_pred_proba > best_thresh).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                acc = accuracy_score(self.y_val, y_pred) * 100\n",
    "                prec = precision_score(self.y_val, y_pred, zero_division=0) * 100\n",
    "                rec = recall_score(self.y_val, y_pred) * 100\n",
    "                f1 = f1_score(self.y_val, y_pred) * 100\n",
    "                f2 = fbeta_score(self.y_val, y_pred, beta=2) * 100\n",
    "                auc = roc_auc_score(self.y_val, y_pred_proba) * 100\n",
    "                \n",
    "                # Combined recall-F1 score for ranking\n",
    "                recall_f1_combined = rec * 0.6 + f1 * 0.4\n",
    "                \n",
    "                results[strategy] = {\n",
    "                    'accuracy': acc, 'precision': prec, 'recall': rec,\n",
    "                    'f1': f1, 'f2': f2, 'auc': auc, 'threshold': best_thresh,\n",
    "                    'recall_f1_score': recall_f1_combined\n",
    "                }\n",
    "                \n",
    "                # Mark stacking with special notation for high recall\n",
    "                marker = \"â˜…\" if strategy == 'stacking' else \" \"\n",
    "                print(f\"{marker}{strategy:>11} {acc:>8.2f} {prec:>8.2f} {rec:>8.2f} {f1:>8.2f} {f2:>8.2f} {recall_f1_combined:>10.2f}\")\n",
    "                \n",
    "                if recall_f1_combined > best_recall_f1_score:\n",
    "                    best_recall_f1_score = recall_f1_combined\n",
    "                    best_strategy = strategy\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{strategy:>12} - Failed: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # DYNAMIC STRATEGY SELECTION BASED ON RECALL\n",
    "        stacking_recall = results.get('stacking', {}).get('recall', 0)\n",
    "        weighted_recall = results.get('weighted', {}).get('recall', 0)\n",
    "        stacking_f1 = results.get('stacking', {}).get('f1', 0)\n",
    "        weighted_f1 = results.get('weighted', {}).get('f1', 0)\n",
    "        \n",
    "        # Select stacking if it has significantly higher recall with acceptable F1 loss\n",
    "        if (stacking_recall - weighted_recall > 5 and \n",
    "            weighted_f1 - stacking_f1 < 5):\n",
    "            print(f\"ðŸ† SELECTING STACKING (Superior Recall: +{stacking_recall-weighted_recall:.1f}%)\")\n",
    "            best_strategy = 'stacking'\n",
    "            selection_reason = \"Stacking selected for superior recall\"\n",
    "        else:\n",
    "            print(f\"ðŸ† BEST STRATEGY: {best_strategy.upper()}\")\n",
    "            selection_reason = f\"{best_strategy.upper()} selected based on recall-F1 score\"\n",
    "        \n",
    "        print(f\"   {selection_reason}\")\n",
    "        print(f\"   Recall-F1 Score: {best_recall_f1_score:.2f}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.best_ensemble_strategy = best_strategy\n",
    "        self.ensemble_comparison_results = results\n",
    "        \n",
    "        return best_strategy, results\n",
    "    \n",
    "    def find_optimal_threshold_recall_focused(self):\n",
    "        \"\"\"\n",
    "        CASCADE-BASED THRESHOLD ANALYSIS\n",
    "        \n",
    "        Uses full cascade predictions (Rulesâ†’BNNâ†’ML blend) on validation set\n",
    "        to find optimal threshold that:\n",
    "        - Maximizes Recall\n",
    "        - With Precision in 50-55% range (optimal for fraud detection)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"ðŸŽ¯ CASCADE-BASED THRESHOLD ANALYSIS\")\n",
    "        ml_strat_display = getattr(self, 'best_voting_strategy', 'average')\n",
    "        print(f\"   Using full pipeline: Rules (â‰¥85% hard block) â†’ BNN â†’ {ml_strat_display.upper()} â†’ Blend\")\n",
    "        print(\"   Constraint: Max Recall for Precision â‰ˆ 50-55%\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # === STEP 1: Get CASCADE predictions on validation set ===\n",
    "        # This mimics evaluate_final() logic but on validation data\n",
    "        \n",
    "        HARD_BLOCK_THRESHOLD = 0.85\n",
    "        HIGH_RISK_THRESHOLD = 0.60\n",
    "        \n",
    "        n_val = len(self.y_val)\n",
    "        cascade_predictions = np.zeros(n_val)\n",
    "        \n",
    "        # Get rule scores on unscaled data\n",
    "        if hasattr(self, 'rule_engine') and hasattr(self, 'feature_names') and hasattr(self, 'X_val'):\n",
    "            rule_scores = self.rule_engine.predict_batch(self.X_val, self.feature_names)\n",
    "            rule_scores_norm = rule_scores / 100.0\n",
    "            \n",
    "            # Hard block\n",
    "            hard_block_mask = rule_scores_norm >= HARD_BLOCK_THRESHOLD\n",
    "            cascade_predictions[hard_block_mask] = 1.0\n",
    "            \n",
    "            # Soft zone processing\n",
    "            soft_mask = ~hard_block_mask\n",
    "            if soft_mask.sum() > 0:\n",
    "                X_soft = self.X_val_scaled[soft_mask]\n",
    "                soft_rule_scores = rule_scores_norm[soft_mask]\n",
    "                \n",
    "                # Get ML predictions using best_voting_strategy (same as evaluate_final)\n",
    "                ml_strategy = getattr(self, 'best_voting_strategy', 'average')\n",
    "                if ml_strategy == 'bouncer':\n",
    "                    ml_strategy = 'average'\n",
    "                \n",
    "                if ml_strategy == 'stack_enhanced':\n",
    "                    ml_probs = self._strategy_stacking_enhanced(X_soft)\n",
    "                elif ml_strategy == 'tiered_vote':\n",
    "                    ml_probs = self._strategy_tiered_threshold(X_soft)\n",
    "                elif ml_strategy == 'conf_weighted':\n",
    "                    ml_probs = self._strategy_confidence_weighted(X_soft)\n",
    "                elif ml_strategy == 'delphi':\n",
    "                    ml_probs = self._strategy_delphi_consensus(X_soft)\n",
    "                else:\n",
    "                    mode_map = {'average': 'average', 'weighted': 'weighted', 'bayesian_opt': 'optimized'}\n",
    "                    ml_probs = self.get_ensemble_predictions(X_soft, mode=mode_map.get(ml_strategy, 'average'))\n",
    "                \n",
    "                # BNN High-Risk Detection\n",
    "                if hasattr(self, 'high_risk_identifier') and self.high_risk_identifier.is_fitted:\n",
    "                    X_soft_unscaled = self.X_val[soft_mask]\n",
    "                    bnn_result = self.high_risk_identifier.predict(\n",
    "                        X_soft_unscaled, self.feature_names, self.rule_engine\n",
    "                    )\n",
    "                    bnn_high_risk = bnn_result['is_high_risk']\n",
    "                else:\n",
    "                    bnn_high_risk = np.zeros(len(X_soft), dtype=bool)\n",
    "                \n",
    "                # Cascade Blending\n",
    "                strong_rule_mask = soft_rule_scores >= HIGH_RISK_THRESHOLD\n",
    "                use_high_risk = bnn_high_risk | strong_rule_mask\n",
    "                \n",
    "                blended = np.zeros(len(X_soft))\n",
    "                blended[use_high_risk] = ml_probs[use_high_risk] * 0.65 + soft_rule_scores[use_high_risk] * 0.35\n",
    "                blended[~use_high_risk] = ml_probs[~use_high_risk] * 0.90 + soft_rule_scores[~use_high_risk] * 0.10\n",
    "                \n",
    "                cascade_predictions[soft_mask] = np.clip(blended, 0, 1)\n",
    "        else:\n",
    "            # Fallback: use ML ensemble only\n",
    "            print(\"   âš ï¸ Rule engine not available, using ML ensemble only\")\n",
    "            strategy = getattr(self, 'best_ensemble_strategy', 'weighted')\n",
    "            cascade_predictions = self.get_ensemble_predictions(self.X_val_scaled, mode=strategy)\n",
    "        \n",
    "        # === STEP 2: Threshold Analysis with CASCADE predictions ===\n",
    "        thresholds = np.linspace(0.10, 0.60, 51)\n",
    "        \n",
    "        print(f\"\\n{'Thresh':>7} â”‚ {'Recall%':>8} {'Prec%':>8} {'F1%':>8} {'F2%':>8} â”‚ {'TP':>6} {'FP':>6} {'FN':>6} â”‚ Notes\")\n",
    "        print(\"â”€\"*95)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            y_pred = (cascade_predictions > thresh).astype(int)\n",
    "            \n",
    "            recall = recall_score(self.y_val, y_pred)\n",
    "            precision = precision_score(self.y_val, y_pred, zero_division=0)\n",
    "            f1 = f1_score(self.y_val, y_pred)\n",
    "            f2 = fbeta_score(self.y_val, y_pred, beta=2)\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(self.y_val, y_pred).ravel()\n",
    "            \n",
    "            result = {\n",
    "                'threshold': thresh, 'recall': recall, 'precision': precision,\n",
    "                'f1': f1, 'f2': f2, 'tp': tp, 'fp': fp, 'fn': fn\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Notes\n",
    "            notes = []\n",
    "            if recall >= 0.80:\n",
    "                notes.append(\"âœ“Recâ‰¥80\")\n",
    "            if 0.50 <= precision <= 0.55:\n",
    "                notes.append(\"â˜…P:50-55%\")\n",
    "            elif precision >= 0.50:\n",
    "                notes.append(\"âœ“Precâ‰¥50\")\n",
    "            \n",
    "            notes_str = \" \".join(notes) if notes else \"\"\n",
    "            print(f\"{thresh:>7.3f} â”‚ {recall*100:>8.2f} {precision*100:>8.2f} {f1*100:>8.2f} {f2*100:>8.2f} â”‚ {tp:>6} {fp:>6} {fn:>6} â”‚ {notes_str}\")\n",
    "        \n",
    "        print(\"â”€\"*95)\n",
    "        \n",
    "        # === STEP 3: AUTOMATIC Threshold Selection ===\n",
    "        # Constraint: Max Recall for Precision in 50-55%\n",
    "        PREC_MIN = 0.50\n",
    "        PREC_MAX = 0.55\n",
    "        \n",
    "        viable = [r for r in all_results if PREC_MIN <= r['precision'] <= PREC_MAX]\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ AUTOMATIC THRESHOLD SELECTION:\")\n",
    "        print(f\"   Constraint: Best F2 with Precision in {PREC_MIN*100:.0f}%-{PREC_MAX*100:.0f}%\")\n",
    "        \n",
    "        if viable:\n",
    "            # Best = max F2 within precision constraint (optimal for hackathon + bank compliance)\n",
    "            best = max(viable, key=lambda x: x['f2'])\n",
    "            print(f\"\\n   âœ… OPTIMAL THRESHOLD (Algorithm-Chosen): {best['threshold']:.3f}\")\n",
    "            print(f\"      â†’ F2-Score: {best['f2']*100:.2f}% (MAXIMIZED)\")\n",
    "            print(f\"      â†’ Recall: {best['recall']*100:.2f}%\")\n",
    "            print(f\"      â†’ Precision: {best['precision']*100:.2f}% (in target range)\")\n",
    "        else:\n",
    "            # Fallback: find threshold with precision closest to 52.5% (midpoint)\n",
    "            midpoint = (PREC_MIN + PREC_MAX) / 2\n",
    "            best = min(all_results, key=lambda x: abs(x['precision'] - midpoint))\n",
    "            print(f\"\\n   âš ï¸ No threshold in exact range, using closest to {midpoint*100:.1f}%\")\n",
    "            print(f\"   SELECTED THRESHOLD: {best['threshold']:.3f}\")\n",
    "            print(f\"      â†’ Recall: {best['recall']*100:.2f}%\")\n",
    "            print(f\"      â†’ Precision: {best['precision']*100:.2f}%\")\n",
    "        \n",
    "        # Also show other reference points\n",
    "        print(f\"\\nðŸ“Š REFERENCE THRESHOLDS:\")\n",
    "        \n",
    "        # Best F2 unconstrained\n",
    "        best_f2 = max(all_results, key=lambda x: x['f2'])\n",
    "        print(f\"   Best F2:       {best_f2['threshold']:.3f} â†’ F2={best_f2['f2']*100:.2f}%, Rec={best_f2['recall']*100:.2f}%, Prec={best_f2['precision']*100:.2f}%\")\n",
    "        \n",
    "        # Best F1 (balanced)\n",
    "        best_f1 = max(all_results, key=lambda x: x['f1'])\n",
    "        print(f\"   Best F1:       {best_f1['threshold']:.3f} â†’ F1={best_f1['f1']*100:.2f}%, Rec={best_f1['recall']*100:.2f}%, Prec={best_f1['precision']*100:.2f}%\")\n",
    "        \n",
    "        # Max recall with P >= 50%\n",
    "        viable_50 = [r for r in all_results if r['precision'] >= 0.50]\n",
    "        if viable_50:\n",
    "            best_rec_50 = max(viable_50, key=lambda x: x['recall'])\n",
    "            print(f\"   Max Rec@Pâ‰¥50%: {best_rec_50['threshold']:.3f} â†’ Rec={best_rec_50['recall']*100:.2f}%, Prec={best_rec_50['precision']*100:.2f}%\")\n",
    "        \n",
    "        self.optimal_threshold = best['threshold']\n",
    "        self.threshold_analysis = all_results\n",
    "        \n",
    "        print(f\"\\nâœ… Selected threshold: {self.optimal_threshold:.3f}\")\n",
    "        \n",
    "        return best\n",
    "    \n",
    "    def evaluate_final_recall_optimized(self):\n",
    "        \"\"\"\n",
    "        FINAL EVALUATION WITH BALANCE FOCUS\n",
    "        Enhanced to show precision-recall balance metrics\n",
    "        \"\"\"\n",
    "        # Header is printed by evaluate_final() - no duplicate needed\n",
    "        \n",
    "        # Get results from parent class\n",
    "        results = self.evaluate_final()\n",
    "        \n",
    "        # === NEW: Balance Analysis ===\n",
    "        print(f\"\\nðŸŽ¯ BALANCE ANALYSIS:\")\n",
    "        \n",
    "        recall = results['recall']\n",
    "        precision = results['precision']\n",
    "        f1 = results['f1']\n",
    "        \n",
    "        # Precision-Recall Gap\n",
    "        pr_gap = abs(precision - recall)\n",
    "        gap_status = 'âœ…' if pr_gap < 20 else 'ðŸ“Š' if pr_gap < 30 else 'ðŸ“Š'\n",
    "        print(f\"   Precision-Recall Gap: {pr_gap:.1f}% {gap_status}\")\n",
    "        \n",
    "        # Balance Score (min/max ratio)\n",
    "        balance_score = min(precision, recall) / (max(precision, recall) + 1e-6)\n",
    "        balance_status = 'âœ…' if balance_score > 0.70 else 'ðŸ“Š' if balance_score > 0.50 else 'ðŸ“Š'\n",
    "        print(f\"   Balance Score: {balance_score:.3f} {balance_status}\")\n",
    "        \n",
    "        # F1 Score Assessment\n",
    "        f1_status = 'âœ…' if f1 > 65 else 'ðŸ“Š' if f1 > 55 else 'ðŸ“Š'\n",
    "        print(f\"   F1 Score: {f1:.2f}% {f1_status}\")\n",
    "        \n",
    "        # Overall balance grade\n",
    "        if balance_score > 0.70 and f1 > 65:\n",
    "            grade = \"EXCELLENT\"\n",
    "            grade_emoji = \"ðŸ†\"\n",
    "        elif balance_score > 0.60 and f1 > 60:\n",
    "            grade = \"GOOD\"\n",
    "            grade_emoji = \"âœ…\"\n",
    "        elif balance_score > 0.50:\n",
    "            grade = \"MODERATE\"\n",
    "            grade_emoji = \"ðŸ“Š\"\n",
    "        else:\n",
    "            grade = \"NEEDS IMPROVEMENT\"\n",
    "            grade_emoji = \"ðŸ“Š\"\n",
    "        \n",
    "        print(f\"   Overall Balance: {grade_emoji} {grade}\")\n",
    "        \n",
    "        # Precision improvement check\n",
    "        baseline_precision = 36.93  # Original system\n",
    "        if precision > baseline_precision:\n",
    "            improvement = precision - baseline_precision\n",
    "            print(f\"\\n   âœ… PRECISION IMPROVED: {precision:.1f}% (+{improvement:.1f}% from baseline)\")\n",
    "        else:\n",
    "            print(f\"   ðŸ“Š Precision: {precision:.1f}%\")\n",
    "        \n",
    "        # Show optimization summary\n",
    "        if hasattr(self, 'optimization_history'):\n",
    "            total_trials = 0\n",
    "            for model_name, study in self.optimization_history.items():\n",
    "                if study:\n",
    "                    total_trials += len(study.trials)\n",
    "            \n",
    "            print(f\"\\nðŸ§¬ OPTIMIZATION SUMMARY:\")\n",
    "            print(f\"   â€¢ Mode: FAST (Hardcoded Optimal Parameters)\")\n",
    "            print(f\"   â€¢ Bayesian Optimization: SKIPPED for all models\")\n",
    "            print(f\"   â€¢ Cost Savings: ~600+ trials avoided\")\n",
    "            # print(f\"   â€¢ Total optimization trials: {total_trials}\")\n",
    "            # print(f\"   â€¢ Models optimized: {', '.join([k for k, v in self.optimization_history.items() if v])}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION WITH BALANCED PRECISION-RECALL OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# In the main execution section:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = r\"C:\\Users\\ADMIN\\Documents\\TechFiesta_2026\\data\\realistic_transaction_dataset_821.csv\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸš€ ULTIMATE FRAUD DETECTION ENGINE \")\n",
    "    print(\"   ðŸŽ¯ Target: Maximize F2 Score with balanced Precision-Recall\")\n",
    "    print(\"   âš¡ 3 ML Models: XGBoost, LightGBM, CatBoost\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        # Initialize enhanced engine\n",
    "        engine = RecallOptimizedFraudDetectionEngine(DATA_PATH)\n",
    "        \n",
    "        # Step 1: Data Preparation & Feature Engineering\n",
    "        print(\"\\n[Step 1/8] Data Preparation & Feature Engineering...\")\n",
    "        engine.prepare_data_with_time_aware_split()\n",
    "        \n",
    "        # Step 2: Verify Class Distribution\n",
    "        print(\"\\n[Step 2/8] Verifying Class Distribution...\")\n",
    "        engine.verify_class_distribution()\n",
    "\n",
    "        # Step 3: Initialising Rule-Based Engine and Training BNN\n",
    "        print(\"\\n[Step 3/8] Initializing Rule-Based Engine and Training BNN\")\n",
    "        engine.train_framework_components()\n",
    "        \n",
    "        # Step 4: Training Models with Optimal Hyperparameters\n",
    "        print(\"\\n[Step 4/8] Training ML Models (pre-tuned hyperparameters)...\")\n",
    "        engine.train_with_bayesian_optimization()\n",
    "        \n",
    "        # Step 5: Ensemble Weights (Direct Assignment)\n",
    "        print(\"\\n[Step 5/8] Assigning Optimized Ensemble Weights...\")\n",
    "        engine.optimize_ensemble_weights_practical_recall_focused(n_trials=0)\n",
    "        \n",
    "        # Step 6: Comparing ALL Voting Strategies (includes stack_enhanced with meta-model)\n",
    "        print(\"\\n[Step 6/8] Comparing ALL Voting Strategies...\")\n",
    "        best_strategy, strategy_results = engine.compare_all_strategies()\n",
    "        \n",
    "        # Step 7: Threshold Analysis\n",
    "        print(\"\\n[Step 7/8] Threshold Analysis (ALL METRICS)...\")\n",
    "        engine.find_optimal_threshold_recall_focused()\n",
    "        \n",
    "        # Step 8: Final Cascaded Evaluation on Test Set\n",
    "        print(\"\\n[Step 8/8] Final Cascaded Evaluation on Test Set...\")\n",
    "        results = engine.evaluate_final_recall_optimized()\n",
    "        \n",
    "        engine.plot_comprehensive_analysis()\n",
    "        \n",
    "        # Save system\n",
    "        engine.save_complete_system('ultimate_fraud_system_fast.joblib')\n",
    "        \n",
    "        # Save results\n",
    "        with open('ultimate_evaluation_results_fast.json', 'w') as f:\n",
    "            enhanced_results = {\n",
    "                **results,\n",
    "                'recall_target': engine.recall_target,\n",
    "                'mode': 'fast_pretrained_params'\n",
    "            }\n",
    "            json.dump(convert_to_serializable(enhanced_results), f, indent=4)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"âœ… FAST MODE ENGINE - COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š SUMMARY:\")\n",
    "        print(f\"   â€¢ Models trained: {len(engine.models)} (with pre-tuned hyperparameters)\")\n",
    "        print(f\"   â€¢ Bayesian optimization: SKIPPED (using best params from previous run)\")\n",
    "        print(f\"   â€¢ Threshold table: Printed above for manual selection\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ PERFORMANCE:\")\n",
    "        print(f\"   â€¢ Recall: {results['recall']:.2f}%\")\n",
    "        print(f\"   â€¢ Precision: {results['precision']:.2f}%\")\n",
    "        print(f\"   â€¢ F1-Score: {results['f1']:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Œ TO OVERRIDE THRESHOLD:\")\n",
    "        print(f\"   results = engine.evaluate_final_recall_optimized()\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ SAVED:\")\n",
    "        print(f\"   - ultimate_fraud_system_fast.joblib\")\n",
    "        print(f\"   - ultimate_evaluation_results_fast.json\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nâŒ ERROR: File not found at {DATA_PATH}\")\n",
    "        print(\"   Please update DATA_PATH with your dataset location.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e978e4-d27f-4e2c-9a00-402a5abd80b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TechFiesta_2026_XGBoost_Env",
   "language": "python",
   "name": "techfiesta_2026_xgboost_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
